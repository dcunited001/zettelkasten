:PROPERTIES:
:ID:       4c531cd8-3f06-47fb-857a-e70603891ed8
:END:
#+title: Hadoop


* Roam
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
+ [[id:79d41758-7ad5-426a-9964-d3e4f5685e7e][Compute]]
+ [[id:ac2a1ae4-a695-4226-91f0-8386dc4d9b07][Devops]]

* Docs

+ [[wikipedia:Category:Hadoop][Hadoop Wiki]] (category)

* Resources

* Overview

From [[https://en.wikipedia.org/wiki/List_of_Apache_Software_Foundation_projects][List of Apache Software Foundation Projects]]. ASF provides a [[https://projects.apache.org/projects.html?category][better
categorization of its products]] or of [[https://incubator.apache.org/projects/][the products in the Apache incubator]].

|---------+-------------------------------------------+-------+--------------------------------|
| Product | Role                                      | Niche | Notes                          |
|---------+-------------------------------------------+-------+--------------------------------|
| Hadoop  | Map Reduce                                |       |                                |
| HBase   | Hadoop Database                           |       |                                |
| Knox    | REST API Gateway for Hadoop Services      |       |                                |
| Ozone   | Object store for Hadoop                   |       | scalable/redundant/distributed |
| Pig     | Platform for analyzing datasets on Hadoop |       |                                |
|---------+-------------------------------------------+-------+--------------------------------|

** Scheduling

|---------+---------------------+-------+-------|
| Product | Role                | Niche | Notes |
|---------+---------------------+-------+-------|
| Oozie   | Workflow scheduling |       |       |
|---------+---------------------+-------+-------|

** Security

|---------+------------------------------------------------+-------+-------------------------------|
| Product | Role                                           | Niche | Notes                         |
|---------+------------------------------------------------+-------+-------------------------------|
| Ranger  | comprehensive data security on Hadoop platform |       | enable/monitor/manage datasec |
|---------+------------------------------------------------+-------+-------------------------------|

** Provisioning and Automation

|-----------+--------------------------------------+-------+-------|
| Product   | Role                                 | Niche | Notes |
|-----------+--------------------------------------+-------+-------|
| Bigtop    | packaging/tests for Hadoop ecosystem |       |       |
| Ambari    | Cluster Provisioning                 |       |       |
| ZooKeeper | Configuration management             |       |       |
|-----------+--------------------------------------+-------+-------|

** Columnar formats

|------------+---------------------------+-----------------------+---------------------------------------|
| Product    | Role                      | Niche                 | Notes                                 |
|------------+---------------------------+-----------------------+---------------------------------------|
| Parquet    | Columnar data at rest     | general purpose, open |                                       |
| CarbonData | Format for fast analytics |                       |                                       |
| ORC        |                           | bigdata workloads     |                                       |
| RCFile     | defines                   |                       | defacto data storage structure format |
|------------+---------------------------+-----------------------+---------------------------------------|

** Serialization Formats

See [[https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats][Comparison data-serialization formats]]

** Columar databases

|-----------+----------------------------+--------------------------+-------|
| Product   | Role                       | Niche                    | Notes |
|-----------+----------------------------+--------------------------+-------|
| Cassandra | scalable columnar database |                          |       |
| HBase     |                            |                          |       |
| Accumulo  |                            | Security policy for data |       |
|-----------+----------------------------+--------------------------+-------|

** Query providers

|---------+----------------------------------+----------------+-------|
| Product | Role                             | Niche          | Notes |
|---------+----------------------------------+----------------+-------|
| Presto  | query heterogenous data services |                |       |
| Drill   |                                  |                |       |
| Arrow   |                                  | Queries in RAM |       |
|---------+----------------------------------+----------------+-------|

** File systems

|-----------------+------+--------------------------+----------------|
| Product         | Role | Niche                    | Notes          |
|-----------------+------+--------------------------+----------------|
| HDFS            |      |                          |                |
| Parascale FS    |      |                          | Parascale      |
| IBRIX Fusion FS |      |                          | HP             |
| MapR FS         |      | Random Access read/write | MapR Tech Inc. |
|-----------------+------+--------------------------+----------------|


* Design

** Data Placement

For requirements (motivating [[wiki:RCFile][RCFile design]])

1. Fast Data Loading
2. Fast Query Processing
3. Highly Efficient Space Utilization
4. Strong adaptivity to dyanmic data access patterns

** Hadoop framework

From [[https://en.wikipedia.org/wiki/Apache_Hadoop][Hadoop wiki]], the base Hadoop framework is composed of five modules:

1. Common: libraries and utilities
2. Hadoop Distributed File System: file system running on "commodity machines"
   designed to be accessed to produce results for mapreduce queries
3. YARN: manages computing resources and allocating them to be scheduled for
   workloads
4. MapReduce: implementation of the MapReduce programming model for large-scale
   data processing
5. Ozone (2020): object store for Hadoop

   The MapReduce & HDFS were inspired by [[https://books.google.com/books?id=axruBQAAQBAJ&pg=PA300][Google's papers on MapReduce and GFS]]

* Parquet


*
