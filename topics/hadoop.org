:PROPERTIES:
:ID:       4c531cd8-3f06-47fb-857a-e70603891ed8
:END:
#+title: Hadoop


* Roam
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
+ [[id:79d41758-7ad5-426a-9964-d3e4f5685e7e][Compute]]
+ [[id:ac2a1ae4-a695-4226-91f0-8386dc4d9b07][Devops]]

* Docs

+ [[wikipedia:Category:Hadoop][Hadoop Wiki]] (category)

* Resources

* Overview

From [[https://en.wikipedia.org/wiki/List_of_Apache_Software_Foundation_projects][List of Apache Software Foundation Projects]]. ASF provides a [[https://projects.apache.org/projects.html?category][better
categorization of its products]] or of [[https://incubator.apache.org/projects/][the products in the Apache incubator]].

|---------+-------------------------------------------+-------+--------------------------------|
| Product | Role                                      | Niche | Notes                          |
|---------+-------------------------------------------+-------+--------------------------------|
| Hadoop  | Map Reduce                                |       |                                |
| HBase   | Hadoop Database                           |       |                                |
| Knox    | REST API Gateway for Hadoop Services      |       |                                |
| Ozone   | Object store for Hadoop                   |       | scalable/redundant/distributed |
| Pig     | Platform for analyzing datasets on Hadoop |       |                                |
|---------+-------------------------------------------+-------+--------------------------------|

** Scheduling

|---------+---------------------+-------+-------|
| Product | Role                | Niche | Notes |
|---------+---------------------+-------+-------|
| Oozie   | Workflow scheduling |       |       |
|---------+---------------------+-------+-------|

** Security

|---------+------------------------------------------------+-------+-------------------------------|
| Product | Role                                           | Niche | Notes                         |
|---------+------------------------------------------------+-------+-------------------------------|
| Ranger  | comprehensive data security on Hadoop platform |       | enable/monitor/manage datasec |
|---------+------------------------------------------------+-------+-------------------------------|

** Provisioning and Automation

|-----------+--------------------------------------+-------+-------|
| Product   | Role                                 | Niche | Notes |
|-----------+--------------------------------------+-------+-------|
| Bigtop    | packaging/tests for Hadoop ecosystem |       |       |
| Ambari    | Cluster Provisioning                 |       |       |
| ZooKeeper | Configuration management             |       |       |
|-----------+--------------------------------------+-------+-------|

** Columnar formats

|------------+---------------------------+-----------------------+---------------------------------------|
| Product    | Role                      | Niche                 | Notes                                 |
|------------+---------------------------+-----------------------+---------------------------------------|
| Parquet    | Columnar data at rest     | general purpose, open |                                       |
| CarbonData | Format for fast analytics |                       |                                       |
| ORC        |                           | bigdata workloads     |                                       |
| RCFile     | defines                   |                       | defacto data storage structure format |
|------------+---------------------------+-----------------------+---------------------------------------|

** Serialization Formats

See [[https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats][Comparison data-serialization formats]]

** Columar databases

|-----------+----------------------------+--------------------------+-------|
| Product   | Role                       | Niche                    | Notes |
|-----------+----------------------------+--------------------------+-------|
| Cassandra | scalable columnar database |                          |       |
| HBase     |                            |                          |       |
| Accumulo  |                            | Security policy for data |       |
|-----------+----------------------------+--------------------------+-------|

** Query providers

|---------+----------------------------------+----------------+-------|
| Product | Role                             | Niche          | Notes |
|---------+----------------------------------+----------------+-------|
| Presto  | query heterogenous data services |                |       |
| Drill   |                                  |                |       |
| Arrow   |                                  | Queries in RAM |       |
|---------+----------------------------------+----------------+-------|

** File systems

|-----------------+------+--------------------------+----------------|
| Product         | Role | Niche                    | Notes          |
|-----------------+------+--------------------------+----------------|
| HDFS            |      |                          |                |
| Parascale FS    |      |                          | Parascale      |
| IBRIX Fusion FS |      |                          | HP             |
| MapR FS         |      | Random Access read/write | MapR Tech Inc. |
|-----------------+------+--------------------------+----------------|


* Design

** Data Placement

For requirements (motivating [[wiki:RCFile][RCFile design]])

1. Fast Data Loading
2. Fast Query Processing
3. Highly Efficient Space Utilization
4. Strong adaptivity to dyanmic data access patterns

** Hadoop framework

From [[https://en.wikipedia.org/wiki/Apache_Hadoop][Hadoop wiki]], the base Hadoop framework is composed of five modules:

1. Common: libraries and utilities
2. Hadoop Distributed File System: file system running on "commodity machines"
   designed to be accessed to produce results for mapreduce queries
3. YARN: manages computing resources and allocating them to be scheduled for
   workloads
4. MapReduce: implementation of the MapReduce programming model for large-scale
   data processing
5. Ozone (2020): object store for Hadoop

   The MapReduce & HDFS were inspired by [[https://books.google.com/books?id=axruBQAAQBAJ&pg=PA300][Google's papers on MapReduce and GFS]]


* Spark

** Docs
+ [[https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts][Spark Environment Variables and Config]]
+ Spark [[https://spark.apache.org/downloads.html][Downloads]] (version compatibility seems to be an issue)
+ [[https://spark.apache.org/docs/latest/sql-programming-guide.html][Spark SQL Guide]]

*** Architecture/Deployment
+ Cluster Mode

** Resources
*** Docker Images

+ [[https://hub.docker.com/r/apache/spark][apache/spark]]
+ [[https://hub.docker.com/r/apache/spark-py][apache/spark-py]]
+ alexmerced/spark3-3-iceburg0.14

*** Clojure
Implementations would depend highly on their ability for clojure primatives to
play nice with java classes and spark datatypes.

+ [[https://github.com/gorillalabs/sparkling/tree/v3.0.0][gorillalabs/sparkling]] (supports Spark 3.2.1 as of 2022/5/22)

** Topics
*** Concepts

The best documentation is the source code. Look at the [[https://github.com/apache/spark/tree/master/python/pyspark][pyspark]] code in
[[https://github.com/apache/spark][apache/spark]]. (okay ... maybe some more context would be needed)

+ [[https://github.com/apache/spark/blob/master/python/pyspark/sql/catalog.py][python/pyspark/sql/catalog.py]] contains classes
  - CatalogMetadata
  - Database
  - Table
  - Column
  - Function
+ [[https://github.com/apache/spark/blob/master/python/pyspark/sql/dataframe.py][python/pyspark/sql/dataframe.py]]
  - glue between spark sql results and pandas (see [[https://github.com/apache/spark/blob/master/python/pyspark/sql/tests/test_dataframe.py][tests]])
  - test setup creates a spark session using =SparkSession.builder=
+ [[https://github.com/apache/spark/blob/master/python/pyspark/context.py#L90-L99][python/pyspark/context.py]]

*** Minimum Requirements

So I've heard the Spark executors need about 3.8 cores or they refuse to run.

*** On Kubernetes

Apache Docs: [[https://spark.apache.org/docs/latest/running-on-kubernetes.html][Running Spark on Kubernetes]]

[[https://developer.hpe.com/blog/on-premise-adventures-how-to-build-an-apache-spark-lab-on-kubernetes/][On-Premise Adventures: How to build an Apache Spark lab on Kubernetes]]

This is overkill with 140 CPU's to run jupyter

**** [[https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwin5bnRq_n-AhVeF1kFHUh5CPQQFnoECAkQAQ&url=https%3A%2F%2Fpipekit.io%2Fblog%2Fargo-workflows-spark&usg=AOvVaw3k81M7pdWtBZE6wszF4QgI][How to use Argo Workflows with Spark]]

ArgoCD enables Git Ops, so something like this /in theory/ could run on [[https://homelab.khuedoan.com/][Khue's
Homelab]].

It could run simple datalake queries with:

+ 9-12 cores, executors running scala/python
+ 4+ cores for the scheduler/driver


* Zookeeper

** Docs
+ [[https://zookeeper.apache.org/doc/current/zookeeperStarted.html][Getting Started ...]]

** Resources

** Topics

*** Migrating ZK/Spark to K8S Cluster

[[https://product.hubspot.com/blog/zookeeper-to-kubernetes-migration][This post]] describes a gradual migration to bundle ZK and Spark (as ZK clients)
into a K8S cluster. They describe one migration path that's prone to forcing the
ZK cluster to continually re-elect while nodes are going down/up, then provide
an alternative:

+ They wrap the ZK cluster behind a K8S endpoint/service
+ Then, if using CNAME's, change the DNS to point to the K8S Services.
+ After checking on client connectivity & ZK clusters, begin swapping out the ZK
  nodes and reregistering them.

There's a one-to-one relationship between ZK the pods and the servers they
replace. They try to preserve the ZK server ID's in the new pods.

I wasn't sure whether it was possible/practical to run the ZK off the K8S
cluster, but it looks like it is. Depending on which arch/implementation details
pop up, I may just run the ZK cluster on-K8S, while running the Spark clients
off-K8S.

**** K8S, ZK for Kafka on GCP

The Kafka/K8S architecture seems to mix too many clusters, so that's what I was
considering on GCP until I kinda realized ... Kafka's not exactly a one-man
band.  Just way too much overhead no matter what you do whereas the pub/sub
would probably cover whatever I need.
