:PROPERTIES:
:ID:       4c531cd8-3f06-47fb-857a-e70603891ed8
:END:
#+title: Hadoop


* Roam
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
+ [[id:79d41758-7ad5-426a-9964-d3e4f5685e7e][Compute]]
+ [[id:ac2a1ae4-a695-4226-91f0-8386dc4d9b07][Devops]]

* Docs

+ [[wikipedia:Category:Hadoop][Hadoop Wiki]] (category)

* Resources

* Overview

From [[https://en.wikipedia.org/wiki/List_of_Apache_Software_Foundation_projects][List of Apache Software Foundation Projects]]. ASF provides a [[https://projects.apache.org/projects.html?category][better
categorization of its products]] or of [[https://incubator.apache.org/projects/][the products in the Apache incubator]].

|---------+-------------------------------------------+-------+--------------------------------|
| Product | Role                                      | Niche | Notes                          |
|---------+-------------------------------------------+-------+--------------------------------|
| Hadoop  | Map Reduce                                |       |                                |
| HBase   | Hadoop Database                           |       |                                |
| Knox    | REST API Gateway for Hadoop Services      |       |                                |
| Ozone   | Object store for Hadoop                   |       | scalable/redundant/distributed |
| Pig     | Platform for analyzing datasets on Hadoop |       |                                |
|---------+-------------------------------------------+-------+--------------------------------|

** Scheduling

|---------+---------------------+-------+-------|
| Product | Role                | Niche | Notes |
|---------+---------------------+-------+-------|
| Oozie   | Workflow scheduling |       |       |
|---------+---------------------+-------+-------|

** Security

|---------+------------------------------------------------+-------+-------------------------------|
| Product | Role                                           | Niche | Notes                         |
|---------+------------------------------------------------+-------+-------------------------------|
| Ranger  | comprehensive data security on Hadoop platform |       | enable/monitor/manage datasec |
|---------+------------------------------------------------+-------+-------------------------------|

** Provisioning and Automation

|-----------+--------------------------------------+-------+-------|
| Product   | Role                                 | Niche | Notes |
|-----------+--------------------------------------+-------+-------|
| Bigtop    | packaging/tests for Hadoop ecosystem |       |       |
| Ambari    | Cluster Provisioning                 |       |       |
| ZooKeeper | Configuration management             |       |       |
|-----------+--------------------------------------+-------+-------|

** Columnar formats

|------------+---------------------------+-----------------------+---------------------------------------|
| Product    | Role                      | Niche                 | Notes                                 |
|------------+---------------------------+-----------------------+---------------------------------------|
| Parquet    | Columnar data at rest     | general purpose, open |                                       |
| CarbonData | Format for fast analytics |                       |                                       |
| ORC        |                           | bigdata workloads     |                                       |
| RCFile     | defines                   |                       | defacto data storage structure format |
|------------+---------------------------+-----------------------+---------------------------------------|

** Serialization Formats

See [[https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats][Comparison data-serialization formats]]

** Columar databases

|-----------+----------------------------+--------------------------+-------|
| Product   | Role                       | Niche                    | Notes |
|-----------+----------------------------+--------------------------+-------|
| Cassandra | scalable columnar database |                          |       |
| HBase     |                            |                          |       |
| Accumulo  |                            | Security policy for data |       |
|-----------+----------------------------+--------------------------+-------|

** Query providers

|---------+----------------------------------+----------------+-------|
| Product | Role                             | Niche          | Notes |
|---------+----------------------------------+----------------+-------|
| Presto  | query heterogenous data services |                |       |
| Drill   |                                  |                |       |
| Arrow   |                                  | Queries in RAM |       |
|---------+----------------------------------+----------------+-------|

** File systems

|-----------------+------+--------------------------+----------------|
| Product         | Role | Niche                    | Notes          |
|-----------------+------+--------------------------+----------------|
| HDFS            |      |                          |                |
| Parascale FS    |      |                          | Parascale      |
| IBRIX Fusion FS |      |                          | HP             |
| MapR FS         |      | Random Access read/write | MapR Tech Inc. |
|-----------------+------+--------------------------+----------------|


* Design

** Data Placement

For requirements (motivating [[wiki:RCFile][RCFile design]])

1. Fast Data Loading
2. Fast Query Processing
3. Highly Efficient Space Utilization
4. Strong adaptivity to dyanmic data access patterns

** Hadoop framework

From [[https://en.wikipedia.org/wiki/Apache_Hadoop][Hadoop wiki]], the base Hadoop framework is composed of five modules:

1. Common: libraries and utilities
2. Hadoop Distributed File System: file system running on "commodity machines"
   designed to be accessed to produce results for mapreduce queries
3. YARN: manages computing resources and allocating them to be scheduled for
   workloads
4. MapReduce: implementation of the MapReduce programming model for large-scale
   data processing
5. Ozone (2020): object store for Hadoop

   The MapReduce & HDFS were inspired by [[https://books.google.com/books?id=axruBQAAQBAJ&pg=PA300][Google's papers on MapReduce and GFS]]

* Kafka

** Docs

** Resources
+ Docker Compose for [[https://zeppelin-kafka-connect-datagen.readthedocs.io/en/latest/][Kafka Broker/Connect & Zookeeper]]

** Topics

* Spark


** Docs
+ [[https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts][Spark Environment Variables and Config]]
+ Spark [[https://spark.apache.org/downloads.html][Downloads]] (version compatibility seems to be an issue)
+ [[https://spark.apache.org/docs/latest/sql-programming-guide.html][Spark SQL Guide]]

** Resources
*** Docker Images

+ [[https://hub.docker.com/r/apache/spark][apache/spark]]
+ [[https://hub.docker.com/r/apache/spark-py][apache/spark-py]]
+ alexmerced/spark3-3-iceburg0.14

*** Clojure
Implementations would depend highly on their ability for clojure primatives to
play nice with java classes and spark datatypes.

+ [[https://github.com/gorillalabs/sparkling/tree/v3.0.0][gorillalabs/sparkling]] (supports Spark 3.2.1 as of 2022/5/22)

** Topics
*** Concepts

The best documentation is the source code. Look at the [[https://github.com/apache/spark/tree/master/python/pyspark][pyspark]] code in
[[https://github.com/apache/spark][apache/spark]]. (okay ... maybe some more context would be needed)

+ [[https://github.com/apache/spark/blob/master/python/pyspark/sql/catalog.py][python/pyspark/sql/catalog.py]] contains classes
  - CatalogMetadata
  - Database
  - Table
  - Column
  - Function
+ [[https://github.com/apache/spark/blob/master/python/pyspark/sql/dataframe.py][python/pyspark/sql/dataframe.py]]
  - glue between spark sql results and pandas (see [[https://github.com/apache/spark/blob/master/python/pyspark/sql/tests/test_dataframe.py][tests]])
  - test setup creates a spark session using =SparkSession.builder=
+ [[https://github.com/apache/spark/blob/master/python/pyspark/context.py#L90-L99][python/pyspark/context.py]]

***
