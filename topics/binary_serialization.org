:PROPERTIES:
:ID:       c99b63b3-e18f-4b4b-8424-dbbac937b596
:END:
#+TITLE: Binary Serialization
#+DESCRIPTION: Protobuf, Thrift, Avro, Parquet, SBE, Cap'n Proto, Parquet
#+TAGS:
* Roam
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
+ [[id:0b80782f-92a8-4b48-958c-a41e7ff8713e][Data Lake]]

* Topics
* Issues

* Protobuf

** Docs

** Resources

** Topics

** Issues


* Flatbuffers

** Docs

** Resources

** Topics

** Issues

* Parquet

** Docs
+ [[https://arrow.apache.org/docs/python/parquet.html][Reading/Writing the Apache Parquet Format]]
  - [[https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html][pyarrow.parquet.ParquetDataset]]
+ [[https://spark.apache.org/docs/2.4.0/sql-data-sources-parquet.html][Spark SQL: Parquet Files]]

** Resources

** Tools
*** pqrs

Inspect parquet files (rust implementation)

*** dsq

Query JSON, CSV, Parquet.

+ Uses SQLite internally, so doesn't work for large datasets.
+ Does offer =ob-dsq= for emacs

** Topics

*** Format and Optimizations

From [[https://www.youtube.com/watch?v=1j8SdS7s_NY][Parquet Format and Perf Optimization]]

+ File-based Partitions (or network-based)
+ Predicate Pushdown
+ Row Group and Page Size
+ Tuning Dictionary Encoding
+ Delta Lake: optimize tuning/indexes/joins/views/imports/exports

  Parquet/ORC files are hybrid columnar & row-based

**** Encoding Schemes

*Plain*

2 encoding schemes:


*RLE_Dictionary* (run length encoding)

6+ encoding schemes.

+ RLE uses bit-packing and dictionary-based compression. The dictionaries are
  small.
+ Assumes Duplicate/Repeated values (string-substitution)
+ Parquet files with sections that exceed max dictionary sizes will fallback to
  plain (they drop compression)

**** Compression

+ Compression should be CPU-minimal. Picking the wrong algorithm can induce a
  lot of I/O. The perf gains are depending on the algorithm: snappy, gzip, lzip,
  etc.
+ Snappy seems to be used alot.

**** Row Groups

Row Group Size: 128MB (default)

+ Row group 0
  - Col A chunk 0
    - Page 0 ... N
  - Col B chunk 0
+ Row group 1 ... N
+ Footer

**** Pages

Page Size: 1MB (default)

+ Col X Chunk N
  - Page 0
    - Page metadata
    - Ref levels
    - Def levels
    - Encoded Values (dictionary)

*** ETL Optimiazation

Overhead for each file:

+ set up internal data strcucts
+ instantiate file readers (handlers)
+ fetch file (network)
+ parse parquet metadata

Use =df.repartition(n_parts)= or =df.coalesce(n_parts)= where applicable

Be aware of incremental workload output:

+ when ingesting parquet streams, you can't always predict the size, this can
  create large numbers of small files, where overhead from I/O and network
  exceeds any benefits.

Data sources like Delta Lake tables store their metadata and deltas in
json/parquet. These need to be =pruned= and then =vacuumed=

**** Vacuuming Data

Do NOT vacuum while data is being ingested.

To circumvent this, data normalization operations will either need
(speculation):

+ setup/load of a mirrored datasource and a coordinated switch-over (of DNS or
  source/sink configuration)
+ or just store the streams/files on multiple S3 buckets and pop one offline,
  normalize/vacuum the data while collecting a delta in a new source
+ or create an additional S3 bucket to accumulate the incoming streams while the
  other S3 buckets are taken offline. when the DLT table renormalization is
  complete, feed the old data into the new S3 buckets ... this would not
  necessarily require updating configuation in the data streams, the
  source/admin of which may not be under your control.

Whatever you do, it's dicey.

For a second, I was thinking CephFS would make this easier .... nope probably
not. Well maybe: it depends on whether you can move disk segments containing
files to an logical/named storage device where they are processed.

***** TODO ... honestly what do these do again?

**** Predicate Pushdown

+ Limit the query to the conditions/columns you need.
+ Take advantage of "materialized views" .......... where practical
+ This can avoid unnecessary compression/translation.
+ Data should be relatively sorted (and normalized, if it's a delta lake table)
+ Types should be explicitly specified.

Ensure =spark.sql.parquet.filterPushdown= or =parquet.filter.dictionary.enabled=
are set.

***** Metadata

Default metadata: min, max, count.

Metadata for parquet files is always read first:

+ For the file, then the row groups.
+ only rowgroups where metadata for columns are valud are read for the queries.

This is because the rowgroup and pages can be indexed in the file:

+ Don't use strings where possible unless they end up in the RLE dictionary.
  - if the sizes of strings can't be predicted, then the parquet reader must
    tract rowgroup & page starts/ends in metadata (it probably already needs to,
    since dictionary sizes can't be known at initial read time.)
+ And strings for structs will consume memory in the client once parsed.

*** Row-based vs Columnar

Columnar creates the opportunity for compression (when queries can't easily
determine the structure in which they'll return results)

**** Low Level Performance

Row-based leads to fragmented memory-access patterns, which tends to lead to CPU
cache invalidation. And who doesn't love their CPU cache on a VM-in-a-VPS (with
NUMA properly configured of course) ... containers perhaps not eligible without
sufficient =niceness=.



* Misc

+ Thrift
+ Apache Avro
+ SBE
+ Cap'n Proto
