:PROPERTIES:
:ID:       c99b63b3-e18f-4b4b-8424-dbbac937b596
:END:
#+TITLE: Data Serialization
#+DESCRIPTION: Protobuf, Thrift, Avro, Parquet, SBE, Cap'n Proto, Parquet
#+TAGS:

For Web 2.5 or Data Engineering, Protobuf is like the "varnish" that you put
onto a finished oil painting. You don't add it until you're confident it's not
going to change ... but you're confident because you're in the top of your
field. Otherwise, you're asking for problems.

So protobuf and other wire protocols help to standardize structure so tasks like
these are easy:

+ how data is unserialized for analysis in xlang
+ how data may be operated on in transit (like decrementing TTL on a IP packet)

Wire protocols are used all over the place in

+ crypto :: doesn't like unnecessary zeros or unintentional compression/patterns
+ science :: elitists are stuck in the 1970's and don't want curious people or
  science journalists reading their custom TIFF formats. yeh good luck getting
  that [[https://www.gebco.net/data_and_products/gridded_bathymetry_data/][GEBCO bathymetry data]] or some [[https://nsidc.org/data/user-resources/help-center/what-netcdf][Climate Data NetCDF]] file into Excel. "You
  must be at least this credentialed to ride ... or be a programmer."

And otherwise baby bird, you'll be working with data sets that are predigested &
regurgitated. viz. without tools like python/etc, the data is perhaps

+ prefiltered
+ cannot be unrolled/disagregated
+ or cannot be analyzed for coverage/sampling/ETL problems or bias

* Human

+ Quick comparison [[https://www.zionandzion.com/json-vs-xml-vs-toml-vs-cson-vs-yaml/][JSON vs XML vs TOML vs CSON vs YAML]]

** Tools

+ [[https://github.com/tyleradams/json-toolkit][tyleradams/json-toolkit]] a many to many library of simple conversion tools,
  mostly python (& standalone), req. go for two tools.


** YAML

* Binary
** Protobuf

Uses variable length integers -- like the MNIST datasets or oldschool science
datasets. See [[https://github.com/dcunited001/handwritten-digits/blob/master/src/digits/mnist.clj#L5-L6][dcunited001/handwritten-digits]] (2013) for an example of when
machine-learning was hard.

#+begin_quote
[[https://github.com/dcunited001/handwritten-digits/blob/master/src/digits/net.clj#L113-L117][I never did finish:]]

+ Wolfe Conditions
+ Polack-Ribiere flavor of conjugate gradients
+ line-search using quadratic/polynomial approximations
+ Wolfe-Powell stopping criteria & slope ratio
  - used for guessing initial step sizes

As it turns out, the Coursera ML class was just "Linear Algebra 2"
#+end_quote

*** Docs

*** Resources
+ [[https://github.com/kinow/protobuf-uml-diagram/][kinow/protobuf-uml-diagram]]

*** Topics

*** Issues


** Flatbuffers

+ Native length integers

*** Docs

*** Resources

*** Topics

*** Issues

** Parquet

*** Docs
+ [[https://arrow.apache.org/docs/python/parquet.html][Reading/Writing the Apache Parquet Format]]
  - [[https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html][pyarrow.parquet.ParquetDataset]]
+ [[https://spark.apache.org/docs/2.4.0/sql-data-sources-parquet.html][Spark SQL: Parquet Files]]

*** Resources

*** Tools
**** pqrs

Inspect parquet files (rust implementation)

**** dsq

Query JSON, CSV, Parquet.

+ Uses SQLite internally, so doesn't work for large datasets.
+ Does offer =ob-dsq= for emacs

*** Topics

**** Format and Optimizations

+ [[https://www.kaggle.com/code/aapokossi/how-to-save-parquet-data-as-ragged-tf-dataset][How to save parquet data as ragged tf.Dataset]]


***** From [[https://www.youtube.com/watch?v=1j8SdS7s_NY][Parquet Format and Perf Optimization]]

+ File-based Partitions (or network-based)
+ Predicate Pushdown
+ Row Group and Page Size
+ Tuning Dictionary Encoding
+ Delta Lake: optimize tuning/indexes/joins/views/imports/exports

  Parquet/ORC files are hybrid columnar & row-based

****** Encoding Schemes

*Plain*

2 encoding schemes:


*RLE_Dictionary* (run length encoding)

6+ encoding schemes.

+ RLE uses bit-packing and dictionary-based compression. The dictionaries are
  small.
+ Assumes Duplicate/Repeated values (string-substitution)
+ Parquet files with sections that exceed max dictionary sizes will fallback to
  plain (they drop compression)

****** Compression

+ Compression should be CPU-minimal. Picking the wrong algorithm can induce a
  lot of I/O. The perf gains are depending on the algorithm: snappy, gzip, lzip,
  etc.
+ Snappy seems to be used alot.

****** Row Groups

Row Group Size: 128MB (default)

+ Row group 0
  - Col A chunk 0
    - Page 0 ... N
  - Col B chunk 0
+ Row group 1 ... N
+ Footer

****** Pages

Page Size: 1MB (default)

+ Col X Chunk N
  - Page 0
    - Page metadata
    - Ref levels
    - Def levels
    - Encoded Values (dictionary)

**** ETL Optimiazation

Overhead for each file:

+ set up internal data strcucts
+ instantiate file readers (handlers)
+ fetch file (network)
+ parse parquet metadata

Use =df.repartition(n_parts)= or =df.coalesce(n_parts)= where applicable

Be aware of incremental workload output:

+ when ingesting parquet streams, you can't always predict the size, this can
  create large numbers of small files, where overhead from I/O and network
  exceeds any benefits.

Data sources like Delta Lake tables store their metadata and deltas in
json/parquet. These need to be =pruned= and then =vacuumed=

***** Vacuuming Data

Do NOT vacuum while data is being ingested.

To circumvent this, data normalization operations will either need
(speculation):

+ setup/load of a mirrored datasource and a coordinated switch-over (of DNS or
  source/sink configuration)
+ or just store the streams/files on multiple S3 buckets and pop one offline,
  normalize/vacuum the data while collecting a delta in a new source
+ or create an additional S3 bucket to accumulate the incoming streams while the
  other S3 buckets are taken offline. when the DLT table renormalization is
  complete, feed the old data into the new S3 buckets ... this would not
  necessarily require updating configuation in the data streams, the
  source/admin of which may not be under your control.

Whatever you do, it's dicey.

For a second, I was thinking CephFS would make this easier .... nope probably
not. Well maybe: it depends on whether you can move disk segments containing
files to an logical/named storage device where they are processed.

****** TODO ... honestly what do these do again?

***** Predicate Pushdown

+ Limit the query to the conditions/columns you need.
+ Take advantage of "materialized views" .......... where practical
+ This can avoid unnecessary compression/translation.
+ Data should be relatively sorted (and normalized, if it's a delta lake table)
+ Types should be explicitly specified.

Ensure =spark.sql.parquet.filterPushdown= or =parquet.filter.dictionary.enabled=
are set.

****** Metadata

Default metadata: min, max, count.

Metadata for parquet files is always read first:

+ For the file, then the row groups.
+ only rowgroups where metadata for columns are valud are read for the queries.

This is because the rowgroup and pages can be indexed in the file:

+ Don't use strings where possible unless they end up in the RLE dictionary.
  - if the sizes of strings can't be predicted, then the parquet reader must
    tract rowgroup & page starts/ends in metadata (it probably already needs to,
    since dictionary sizes can't be known at initial read time.)
+ And strings for structs will consume memory in the client once parsed.

**** Row-based vs Columnar

Columnar creates the opportunity for compression (when queries can't easily
determine the structure in which they'll return results)

***** Low Level Performance

Row-based leads to fragmented memory-access patterns, which tends to lead to CPU
cache invalidation. And who doesn't love their CPU cache on a VM-in-a-VPS (with
NUMA properly configured of course) ... containers perhaps not eligible without
sufficient =niceness=.



** Misc Formats

+ Thrift
+ Apache Avro
+ SBE
+ Cap'n Proto

* Roam
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
+ [[id:0b80782f-92a8-4b48-958c-a41e7ff8713e][Data Lake]]
+ [[id:e0880f60-63db-4f34-b478-c3b733f1ab96][XML]]
