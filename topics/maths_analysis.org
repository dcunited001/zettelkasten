:PROPERTIES:
:ID:       a0ef7bfe-1587-4fec-ac87-f7dda5dc0d21
:END:
#+TITLE: Maths: Analysis
#+DESCRIPTION: The Shapes of Clouds and Stuff
#+TAGS:

* Docs
+ [[wikipedia:Mathematical_analysis][wikipedia:Mathematical_analysis]]

* Techniques

** Image Stitching

This [[https://www.youtube.com/watch?v=SSRUOIAydaI&t=600s][video about Merlin]] is a bit weird, but has a fascinating video effect that
seems to be based on image stitching across time. It is perhaps using lower
level computer vision features to cause segments of the image to appear to move
at different rates of time. The effect takes into account the Sobel filter
(contrast map) and benefits from the lighting used in the video, the latter of
which causes sharp edges (of either columns or skulls) and noise patterns (from
surface textures) to be relatively preserved in the rendered 2D image space. The
effect is first clearly understood when you see that the smoke @10:00 is moving
backwards.

The red stitching in this image connect points where there is minimal change in
the overlapping images. If you imagine a 3D stack of video frames moving through
time (in other words, 2,1 spacetime), the video effect is doing something where
you have analogous image regions that are segmented in a similar way. The
regions where there are minimal changes seem to be preserved across time.

[[file:img/image-stitching.jpg]]

* Modular Forms

** Docs

** Resources

+ [[https://math.vanderbilt.edu/rolenl/ModularForms.html][Lectures for Vanderbilt 9800]] (2020)
*** Stein

+ Stein [[https://www.youtube.com/watch?app=desktop&v=AxPwhJTHxSg][Counting points on the E8 Lattice with Modular Forms (theta functions)]]
+ Stein [[https://wstein.org/books/ribet-stein/][Lectures on Modular Forms and Heck Operators]] ([[https://github.com/williamstein/ribet-stein-modforms][source]])
+ [[https://wstein.org/books/modform/modform/][Modular Forms a Computational Approach (2006)]]
+ LMFDB: [[http://www.lmfdb.org/][L-functions, Modular Forms and Friends DB]]

Stein has also written a book on the Birch Swinnerton-Dyer Conjecture, but
... just run.

*** Lattices

+ Visualizations of [[http://theoryofeverything.org/theToE/2020/05/04/nested-lattices-of-e8/][E8 Root Lattice]] at ToE. These include Useful values when
  working with modular forms involving these lattice groups.

** Topics

*** Arithemetic

**** Dragons? Where?

With high-dimensional spaces, esp. involving complex numbers, then (i believe)
interpolation techniques fail for multilinear & nonlinear equations. worse,
number-theoretic problems you may encounter are *infective* with
high-dimensional data. The dot product isn't so bad with vectors... but esp with
iteration, then outer products or matrix multiplication can quickly accumulate
problems.

It would help to work with systems numerically (symbolically) where possible,
but it is not always possible. This is really way out of my league... but in the
past few days I've been thinking about:

+ difficulties I may encounter when animating modular forms using the GPU
+ ... which I would've just barely tapped into when calculating the Jacobion to
  animate the gradient.

Frederik Johannson covers

+ [[https://fredrikj.net/][Arb & Calcium merged into FLINT]] (See [[https://fredrikj.net/blog/2014/10/modular-forms-in-arb/][Modular Forms in Arb]], which uses
  [[https://arblib.org/acb_modular.html][acb_modular.h]]). He alludes to arbitrary prescision ball arithmetic here (2014)
  and published [[https://arxiv.org/abs/1611.02831][arXiv:1611.02831]]
+ And goes on to publish libraries on Hackage and apparently has taught a
  bit about it: [[https://fredrikj.net/math/berkeley2019.pdf][Progress in arbitrary-precision ball arithmetic: numerical
  integration and faster arithmetic]]

*** Lattices

+ self-dual if $disc\left(\Lambda\right) = 1 = det\left(G\right)$ where $G$ is
  the Gram Matrix.







* Topics

**** FIXME test HTML export on Github for citar citation

Does cool thing with cross product [cite:@rossling-2022-wang-algeb-from]

** Combinatorial Optimization

*** Wang Algebra

[[https://arxiv.org/pdf/2208.09649][Wang Algebra: From Theory to Practice]]

The wang algebra was developed to solve systems of equations representing flows
of electricity in circuits where each loop's overall impact on the others needs
to balance out.

Since it applies to the polynomials one might generate from a graph -- using a
matroid optimization method -- then you might find that it's relevant to the ole
"coin counting problem." You know, the one you solved in CS in High School
in 2002. I'm bored. Anyways, this problem is an interesting combination of
modular arithmatic and linear algebra.

+ All coin counts must be positive. \(n_i > 0\) and \(W_i > 0\), so we only need
  to consider the positive "quadrant" of an n-dimensional space where the axes
  correspond to $n_i$
+ \[V(W,N) = \sum_{i}^{m} W_i n_i\] for weights and number of coins. Notice the
  positivity, again.

What shape does $V(W,N)$ take when parameterized by the integer domain of the
space? It is always increasing in one particular direction which isn't precisely
"in the same direction", since $n_i$ can only take integer values. So this is
the prototypical greedy algorithm.

After trying to figure out how to structure products/sums to arrive at a dual
basis for \(W\prime\) that would subtract out the diagonal from the outer
product of \(n \times W \) as zero ... it just felt like something connected to
geometric algebra, which is a cop out, since I can't explain it. You already
know you'd like \(V_\$ = W \cdot n\), but that's the answer. What other
information is available in the combinations of numbers that informs your
exploration of the space? ... Though for problems involving greedy optimization,
this is pretty mundane.

The outer product doesn't really help here. I can't really bridge what I tried
to Wang Algebra ... but it is related to combinatorial optimization since it
generates trees/cotrees of a graph. Furthermore, it is somehow related to the
Grassman Algebra.

I tried learning about matroids around 2016ish ... but without help, structure,
gradual reinforcements or foundations, concepts over time tend to blur
together. if i didn't think there was some undeveloped talent or that i could
apply math to develop models for understanding personal/social/philosophical
problems, then what's the purpose behind being such a tryhard? people have
continually told me to give up throughout my entire adult life, even when they
mean the best.

At least not without repetition and structure/order to learning. it's just not
efficient for me differentiate between set-theoretic objects like what's below.
Much of this stuff is just foundational for what's actually interesting.

|                              | Application                                                         |
|------------------------------+---------------------------------------------------------------------|
| Power Set                    | The basics                                                          |
| Hom Set                      | Anything Combinatorial in Category Theory without being overwhelmed |
| Borel Set & Sigma Algebra    | Relating measure on spaces to enable statistics.                    |
| Independent Sets of Matroids | Graphs -> polynomials, Optimization, portfolio mgmt, game theory    |
| Topology                     | Functional Analysis                                                 |
| Simplicial Complices         | Generalized Covariant Derivative                                    |

Sigma Algebra is necessary for any stocastic process. Also, when deciding
between Lebesgue or Reimann integrals, a theoretical physicist needs strong
grounding in the same ideas to ensure measure is preserved. For experimental
physics or anomaly detection -- like when determining whether it's a "6-sigma
event" -- the ability to infer, extrapolate or parameterize events from data
depends on your ability to extract as many insights as possible from redundant
measures on the Borel Set.

Try grouping/counting all unique simplicial complices. The generalized covariant
derivative can be used for Calculus with Groupoids (eventually getting to
whitehead towers) or for calculus on networks where you have criticality or
phase changes. It's hard to see how these are related ... but if you figure it
out, then the Covariant Derivative, Cristoffel symbols and Ricci Tensors should
be much easier in General Relativity.

Matroids aren't the only objects that can transform to/from systems of
equations. Generally, any object can be represented in matrix form. Block
diagrams secretly shapeshift into matrices. Orthogonal polynomial systems are
also generally useful. If you learn to generally transform discrete math into
equations, then it should be simple to turn an conceptual diagram into:

+ Tensor-based multilinear systems
+ A tensor algebra with rules for solving right-brained problems like CCG's [cite:@kartsaklisramgoolamsadrzadeh-2017-linguis-matrix-theor]:
  - see [[https://arxiv.org/pdf/1703.10252.pdf][Linguistic Matrix Theory]]
+ Tensor decompositions (using permutation/communication matrices, lattices or
  sparse matrices)

Or maybe you can't, but you discover that autocomplete in Jupyter magically
shows you names of methods you have some familiarity with and, as long as
they're not slow, you can hunt and peck for the math you need to rake in
$200,000/year.

That probably doesn't work out in the long-run, but if I was a painter, I'd know
my pigments and colors. Some of those don't mix/match. If someone wrote a
library, you can write an application that's about as easy as the MLOps -- only
if scaling is important. Otherwise, you have to be skilled at what other people
believe is difficult in order to be creative.

* References
* Roam
+ [[id:a24b12f8-b3e3-4f66-9a5c-f29b715e1506][Math]]
