:PROPERTIES:
:ID:       a24b12f8-b3e3-4f66-9a5c-f29b715e1506
:END:
#+TITLE: Math


* Resources

** LaTeX

+ [[https://am121.seas.harvard.edu/site/wp-content/uploads/2014/08/latex_snippets.pdf][Useful LaTeX Code Snippets]]


* Oidification (ncatlab)

** Categories (ncatlab)

*** Differential/Integral

Differential algebra/[[https://ncatlab.org/nlab/show/differential+algebroid][algebroid]]

+ Examples:
  + Treesitter :: syntax highlighting by connecting (δ-text -> δ-AST) to restructure
    +

** Algebras (ncatlab)


* Differential Geometry




* Machine Learning

** Kernels

*** Notes from [[https://www.youtube.com/watch?v=MtZV82LCNHc&t=2105s&pp=ygUpUmllbWFubmlhbiBtYW5pZm9sZHMsIGtlcm5lbHMgYW5kIGxlYXJuaW4%3D][Riemannian manifolds, kernels and learning]]

A kernel is basically a similarity measure and in some cases equates to the
inner product.

+ When a symmetric kernel is positive definite, then it is the inner product.
+ Usually involves a quadratic (non-linear) methods. For example, the inner
  product req. two arguments.

**** Applications

+ Kernel SVM
+ Kernel PCA
+ Kernel Fisher Discriminant Analysis

**** Positive Definite Kernel

A kernel K is positive definite if, for all reals $c_i$ and all choices of $X_i,X_j$

\(
\sum_{i=1}^n{c_i c_j K\left( X_i,X_j \right)} \geq 0
\)

Thm: If a symmetric kernel is positive definite, then it is just like an inner
product: there exists a map \(\phi : X \rightarrow H\) to a hilbert space s.t.:

\(
K\left(X,Y\right) = \langle \phi(X) , \phi(Y)\rangle_H
\)

**** Radial Basis Kernel

This is always a pos/def kernel for all $\sigma$ if $||\cdot{}||$ is a norm in a
hilbert space.

\(
K(x,y) = e^{\frac{||x-y||^2}{\sigma^2}}= e^{\frac{-d(x,y)^2}{\sigma^2}}
\)

where d is a distance function defined on a set S (metric space).

Thm: the RDF K(X,Y) is pos/def for all $\sigma$ iff S can be isometrically
embedded in a Hilbert Space:

\(d(X,Y) = ||\phi(X),\phi(Y)||_H\)

A Banach Space is not sufficient (the inner product is necessary)

***** Examples

The chordal distance on a sphere is embedded in Hilbert H, so:

\(d(X,Y) = 2sin(\frac{\theta}{2})\) gives kernel \(exp(\frac{d^2}{\sigma^2})\)

The geodesic (arc) diestance \(d(X,Y) = arccos(X,Y) = \theta\) does not

**** Positive Definite Matrices

These are encountered in Computer Vision.

+ Pos/def $n \times n$ form a cone (not a lin. subspace)
+ Affine invariance \(d(X,Y) = d(A^{T}XA,A^{T}YA)\)

It is possible to define an "affine invariant" Riemannian metric (but not
possible to run it in Tensorflow Lite!)

Other Metrics:

+ Logarithm  \(d(X,Y) = || log(X) - log(Y)||_F\)
+ Stein Metric   \(d(X,Y)^2 = || -log(det(XY)) + 2log(\frac{det(X+Y)}{2})  ||\)

Kernels on Pos/Def Matrices.

| Metric Name         | Formula | Geodesic Distance | Pos/Def Gaussian Kernel |
| Log-Euclidean       |         | Yes               | yes                     |
| Affine Invariant    |         | Yes               |                         |
| Cholesky            |         |                   | Yes                     |
| Power Euclidean     |         |                   | Yes                     |
| Root Stein Distance |         |                   |                         |

At least some of the above can be used with varying levels of success for
Manifold-based k-means and kernel k-means clustering.

**** Positive Sym Definite

DTI segmentation is shown. "Diffusion tensor at the voxel is directly used as
the descriptor"

Kernel k-means is used to cluster points on $Sym^{+}_d$, yielding
segmentation. The Riemannian kernel beats the shit out of the Euclidean kernel,
but Fractional Anisotropy yields the best results.

**** Dictionary Learning

Computer vision application, using Grassman Manifolds -- i.e. it uses
Geometric algebra. I have the Part 2 textbook, but Part 1 coverse GA for CV.

The method for dictionary learning is presented and then again as a kernelized
version.

***** Grassman manifold

A manifold composed of all linear subspaces of given dimension. That is these
are the classic subspaces that I would term "the useless subspaces" that all
include the origin and form groups/subgroups of the space from which they're
formed.

#+begin_quote
Applying this must require some kind of "affine wizardry" that has thus far
escaped me -- as has any person who might understand any question I have on
these subjects. But alas! the socialists must have some noble reason for
keeping me under their post-modern clown boots.
#+end_quote

** On Manifolds

From [[https://www.youtube.com/watch?v=ELo2xBRxzCM&t=1906s][Noémie Jaquier - Bayesian optimization on Riemannian manifolds for robot learning]]

... basically: why not all regions of explicit geometry are equivalent for their
implicit parameter values.



.... and i guess Christoffel symbols are necessary for any differentiation on a
Riemannian manifold =np.doitforme('abc')=

* Statistics

** Geometric Median

*** Weiszfeld's Algorithm

And efficient method for computing the [[https://en.wikipedia.org/wiki/Geometric_median#Computation][geometric median]]. A little tricky when
computing on a manifold: req. mapping back & forth using exponential/logarithm
maps.

* Interesting Problems
