:PROPERTIES:
:ID:       cea7d11c-8357-4e4f-90b3-fa8210eff796
:END:
#+title: AI

* Docs

* Resources

* Communities
+ [[https://ai.science/][AI.science]] (slack)

* Ideas

** Topology of the Hyperplane

*** Amateur Go Player Beats Top Go AI With 100% Win Rate

Sources:

+ [[The HUGE Problem with ChatGPT]]
+ [[github:HumanCompatibleAI/go_attack][HumanCompatibleAI/go_attack]]
+ [[https://arxiv.org/abs/2211.00241][Adversarial Policies Beat Superhuman Go AIs]]

So basically, the AI is not aware of the topology of the hyperplane. This is why
the strategy to beat the AI involves ... the topology (or shape) of the
arrangement of Go pieces on the board.

Since the AI algorithms for Go utilize information compression to render the
game of Go tractible, then they are unlikely to be adapted to deal with this
without being augmented with another technique -- such as the algorithm attained
by the MIT researchers themselves.

**** This "Topological perspective" also explains LLM Jailbreaks

This also explains jailbreak techniques and why it's so difficult to guard
against them. Because it's difficult to define regions of the hyperplane so that
they are "convex" for some loose definition of the term, it's also difficult to
construct an XAI policy to wall those regions off. The structure of the
hyperplane for language-based models probably moreso resembles swiss chess than
anything that can be "convexified".

That is, the regions of the hyperplane (or a LLM's state space, if I'm mincing
terminology) form a kind of labyrinth. Your interactions with it lead it down
various paths. The space represents language, conversation and interaction
state, Because of the connectedness of this space (it is Riemannian), then given
its n-dimensional holes, then there are no simple way to define XAI-based
semantic policy to prevent someone's interaction path from reaching certain
regions of the space -- not without significantly altering the behavior of the
AI.

Just like you can flip a convex function into a concave function you could
perhaps invert this behavior: simply define interaction paths starting from a
point within a boundary and limit the paths leaving that bounded
region. However, doing so will significantly alter the behavior of the AI. To
further confound this, it requires prohibitive levels of computation /and time
lag introduced from ML ops to update the LLM policies from XAI techniques./

And convexity/concavity are extremely complicated to define even in lower
n-dimensional Euclidean space (with or without the need for variational
methods). Here your topological concerns are mostly incidential to the functions
you're analyzing, not the connectedness of manifolds or the definition of
metrics to determine some arbitrary "increase/decrease." The definitions of
convexity here are based on derivation of functions or other tools -- anyways,
is a saddle point concave or convex? How about a local minima in a 9-D subspace
of an function on 11-D Euclidian space? It's totally convex in 9-D? What is it
then in 11-D? When this happens in Riemannian manifolds, it creates
wormhole-like structures: hence the "swiss" in the swiss cheese I menioned
above.

**** Thence:

[[https://te.xel.io/posts/2019-04-12-the-analytic-iching-a-novel-exegesis-for-the-age-of-data-science.html#the-algebraic-topology-of-hyperplanes][The Algebraic Topology of Hyperplanes … Donuts My Mind May Never Digest]] where I
describe the need for a "Gauss Bonnet Theorum" to analyze the structure of the
hyperplane and define such analysis as metacognition:

#+begin_quote
... reasoning about the topological features of the hyperplane corresponds to metacognition.
#+end_quote

From the bundle of crazy I wrote in 2019, [[https://te.xel.io/posts/2019-04-12-the-analytic-iching-a-novel-exegesis-for-the-age-of-data-science.html][The Analytic I Ching: A Novel Exegesis
In The Age of Data Science]].

... But then again, I'm not even allowed to exist. Or, at least, my name is not
showing up on TV or on some influencer's channels. And no, I didn't make it, but
once someone's life has been destroyed, network theory and social physics
implies that fixing it is a bit like escaping a black hole.

**** So I Opens the article from MIT

And what do you know: [[https://arxiv.org/pdf/2211.00241.pdf][they didn't cite me]]. I've only ever been cited for a 51/50
TDO (for short-term psychiatric care).

#+begin_quote
There are a number of situations that are known to be challenging for computer
Go players. Some can be countered through targeted modifications and additions
to the model archi- tecture or training, however, as we see with Cyclic
Topology, it is difficult to design and implement solutions one-by-one to fix
every possibility. Further, the weaknesses may be unknown or not clearly
understood – for instance, *Cyclic Topology* is normally rare, but through our
work we now know it can be produced consistently. Thus, it is critical to
develop algorithmic approaches for detecting weaknesses, and eventually for
fixing them
#+end_quote

Other failure modes include:

+ Ladders :: the phenomena ever beginner go player recognizes as the thing
  indicates a "problem." Democrats still haven't figured this one out. This is
  to some extent topological because, speaking in strategery of course, there
  really is an "edge" of the board.
+ Complicated Openings :: Hmmmm
+ Cyclic Topology :: The researchers seem to be real worried about the swiss
  cheese.
+ Mirror Go :: Symmetry? That's topology. It's possible that mirroring the AI's
  moves could actually glitch the AI and produce tactical gains from more than
  simply the change in the board.

** Art
*** IDEA ideas for Krita plugins in python

**** AI assisted drawing

a few ideas but mainly just keys to flip between your painting and
overlays/side-panels that help illustrate "what the AI thinks you're painting."
there are many ways to interpret that and many ways to use something like
this.

it's hard to say really, but what I'm thinking is that as you either give it
reference images or supply tags, perhaps with a dynamic tag cloud that can be
toggled to shift it's perception. as you lay enough strokes down , the
possibility space narrows down so that the AI can:

+ display various visualizations of the collapsed possibility space. this would
  probably be an overlay where the color channels are distorted where there is
  dissonance.

It would just wouldn't be what I'm thinking if it does the work for you. The
point is to get visual feedback? ... i donno. It also doesn't work well with
interpreting styles. The AI would also need to retain its state ... and yet it
needs to refresh its state or its apprehension/perception of what you're
drawing/making. i'm not sure you can program/condition a artificial mind that
can "hold on to" or "regain" its zen. not without resetting itself.


**** Hausdorf Space

.... great. Now, I've gotten on thinking about this.

Thinking about the present range of possibilities (potential volumes surrounding
cluster centroids) and comparing it with the future range of possibilities as
the network's state changes can benefit from the Hausdorf Metric on Reimanninan
manifolds. Given its current state (and path in the recent path), if the network
were to sample from its own parameter space to determine how its state will
evolve, it needs to measure the volume of a ball surrounding points of interest
(think functional analysis). The holes, curves, distances and directions in
high-dimensional Riemannian manifolds cause changes in the volume of the ball --
probability is like an incompressible fluid.

The purpose of developing a sense of this measure is to determine whether its
confidence in producing values is likely to change given new information. As you
begin to draw something, there is almost no information, the AI network's state
is very under-determined (or underfit, though this is not the models training
but its state). The possibility space is shaped by the features that channel
"probability fluids" motion. Considering the cluster centroids for image class
candidates that the AI considers reasonable, if the packets of fluid surrounding
these start to converge towards each other or dissipate entirely, it signifies
that a change in belief is necessary. The packets of fluid represent the the
"likelihood" volume under the probability curve.

To model this, you need to /something like/ their Hausdorff dimension calculated
pairwise (since it's a sum of pairwise products, it's very similar to tensor
contraction, but at a higher order than the polytopes in this paper ... which I
don't fully understand. It does nominally involve the Hausdorff space. It is
apropos, since it models a measure for predicting reasonable ranges for accuracy
that simple K-means clustering network will arrive at.

[[Neural Network Approximation based on Hausdorff distance of Tropical Zonotopes ]]
(2022)

#+begin_quote
Therefore, it is expected that two tropical polynomials with approximately equal
extended Newton polytopes should attain similar values. In fact, this serves as
the intuition for our theorem. The metric we use to define the distance between
extended Newton polytopes is the Hausdorff distance.
#+end_quote

Sudden motion of the probability fluid, even when it seems to move with
convergence, doesn't actually mean that the expected image class prediction at
that time will become the AI network's final belief. The less that's on the
paper, the more broadly the range of classes would need to be considered.

In other words, AI (or most any person) would need to counteract the tendency to
assume that what seems to be developing actually represents where things are
going (or what beliefs are reasonable, etc). The more that someone narrows their
mind to follow "the recent past as the best predictor", the more strongly that
"path through belief-space" itself shapes the beliefs they form in the end. This
sorta relates to the concept of "zen" as I understand it, since the more
information/experience our minds accumulate, the more engrained of an effect the
"default mode network" has in constructing the "constraints" we put on what we
believe to be...

Jeez, i guess you can really just hyphen-space anything now.

* Topics

** Politiics
*** Have we seen the left held accountable for any of their failures? Why would AI/Climate be any different?

Being perpetually alienated for voting wrong really sucks.

How many silicon valley banks have to collapse before the liberals/moderates
admit that they have no idea what they're doing and the only reason they don't
lose elections is because they force everyone to focus on trivial cultural
issues to hack the vote (while derelicting their duty on AI/Climate). They can
set the country on fire with impunity.

Well who created AI? Have you been held accountable for anything? What are the
odds that we can hold you accountable for failing to act on AI policy after
having created it?

You suck. Good luck with that though. I'm glad we can legitimately blame you for
everything that happens, but I would rather have real leaders who do real things
that matter who are in power in Washington. We weren't allowed to have a leader
during Coronavirus though. Why? Because liberals can't stand up to leftists,
since they depend on leftist games to hack the vote.

** Economic
*** Restructuring the Services Sector

Many business in various service sectors enjoy competitive advantage because of
the social networks composed of their employees in the businesses'
hierarchies. However, in order to measure or project their cost/pricing
structures, these businesses need to make expectations about revenue. If AI
opens access to knowledge/experience, it undermines the value that employees in
various roles contribute to their organizations/departments. This leads to
consolidation of labor/role and then reorganization of corporations. It affects
the dynamics that determine how competitive businesses are in various sectors
while either reducing or increasing competitiveness of large players.

The disruption will lead to corporate protectionism, but may first cause
reorganization of white-collar labor/roles within these businesses. If so, there
may be quite a few disaffected employees who were no longer seen as contributing
value. If people are laid off en masse from various roles, then not only do they
face a more competitive labor market (and require social services), but they may
find that starting a small company makes more sense. The larger companies who
lay off will also constrict their access to social networks adjacent to their
former employees. Lacking some other compensating means for branching out
through society, these organizations will have lost their bridges out into
social networks in addition to the industry-specific or niche-specific
experience/knowledge that their former employees contributed to the
organization. Since these quantities are not easily measured (i.e. they are
qualitative) it is likely that micro/macro-economic strain would cause some
organizations to turnover employees quickly without necessarily hiring
replacements. The replacements may not be as experienced.

So, while this is speculative and complicated, I do think the social
mechanisms/dynamics need to be considered. Corporations and organizations need
to reconsider what their value truly is and where it truly comes from.
Especially when they are service-oriented, their products which are not
brick-and-mortar, are much more difficult to price or valuate. Many
organizations do not realize how important their people are. Corporations have
legal rights -- à la Citizens United and other cases -- but they are not
people. Their roles in the wider economy are socially mediated -- that is, what
they provide to people affects people and the effects must in some way ripple
through social networks. You can construct the most efficient enterprise,
quantatively, but when you remove the people from your organization, determining
how the social waves propagate through society is a bit like thinking about
acoustics in a vacuum: there are many fewer particles which propagate their
energy. The rules that previously may determine what happens become far less
known or measurable. What actually results and how it plays out is probably not
so hyperbolic as the acoustics in a vacuum. Furthermore, one must consider the
complement: there may be quite a few people who relate to each other as
displaced more than they do to any organizations.

#+begin_quote
s/o to Lovecraft who once referenced auditory distortions you might hear when
entering/leaving a vacuum, the fucking genius. Though wherever that was, I can't
remember and it's likely as not obvious that it's a reference to that.
#+end_quote

It may actually be that America must return to the industrialized production of
tangible goods. Much of this can be automated, but it usually involves physical
space and matter (as opposed to cyberspace and logic). Transforming the physical
world more clearly indicates economic value IMO than most services which flip
bits on servers in some data center, the halls of which very few will ever
walk.

Many of these businesses can only "contribute to GDP" by influencing consumer or
business behavior -- this only matters when consumers or businesses have access
to liquidity. You wouldn't market most of what is advertised to people who can't
possibly buy it unless those people could influence the behavior of others --
which is what? /Socially mediated/, but marketing to people /who may indirectly
influence other people/ is worrying about second-order effects -- and these have
at least two condititional probability conditions to satisfy before they result
in real-world effects. That's the thing about higher-order tactics/strategy:
without compensating in some way, it is almost one-to-one bayesian statistics in
a fairly depressing manner. Multiply any three numbers whose value is less than
one ... there's never much probability left, is there? Finding ways to integrate
across conditions requires energy/money.

It doesn't look good for the services sectors and marketing depends on the
exchange of real goods, at some level or another even if that is ultimately just
cash for service.

** Legal

*** Agency Law

A good primer on the background behind legal personhood/identity/agency is [[https://press.princeton.edu/books/paperback/9780691157870/the-law-is-a-white-dog][The
Law is a White Dog]], which digs into some of the background on foundational
concepts in Western Legal Theory/Practice.

**** Thought Experiment: Suing a Robocop

Automation and AI convolute legal issues arising around agency (in both the
philosophical and legal senses). A good thought experiment is a robocop: if a
robot (or more generally, an automated agent of the state) acts inappropriately,
who is at fault? A robot lacks the kind of legal identity/personhood/standing to
face charges in court. Does a new version or a separate training version qualify
as a separate identifiable agent in court? Obviously the state is the entity
that empowered the agent to act on its behalf, but what does this mean in court
if the agent is personlike but without identity?

The examples referred in this video on [[https://www.youtube.com/watch?v=fOTuIhOWFXU][ToS for OpanAI/ChatGPT]] are already
complicated enough -- regarding waived indemnification in 3-way negatiations.

The way I think this "convolution of legal agency" will play out is that it
gives people with power/money another layer by which to obfuscate their actions
or to diffuse responsibility -- in other words: it's [[https://www.imdb.com/title/tt0119978/][Rainmakers]] all the way
down. If you have to legally break down a few of these layers with legal
expenses just to get some real issues in court, it is very convenient for large
corporations to hide behind automated agents — whose reasoning/testimony will be
simple in domes cases of explainable AI but impossible in others (LLM’s are not
explainable, especially the larger they get).

See [[https://mohitmayank.medium.com/explainable-ai-language-models-b4b75f56bfe2][Explainable AI: Language Models]] for a more "rigorous" explanation of why the
neural network architectures create explainability issues in LLM's.

But a "stupid smart" common sense way of thinking about this: is there any
definite objective meaning in language/communication? Legal language is about as
close as you can get to language that is parsed with precise meaning, but even
it is riddled with problems that require invoking various legal theories to
resolve -- e.g. the dependence on precedent in common law or constructivism
vs. positivism.
