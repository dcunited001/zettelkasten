:PROPERTIES:
:ID:       cea7d11c-8357-4e4f-90b3-fa8210eff796
:END:
#+title: AI

* Docs

* Resources

* Communities
+ [[https://ai.science/][AI.science]] (slack)

* Ideas

** Topology of the Hyperplane

*** Amateur Go Player Beats Top Go AI With 100% Win Rate

Sources:

+ [[The HUGE Problem with ChatGPT]]
+ [[github:HumanCompatibleAI/go_attack][HumanCompatibleAI/go_attack]]
+ [[https://arxiv.org/abs/2211.00241][Adversarial Policies Beat Superhuman Go AIs]]

So basically, the AI is not aware of the topology of the hyperplane. This is why
the strategy to beat the AI involves ... the topology (or shape) of the
arrangement of Go pieces on the board.

Since the AI algorithms for Go utilize information compression to render the
game of Go tractible, then they are unlikely to be adapted to deal with this
without being augmented with another technique -- such as the algorithm attained
by the MIT researchers themselves.

**** This "Topological perspective" also explains LLM Jailbreaks

This also explains jailbreak techniques and why it's so difficult to guard
against them. Because it's difficult to define regions of the hyperplane so that
they are "convex" for some loose definition of the term, it's also difficult to
construct an XAI policy to wall those regions off. The structure of the
hyperplane for language-based models probably moreso resembles swiss chess than
anything that can be "convexified".

That is, the regions of the hyperplane (or a LLM's state space, if I'm mincing
terminology) form a kind of labyrinth. Your interactions with it lead it down
various paths. The space represents language, conversation and interaction
state, Because of the connectedness of this space (it is Riemannian), then given
its n-dimensional holes, then there are no simple way to define XAI-based
semantic policy to prevent someone's interaction path from reaching certain
regions of the space -- not without significantly altering the behavior of the
AI.

Just like you can flip a convex function into a concave function you could
perhaps invert this behavior: simply define interaction paths starting from a
point within a boundary and limit the paths leaving that bounded
region. However, doing so will significantly alter the behavior of the AI. To
further confound this, it requires prohibitive levels of computation /and time
lag introduced from ML ops to update the LLM policies from XAI techniques./

And convexity/concavity are extremely complicated to define even in lower
n-dimensional Euclidean space (with or without the need for variational
methods). Here your topological concerns are mostly incidential to the functions
you're analyzing, not the connectedness of manifolds or the definition of
metrics to determine some arbitrary "increase/decrease." The definitions of
convexity here are based on derivation of functions or other tools -- anyways,
is a saddle point concave or convex? How about a local minima in a 9-D subspace
of an function on 11-D Euclidian space? It's totally convex in 9-D? What is it
then in 11-D? When this happens in Riemannian manifolds, it creates
wormhole-like structures: hence the "swiss" in the swiss cheese I menioned
above.

**** Thence:

[[https://te.xel.io/posts/2019-04-12-the-analytic-iching-a-novel-exegesis-for-the-age-of-data-science.html#the-algebraic-topology-of-hyperplanes][The Algebraic Topology of Hyperplanes … Donuts My Mind May Never Digest]] where I
describe the need for a "Gauss Bonnet Theorum" to analyze the structure of the
hyperplane and define such analysis as metacognition:

#+begin_quote
... reasoning about the topological features of the hyperplane corresponds to metacognition.
#+end_quote

From the bundle of crazy I wrote in 2019, [[https://te.xel.io/posts/2019-04-12-the-analytic-iching-a-novel-exegesis-for-the-age-of-data-science.html][The Analytic I Ching: A Novel Exegesis
In The Age of Data Science]].

... But then again, I'm not even allowed to exist. Or, at least, my name is not
showing up on TV or on some influencer's channels. And no, I didn't make it, but
once someone's life has been destroyed, network theory and social physics
implies that fixing it is a bit like escaping a black hole.

**** So I Opens the article from MIT

And what do you know: [[https://arxiv.org/pdf/2211.00241.pdf][they didn't cite me]]. I've only ever been cited for a 51/50
TDO (for short-term psychiatric care).

#+begin_quote
There are a number of situations that are known to be challenging for computer
Go players. Some can be countered through targeted modifications and additions
to the model archi- tecture or training, however, as we see with Cyclic
Topology, it is difficult to design and implement solutions one-by-one to fix
every possibility. Further, the weaknesses may be unknown or not clearly
understood – for instance, *Cyclic Topology* is normally rare, but through our
work we now know it can be produced consistently. Thus, it is critical to
develop algorithmic approaches for detecting weaknesses, and eventually for
fixing them
#+end_quote

Other failure modes include:

+ Ladders :: the phenomena ever beginner go player recognizes as the thing
  indicates a "problem." Democrats still haven't figured this one out. This is
  to some extent topological because, speaking in strategery of course, there
  really is an "edge" of the board.
+ Complicated Openings :: Hmmmm
+ Cyclic Topology :: The researchers seem to be real worried about the swiss
  cheese.
+ Mirror Go :: Symmetry? That's topology. It's possible that mirroring the AI's
  moves could actually glitch the AI and produce tactical gains from more than
  simply the change in the board.

* Topics

** Politiics
*** Have we seen the left held accountable for any of their failures? Why would AI/Climate be any different?

Being perpetually alienated for voting wrong really sucks.

How many silicon valley banks have to collapse before the liberals/moderates
admit that they have no idea what they're doing and the only reason they don't
lose elections is because they force everyone to focus on trivial cultural
issues to hack the vote (while derelicting their duty on AI/Climate). They can
set the country on fire with impunity.

Well who created AI? Have you been held accountable for anything? What are the
odds that we can hold you accountable for failing to act on AI policy after
having created it?

You suck. Good luck with that though. I'm glad we can legitimately blame you for
everything that happens, but I would rather have real leaders who do real things
that matter who are in power in Washington. We weren't allowed to have a leader
during Coronavirus though. Why? Because liberals can't stand up to leftists,
since they depend on leftist games to hack the vote.

** Legal

*** Agency Law

A good primer on the background behind legal personhood/identity/agency is [[https://press.princeton.edu/books/paperback/9780691157870/the-law-is-a-white-dog][The
Law is a White Dog]], which digs into some of the background on foundational
concepts in Western Legal Theory/Practice.

**** Thought Experiment: Suing a Robocop

Automation and AI convolute legal issues arising around agency (in both the
philosophical and legal senses). A good thought experiment is a robocop: if a
robot (or more generally, an automated agent of the state) acts inappropriately,
who is at fault? A robot lacks the kind of legal identity/personhood/standing to
face charges in court. Does a new version or a separate training version qualify
as a separate identifiable agent in court? Obviously the state is the entity
that empowered the agent to act on its behalf, but what does this mean in court
if the agent is personlike but without identity?

The examples referred in this video on [[https://www.youtube.com/watch?v=fOTuIhOWFXU][ToS for OpanAI/ChatGPT]] are already
complicated enough -- regarding waived indemnification in 3-way negatiations.

The way I think this "convolution of legal agency" will play out is that it
gives people with power/money another layer by which to obfuscate their actions
or to diffuse responsibility -- in other words: it's [[https://www.imdb.com/title/tt0119978/][Rainmakers]] all the way
down. If you have to legally break down a few of these layers with legal
expenses just to get some real issues in court, it is very convenient for large
corporations to hide behind automated agents — whose reasoning/testimony will be
simple in domes cases of explainable AI but impossible in others (LLM’s are not
explainable, especially the larger they get).

See [[https://mohitmayank.medium.com/explainable-ai-language-models-b4b75f56bfe2][Explainable AI: Language Models]] for a more "rigorous" explanation of why the
neural network architectures create explainability issues in LLM's.

But a "stupid smart" common sense way of thinking about this: is there any
definite objective meaning in language/communication? Legal language is about as
close as you can get to language that is parsed with precise meaning, but even
it is riddled with problems that require invoking various legal theories to
resolve -- e.g. the dependence on precedent in common law or constructivism
vs. positivism.
