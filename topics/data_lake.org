:PROPERTIES:
:ID:       0b80782f-92a8-4b48-958c-a41e7ff8713e
:END:
#+TITLE: Data Lake
#+AUTHOR:    David Conner
#+EMAIL:     noreply@te.xel.io
#+DESCRIPTION: notes


**** TODO RDD (resilient programming guix)
+ [[vs executor][RDD Programming Guide]]
+ [[https://www.databricks.com/glossary/what-is-rdd][RDD (databricks)]]
+ [[https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html][pyspark.RDD]]

**** TODO ectorepo's for tensorflow and pyspark
+ [ ] figure out how to test =repo sync= from file-based manifest first

**** TODO worker (executor) vs driver

**** TODO move/rename parquet files

#+begin_src shell
for old in ./images*.png; do
    new=$(echo $old | sed -e 's/\.png$/test.png/')
    mv -v "$old" "$new"
done
#+end_src

**** TODO create additional datalake tables

+ [ ] edge graphs in tables
+ [ ] edge length calculations (validate z coordinates)
+ [ ] convert csv/json to parquet/delta
+ [ ] calculate normals/angles

**** TODO docker images for tensorflow + mediapipe + pyspark?

+ [ ] Or output new parquet tables

* Roam
+ [[id:4c531cd8-3f06-47fb-857a-e70603891ed8][Hadoop]]
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]

* Docs

* Resources

* Queries

** Arrow

*** docs

+ [[https://arrow.apache.org/docs/][Main Docs]]
  - [[https://arrow.apache.org/docs/index.html][User Guides]] (learning oriented & more in-depth than the cookbook)
+ [[github:apache/arrow][apache/arrow]]
+ [[https://arrow.apache.org/docs/python/index.html][Python docs]]

*** Resources

+ Repos
  - Rust: [[github.com:apache/arrow-rs][apache/arrow-rs]]
  - Julia: [[github.com:apache/arrow-julia][apache/arrow-julia]]
  - Cookbook: [[github:apache/arrow-cookbook][apache/arrow-cookbook]] (task-oriented; includes java, python, c++ and r)

*** Python

+ [[https://arrow.apache.org/docs/python/parquet.html][Reading/writing parquet format]]

Compatible with python 3.7 tp 3.11 (2023)

*** Tools
**** Arrow Gandiva

Make arrow go vroom in runtime.

#+begin_quote
Gandiva is a runtime expression compiler that uses LLVM to generate efficient
native code for compute on Arrow record batches. Gandiva only handles
projections and filters; for other transformations, see [[https://arrow.apache.org/docs/cpp/compute.html#compute-cpp][Compute Functions.]]
#+end_quote

From [[https://www.youtube.com/watch?v=5o5E-CfC8gw][Data Science Across Data Sources with Apache Arrow]]

Can be 4.5x to 90x faster than the Java implementation (for some queries)

Enables vectorized execution with Intel SIMD instruction:

+ Hence: the 4.5x number (for some queries). At least I think this is why the
  number works out to that, when things can be vectorized.
+ From my previous unpaid internship (with myself, researching Metal/Swift and
  WebGL) I gots to thinking about bitcrunching with SIMD where available. The
  SIMD instructions typically require multple clock cycles (as do many of the
  accelerated instrcutions like AVX)
+ It turns out you can write virtual machines in this shit. That's it. it's
  over: I just won computer science (... not really, but this was a hunch I
  had for awhile and my friend Google Scholar confirmed)
  - [[https://www.cs.drexel.edu/~jjohnson/2010-11/summer/cs680/resources/doc/ptx_isa_2.3.pdf][PTX parallel thread execution]] (assembler for CUDA)
  - This is close to what I'm thinking, except the logic would occur using
    binary operations on the data (not the PTX/Cuda). It would probably be slow,
    but /could/ minimize paused threads in a warp. Metal and OpenGL will give
    weird errors at runtime & compile-time. And with CUDA I imagine runtime
    errors suck.
  - [[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9370326&casa_token=eauMltx9zq4AAAAA:9SH7Eow-9Q1NCjb5VJW6aU-PWV0HMT1mRNef0vfsMYbOW0lIU0lLjyakNjrVhOojqV5dV96X3g&tag=1][Efficient Execution of Graph Algorithms on CPU with SIMD Extensions]]
  - But it seems they just don't know how to solve B-SAT with ... a planet-sized
    computer and dynamic programming (this is all a joke btw)

**** Arrow Flight/SQL

From [[https://www.youtube.com/watch?v=HavgysXOlyo][Arrow Flight SQL: Accelerating Database Access]]

Built on a single set of gRPC client libraries.

+ Arrow Flight :: Arbitrary tabluar transfers
+ Arrow Flight SQL :: Transfers from DB's

Offers parallel data transfers: one-to-many and many-to-many

|-------------+------------------------+-------------------------------|
|             | Trad. DB               | Arrow                         |
|-------------+------------------------+-------------------------------|
| Driver Mgmt | One Driver per DB Type | One Driver to rule them all   |
| Performance | Row-based              | Data Transfer is column based |
|-------------+------------------------+-------------------------------|

Enables parallel/distributed fetch for distributed databases.

***** ODBC/JDBC vs Arrow Flight SQL

There are [[https://www.youtube.com/watch?v=6q8AMrQV3vE&t=1629s][two opportunities for performance increase]] (video) if both
client/driver are using columnar format.

+ Generally, at least one place for performance increase with columnar format,
  but if you're connecting to a trad. DB then you should use the native driver's
  row-based transfer to avoid col-to-row-to-col delay.


*** Topics

**** Types

Primative types: int, float, string, decimal, timestamp, etc

**** Parquet in Apache Spark

Two Readers:

*Non-vectorized* (fallback java implementation)

+ Supports all types (complex and primitive)
+ Reads rows

*Vectorized* (spark-native implementation)

+ Only primitives, but some complex types in Spark 3.3
+ Reads/scans data in batches (hence vectorized)
+ much better at memory locality (and thus i/o and cache utilization)
+ offers encoding-specific optimizations
+ can use =memcpy= directly in some cases (copies 1-D regions of RAM)

* Zeppelin

Notebooks for Apache Spark.

+ Zep 10.1 was pushed on 2022/02/26. The dockerfile implies that it should work
  with Spark 3.2

** Docs
+ [[github:apache/zeppelin][apache/zeppelin]]
+ [[https://zeppelin.apache.org/docs/latest/interpreter/spark.html][Spark Interpreter for Apache Zeppelin]]
  - [[https://zeppelin.apache.org/docs/latest/usage/interpreter/overview.html#inline-generic-configuration][Interpreter in Apache Zeppelin]]
+ [[https://zeppelin.apache.org/docs/latest/quickstart/kubernetes.html][Zeppelin on Kubernetes]]
  - Hopefully conveys how to connect to Spark on the network
  - [[https://raw.githubusercontent.com/apache/zeppelin/master/k8s/zeppelin-server.yaml][zeppelin-server.yaml]] k8s chart

** Resources
+ [[https://github.com/apache/zeppelin/tree/master/notebook/Spark%20Tutorial][Spark Tutorial]] for zeppelin
  - [[https://github.com/apache/zeppelin/blob/master/Dockerfile][Dockerfile]]

** Issues
*** Version Compatibility

Definitely need to reference the above links on interpreters if running into
version compatibility issues.

The =%spark.conf= directive provides the ability to tune inline configuration,
but the interpreters need to be restarted.

#+begin_quote
%spark.conf

spark.jars.packages io.delta:delta-core_2.12:2.0.2
spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.warehouse.dir /tmp/warehouse

zeppelin.spark.enableSupportedVersionCheck false

#+end_quote


** Topics
*** Docker Container

I was under the assumption that, since Spark is a networked service, it needs to
run as one ... but apparently that's not necessarily the case -- at least if
you're working on your own or running Zeppelin notebooks. It seems that even
doing so requires quite a bit more knowledge of the Spark ecosystem

So my assumptions now:

+ Applications building on Spark just need the JAR to talk to a Spark
  server/cluster anyways. This would give Zeppelin the ability to run all the
  Spark/SQL or PySpark code ... since it's going to need those JARs.
+ And that, to connect to them with something like Zeppelin would require adding
  some kind of connection config (like a SqlServer conncetion). This is what the
  K8S yaml seems to imply.

I would like to read from one set of Parquet files and perhaps transform them
somewhere else.

**** [[https://hub.docker.com/r/apache/zeppelin][apache/zeppelin]]

The image builds =FROM openjdk:8 as builder=

It sets these =MAVEN_OPTS= ... which probably need to be adjusted (or maybe not
if using Delta Lake as a frontend for data access)

|-------------------------+--------|
| Param                   | Value  |
|-------------------------+--------|
| -Xms                    | 1024MB |
| -Xmx                    | 2048MB |
| -XX:MaxMetaspaceSize    | 1024MB |
| -XX:-UseGCOverheadLimit |        |
|-------------------------+--------|

And =-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn=

* Delta Lake

** Docs

[[https://docs.delta.io/latest/index.html][Main Docs]]

+ [[https://docs.delta.io/latest/quick-start.html][Quickstart]]
+ [[https://github.com/delta-io/delta/blob/master/examples/cheat_sheet/delta_lake_cheat_sheet.pdf][Cheatsheet]]
+ [[https://docs.delta.io/latest/releases.html][Releases and version compatibility]]


*** Deployments
Delta Lake requires being structured around a Maven project.

+ [[https://docs.delta.io/latest/delta-standalone.html#][Delta Standalone]]. Supports java/scala only.

** Resources
+ Python [[https://delta.io/blog/2023-02-27-deltalake-0.7.0-release/][deltalake]] (with [[https://github.com/delta-io/delta-examples/blob/master/notebooks/delta-rs-python/blog_0_7_0.ipynb][notebook]])

** Topics

*** Data Ingestion

**** [[https://delta.io/blog/2022-09-23-convert-parquet-to-delta/][Convert Parquet to Delta Lake]]

The conversion is an in-place operation (no parquet files are modified)

*** Data Governance

**** From [[https://www.youtube.com/watch?v=SfNglvSeOoA][Simplify ETL Pipelines on Databricks Lakehouse]]

Issues with other data sources & providers

|----------------------+------------------+-----------------------------------------------------|
| Source/Provider      | Permissions On   | 10,000ft Problem                                    |
|----------------------+------------------+-----------------------------------------------------|
| Data Lake            | Files            | No row/col permissions                              |
| Hive Metadata        | Tables/views     | Metadata syncing complicates permission enforcement |
| Data Warehouse       | Tables/cols/rows | Disparate governance model                          |
| ML Models/Dashboards | Models/Web/API   | Consumer-facing. Analysts? Data already entangled.  |
|----------------------+------------------+-----------------------------------------------------|

The above offer coarse grained permissions controls, which are difficult to
manage in a multi-cluster and/or hybrid-cloud environment

+ "This would require developing a custom administration webapp to unify
  authentication/identity/authorization across multiple clouds, where the data
  (and schema) can change quite rapidly." - me
+ Databricks offers cloud portability, a unified architecture and the "Unity
  catalog" which is not marketing jargon at all. This comes with permissions
  declared in the style of "ANSI SQL GRANT"

Regardless, their platform will simplify quite a few things or at least provide
a central basis from which permissions management in other 3rd party tools is
easier to layer on.



*** Docker Containers

**** [[https://hub.docker.com/r/bitnami/spark][bitnami/spark]]

The image offers a good consolidated description of the Apache Spark environment
variables (The [[https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts][Spark docs]] contains a more complete description)

+ [[https://github.com/bitnami/containers/blob/main/bitnami/spark/3.3/debian-11/Dockerfile][Dockerfile]] here in the [[https://github.com/bitnami/containers][bitnami/containers]] repo.
+ [[https://github.com/bitnami/charts/tree/master/bitnami/spark][Spark charts for Kubernetes]] are here in [[https://github.com/bitnami/charts][bitnami/charts]].


**** [[https://hub.docker.com/r/ohdsi/broadsea-spark-sql][ohdsi/broadsea-spark-sql]]

[[github:OHDSI/Broadsea-Spark-SQL][OHDSI/Broadsea-Spark-Sql]], repo with Dockerfile.

+ This is an image building on bitnami/spark for Spark SQL with Delta Lake
  extension. docker image here
+ It starts

**** Delta Lake with Zeppelin Notebooks

* Duck DB

Self-contained, simpler DB for queries on flat files


* Formats

** Orc

** Parquet

*** Docs
+ [[https://arrow.apache.org/docs/python/parquet.html][Reading/Writing the Apache Parquet Format]]
  - [[https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html][pyarrow.parquet.ParquetDataset]]
+ [[https://spark.apache.org/docs/2.4.0/sql-data-sources-parquet.html][Spark SQL: Parquet Files]]

*** Resources

*** Tools
**** pqrs

Inspect parquet files (rust implementation)

**** dsq

Query JSON, CSV, Parquet.

+ Uses SQLite internally, so doesn't work for large datasets.
+ Does offer =ob-dsq= for emacs

*** Topics

**** Format and Optimizations

From [[https://www.youtube.com/watch?v=1j8SdS7s_NY][Parquet Format and Perf Optimization]]

+ File-based Partitions (or network-based)
+ Predicate Pushdown
+ Row Group and Page Size
+ Tuning Dictionary Encoding
+ Delta Lake: optimize tuning/indexes/joins/views/imports/exports

  Parquet/ORC files are hybrid columnar & row-based

***** Encoding Schemes

*Plain*

2 encoding schemes:


*RLE_Dictionary* (run length encoding)

6+ encoding schemes.

+ RLE uses bit-packing and dictionary-based compression. The dictionaries are
  small.
+ Assumes Duplicate/Repeated values (string-substitution)
+ Parquet files with sections that exceed max dictionary sizes will fallback to
  plain (they drop compression)

***** Compression

+ Compression should be CPU-minimal. Picking the wrong algorithm can induce a
  lot of I/O. The perf gains are depending on the algorithm: snappy, gzip, lzip,
  etc.
+ Snappy seems to be used alot.

***** Row Groups

Row Group Size: 128MB (default)

+ Row group 0
  - Col A chunk 0
    - Page 0 ... N
  - Col B chunk 0
+ Row group 1 ... N
+ Footer

***** Pages

Page Size: 1MB (default)

+ Col X Chunk N
  - Page 0
    - Page metadata
    - Ref levels
    - Def levels
    - Encoded Values (dictionary)

**** ETL Optimiazation

Overhead for each file:

+ set up internal data strcucts
+ instantiate file readers (handlers)
+ fetch file (network)
+ parse parquet metadata

Use =df.repartition(n_parts)= or =df.coalesce(n_parts)= where applicable

Be aware of incremental workload output:

+ when ingesting parquet streams, you can't always predict the size, this can
  create large numbers of small files, where overhead from I/O and network
  exceeds any benefits.

Data sources like Delta Lake tables store their metadata and deltas in
json/parquet. These need to be =pruned= and then =vacuumed=

***** Vacuuming Data

Do NOT vacuum while data is being ingested.

To circumvent this, data normalization operations will either need
(speculation):

+ setup/load of a mirrored datasource and a coordinated switch-over (of DNS or
  source/sink configuration)
+ or just store the streams/files on multiple S3 buckets and pop one offline,
  normalize/vacuum the data while collecting a delta in a new source
+ or create an additional S3 bucket to accumulate the incoming streams while the
  other S3 buckets are taken offline. when the DLT table renormalization is
  complete, feed the old data into the new S3 buckets ... this would not
  necessarily require updating configuation in the data streams, the
  source/admin of which may not be under your control.

Whatever you do, it's dicey.

For a second, I was thinking CephFS would make this easier .... nope probably
not. Well maybe: it depends on whether you can move disk segments containing
files to an logical/named storage device where they are processed.

****** TODO ... honestly what do these do again?

***** Predicate Pushdown

+ Limit the query to the conditions/columns you need.
+ Take advantage of "materialized views" .......... where practical
+ This can avoid unnecessary compression/translation.
+ Data should be relatively sorted (and normalized, if it's a delta lake table)
+ Types should be explicitly specified.

Ensure =spark.sql.parquet.filterPushdown= or =parquet.filter.dictionary.enabled=
are set.

****** Metadata

Default metadata: min, max, count.

Metadata for parquet files is always read first:

+ For the file, then the row groups.
+ only rowgroups where metadata for columns are valud are read for the queries.

This is because the rowgroup and pages can be indexed in the file:

+ Don't use strings where possible unless they end up in the RLE dictionary.
  - if the sizes of strings can't be predicted, then the parquet reader must
    tract rowgroup & page starts/ends in metadata (it probably already needs to,
    since dictionary sizes can't be known at initial read time.)
+ And strings for structs will consume memory in the client once parsed.

**** Row-based vs Columnar

Columnar creates the opportunity for compression (when queries can't easily
determine the structure in which they'll return results)

***** Low Level Performance

Row-based leads to fragmented memory-access patterns, which tends to lead to CPU
cache invalidation. And who doesn't love their CPU cache on a VM-in-a-VPS (with
NUMA properly configured of course) ... containers perhaps not eligible without
sufficient =niceness=.



* ETL

** Delta Live Tables

Use DLT for ingest/transform

+ Ingest with =Auto Loader= and =COPY INTO=
+ Transform with automated ETL: DLT's API combines batch & streaming

Use Databricks workflows for orchestration

*** Docs

*** Resources
+ [[https://www.databricks.com/blog/2022/04/25/simplifying-change-data-capture-with-databricks-delta-live-tables.html][Simplifying Change Data Capture With Databricks Delta Live Tables]]

*** Topics

**** Security and Governance

See [[https://www.databricks.com/product/unity-catalog][Unity Catalog]]

**** Automation

***** Main DLT Benefits

Accelerates ETL development:

+ Declare SQL/Python and DLT auto-orchestrates the DAG graph, handles retries
  and adapts to changing schema in the provided data.

Automates infrastructure:

+ recovery, autoscaling and perf. optimization are handled

Ensure Data Quality

+ Quality Controls
+ Testing
+ Monitoring
+ Policy/Permissions Enforcement

Unifies Batch/Streaming into one API

* Orchestration

** Databricks
