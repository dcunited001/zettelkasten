:PROPERTIES:
:ID:       0b80782f-92a8-4b48-958c-a41e7ff8713e
:END:
#+TITLE: Data Lake
#+AUTHOR:    David Conner
#+EMAIL:     noreply@te.xel.io
#+DESCRIPTION: notes


**** TODO RDD (resilient programming guix)
+ [[vs executor][RDD Programming Guide]]
+ [[https://www.databricks.com/glossary/what-is-rdd][RDD (databricks)]]
+ [[https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html][pyspark.RDD]]

**** TODO ectorepo's for tensorflow and pyspark
+ [ ] figure out how to test =repo sync= from file-based manifest first

**** TODO worker (executor) vs driver

**** TODO move/rename parquet files

#+begin_src shell
for old in ./images*.png; do
    new=$(echo $old | sed -e 's/\.png$/test.png/')
    mv -v "$old" "$new"
done
#+end_src

**** TODO create additional datalake tables

+ [ ] edge graphs in tables
+ [ ] edge length calculations (validate z coordinates)
+ [ ] convert csv/json to parquet/delta
+ [ ] calculate normals/angles

**** TODO docker images for tensorflow + mediapipe + pyspark?

+ [ ] Or output new parquet tables

* Roam
+ [[id:4c531cd8-3f06-47fb-857a-e70603891ed8][Hadoop]]
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]

* Docs

* Resources

* Queries

** Arrow

*** docs

+ [[https://arrow.apache.org/docs/][Main Docs]]
  - [[https://arrow.apache.org/docs/index.html][User Guides]] (learning oriented & more in-depth than the cookbook)
+ [[github:apache/arrow][apache/arrow]]
+ [[https://arrow.apache.org/docs/python/index.html][Python docs]]

*** Resources

+ Repos
  - Rust: [[github.com:apache/arrow-rs][apache/arrow-rs]]
  - Julia: [[github.com:apache/arrow-julia][apache/arrow-julia]]
  - Cookbook: [[github:apache/arrow-cookbook][apache/arrow-cookbook]] (task-oriented; includes java, python, c++ and r)

*** Python

+ [[https://arrow.apache.org/docs/python/parquet.html][Reading/writing parquet format]]

Compatible with python 3.7 tp 3.11 (2023)

*** Tools
**** Arrow Gandiva

Make arrow go vroom in runtime.

#+begin_quote
Gandiva is a runtime expression compiler that uses LLVM to generate efficient
native code for compute on Arrow record batches. Gandiva only handles
projections and filters; for other transformations, see [[https://arrow.apache.org/docs/cpp/compute.html#compute-cpp][Compute Functions.]]
#+end_quote

+ Can be 4.5x to 90x faster than the Java implementation (for some queries)

Enables vectorized execution with Intel SIMD instruction:

+ Hence: the 4.5x number (for some queries). At least I think this is why the
  number works out to that, when things can be vectorized.
+ From my previous unpaid internship (with myself, researching Metal/Swift and
  WebGL) I gots to thinking about bitcrunching with SIMD where available. The
  SIMD instructions typically require multple clock cycles (as do many of the
  accelerated instrcutions like AVX)
+ It turns out you can write virtual machines in this shit. That's it. it's
  over: I just won computer science (... not really, but this was a hunch I
  had for awhile and my friend Google Scholar confirmed)
  - [[https://www.cs.drexel.edu/~jjohnson/2010-11/summer/cs680/resources/doc/ptx_isa_2.3.pdf][PTX parallel thread execution]] (assembler for CUDA)
  - This is close to what I'm thinking, except the logic would occur using
    binary operations on the data (not the PTX/Cuda). It would probably be slow,
    but /could/ minimize paused threads in a warp. Metal and OpenGL will give
    weird errors at runtime & compile-time. And with CUDA I imagine runtime
    errors suck.
  - [[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9370326&casa_token=eauMltx9zq4AAAAA:9SH7Eow-9Q1NCjb5VJW6aU-PWV0HMT1mRNef0vfsMYbOW0lIU0lLjyakNjrVhOojqV5dV96X3g&tag=1][Efficient Execution of Graph Algorithms on CPU with SIMD Extensions]]
  - But it seems they just don't know how to solve B-SAT with ... a planet-sized
    computer and dynamic programming (this is all a joke btw)

*** Topics

**** Types

Primative types: int, float, string, decimal, timestamp, etc

**** Parquet in Apache Spark

Two Readers:

*Non-vectorized* (fallback java implementation)

+ Supports all types (complex and primitive)
+ Reads rows

*Vectorized* (spark-native implementation)

+ Only primitives, but some complex types in Spark 3.3
+ Reads/scans data in batches (hence vectorized)
+ much better at memory locality (and thus i/o and cache utilization)
+ offers encoding-specific optimizations
+ can use =memcpy= directly in some cases (copies 1-D regions of RAM)

** Arrow SQL

* Zeppelin

Notebooks for Apache Spark.

+ Zep 10.1 was pushed on 2022/02/26. The dockerfile implies that it should work
  with Spark 3.2

** Docs
+ [[github:apache/zeppelin][apache/zeppelin]]
+ [[https://zeppelin.apache.org/docs/latest/interpreter/spark.html][Spark Interpreter for Apache Zeppelin]]
  - [[https://zeppelin.apache.org/docs/latest/usage/interpreter/overview.html#inline-generic-configuration][Interpreter in Apache Zeppelin]]
+ [[https://zeppelin.apache.org/docs/latest/quickstart/kubernetes.html][Zeppelin on Kubernetes]]
  - Hopefully conveys how to connect to Spark on the network
  - [[https://raw.githubusercontent.com/apache/zeppelin/master/k8s/zeppelin-server.yaml][zeppelin-server.yaml]] k8s chart

** Resources
+ [[https://github.com/apache/zeppelin/tree/master/notebook/Spark%20Tutorial][Spark Tutorial]] for zeppelin
  - [[https://github.com/apache/zeppelin/blob/master/Dockerfile][Dockerfile]]

** Issues
*** Version Compatibility

Definitely need to reference the above links on interpreters if running into
version compatibility issues.

The =%spark.conf= directive provides the ability to tune inline configuration,
but the interpreters need to be restarted.

#+begin_quote
%spark.conf

spark.jars.packages io.delta:delta-core_2.12:2.0.2
spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.warehouse.dir /tmp/warehouse

zeppelin.spark.enableSupportedVersionCheck false

#+end_quote


** Topics
*** Docker Container

I was under the assumption that, since Spark is a networked service, it needs to
run as one ... but apparently that's not necessarily the case -- at least if
you're working on your own or running Zeppelin notebooks. It seems that even
doing so requires quite a bit more knowledge of the Spark ecosystem

So my assumptions now:

+ Applications building on Spark just need the JAR to talk to a Spark
  server/cluster anyways. This would give Zeppelin the ability to run all the
  Spark/SQL or PySpark code ... since it's going to need those JARs.
+ And that, to connect to them with something like Zeppelin would require adding
  some kind of connection config (like a SqlServer conncetion). This is what the
  K8S yaml seems to imply.

I would like to read from one set of Parquet files and perhaps transform them
somewhere else.

**** [[https://hub.docker.com/r/apache/zeppelin][apache/zeppelin]]

The image builds =FROM openjdk:8 as builder=

It sets these =MAVEN_OPTS= ... which probably need to be adjusted (or maybe not
if using Delta Lake as a frontend for data access)

|-------------------------+--------|
| Param                   | Value  |
|-------------------------+--------|
| -Xms                    | 1024MB |
| -Xmx                    | 2048MB |
| -XX:MaxMetaspaceSize    | 1024MB |
| -XX:-UseGCOverheadLimit |        |
|-------------------------+--------|

And =-Dorg.slf4j.simpleLogger.log.org.apache.maven.cli.transfer.Slf4jMavenTransferListener=warn=

* Delta Lake

** Docs

[[https://docs.delta.io/latest/index.html][Main Docs]]

+ [[https://docs.delta.io/latest/quick-start.html][Quickstart]]
+ [[https://github.com/delta-io/delta/blob/master/examples/cheat_sheet/delta_lake_cheat_sheet.pdf][Cheatsheet]]
+ [[https://docs.delta.io/latest/releases.html][Releases and version compatibility]]


*** Deployments
Delta Lake requires being structured around a Maven project.

+ [[https://docs.delta.io/latest/delta-standalone.html#][Delta Standalone]]. Supports java/scala only.

** Resources
+ Python [[https://delta.io/blog/2023-02-27-deltalake-0.7.0-release/][deltalake]] (with [[https://github.com/delta-io/delta-examples/blob/master/notebooks/delta-rs-python/blog_0_7_0.ipynb][notebook]])

** Topics

*** Data Ingestion

**** [[https://delta.io/blog/2022-09-23-convert-parquet-to-delta/][Convert Parquet to Delta Lake]]

The conversion is an in-place operation (no parquet files are modified)

*** Data Governance

Issues with other data sources & providers

|----------------------+------------------+-----------------------------------------------------|
| Source/Provider      | Permissions On   | 10,000ft Problem                                    |
|----------------------+------------------+-----------------------------------------------------|
| Data Lake            | Files            | No row/col permissions                              |
| Hive Metadata        | Tables/views     | Metadata syncing complicates permission enforcement |
| Data Warehouse       | Tables/cols/rows | Disparate governance model                          |
| ML Models/Dashboards | Models/Web/API   | Consumer-facing. Analysts? Data already entangled.  |
|----------------------+------------------+-----------------------------------------------------|


*** Docker Containers

**** [[https://hub.docker.com/r/bitnami/spark][bitnami/spark]]

The image offers a good consolidated description of the Apache Spark environment
variables (The [[https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts][Spark docs]] contains a more complete description)

+ [[https://github.com/bitnami/containers/blob/main/bitnami/spark/3.3/debian-11/Dockerfile][Dockerfile]] here in the [[https://github.com/bitnami/containers][bitnami/containers]] repo.
+ [[https://github.com/bitnami/charts/tree/master/bitnami/spark][Spark charts for Kubernetes]] are here in [[https://github.com/bitnami/charts][bitnami/charts]].


**** [[https://hub.docker.com/r/ohdsi/broadsea-spark-sql][ohdsi/broadsea-spark-sql]]

[[github:OHDSI/Broadsea-Spark-SQL][OHDSI/Broadsea-Spark-Sql]], repo with Dockerfile.

+ This is an image building on bitnami/spark for Spark SQL with Delta Lake
  extension. docker image here
+ It starts

**** Delta Lake with Zeppelin Notebooks

* Duck DB

Self-contained, simpler DB for queries on flat files


* Formats

** Orc

** Parquet

*** Docs
+ [[https://arrow.apache.org/docs/python/parquet.html][Reading/Writing the Apache Parquet Format]]
  - [[https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html][pyarrow.parquet.ParquetDataset]]
+ [[https://spark.apache.org/docs/2.4.0/sql-data-sources-parquet.html][Spark SQL: Parquet Files]]

*** Resources

*** Tools
**** pqrs

Inspect parquet files (rust implementation)

**** dsq

Query JSON, CSV, Parquet.

+ Uses SQLite internally, so doesn't work for large datasets.
+ Does offer =ob-dsq= for emacs

*** Topics


* ETL

** Delta Live Tables

Use DLT for ingest/transform

+ Ingest with =Auto Loader= and =COPY INTO=
+ Transform with automated ETL: DLT's API combines batch & streaming

Use Databricks workflows for orchestration

*** Docs

*** Resources
+ [[https://www.databricks.com/blog/2022/04/25/simplifying-change-data-capture-with-databricks-delta-live-tables.html][Simplifying Change Data Capture With Databricks Delta Live Tables]]

*** Topics

**** Security and Governance

See [[https://www.databricks.com/product/unity-catalog][Unity Catalog]]

**** Automation

***** Main DLT Benefits

Accelerates ETL development:

+ Declare SQL/Python and DLT auto-orchestrates the DAG graph, handles retries
  and adapts to changing schema in the provided data.

Automates infrastructure:

+ recovery, autoscaling and perf. optimization are handled

Ensure Data Quality

+ Quality Controls
+ Testing
+ Monitoring
+ Policy/Permissions Enforcement

Unifies Batch/Streaming into one API

* Orchestration

** Databricks
