:PROPERTIES:
:ID:       ac2a1ae4-a695-4226-91f0-8386dc4d9b07
:END:
#+TITLE:     DevOps
#+AUTHOR:    David Conner
#+EMAIL:     noreply@te.xel.io
#+DESCRIPTION: notes

* Docs

* Resources

* Issues

* Networking
** eBPF
Intelligent packet filters for inter-container networking based on BPF (Berkeley
Packet Filter)

** Security
*** [[https://cilium.io/][Cilium]]
*** Litmus
+ Repo: [[https://github.com/litmuschaos/litmus][litmuschaos/litmus]]

An opinionated approach inspired by "chaos engineering". It uses fuzzing-like
approaches to testing container/cloud configs. Used for stress-testing or
constructing experiments to see what could go wrong.

Basically "the opposite of GitOps", in spirit.

* Orchestration
** Rancher
*** Docs
+ [[https://rancher.com/docs/rancher/v2.6/en/][Rancher]] 2.6
+ Rancher's own [[https://github.com/rancher/charts][rancher/charts]]

*** Resources
+ [[https://rancher.com/docs/rancher/v2.6/en/best-practices/rancher-server/rancher-in-vsphere/][Installing Rancher in a vSphere Environment]]
+ [[https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/][Cluster Provisioning with Rancher]]
+ [[https://rancher.com/docs/rancher/v2.6/en/helm-charts/][Helm Charts in Rancher]]
+ [[https://rancher.com/docs/rancher/v2.6/en/overview/architecture-recommendations/][Architecture Recommendations]]
+ [[https://rancher.com/docs/rancher/v2.6/en/k8s-in-rancher/][Links to Kubernetes Resources]]

** Kubernetes
*** Docs
+ [[github:kelseyhightower/kubernetes-the-hard-way][kelseyhightower/kubernetes-the-hard-way]]
*** Resources
+ [[https://github.com/marco-lancini/k8s-lab-plz][marco-lancici/k8s-lab-plz]]: deploy a kubernetes lab cluster with [[https://please.build/basics.html][please.build]],
  a Make-style build tool inspired by Blaze/Bazel/Buck, etc.
  - walkthrough: [[https://www.marcolancini.it/2021/blog-kubernetes-lab-baremetal/][Kubernetes Lab on Baremetal]]
+ [[https://www.dasblinkenlichten.com/getting-started-kubernetes-using-ansible/][Getting started with kubernetes using ansible]]

**** Deployments
+ [[https://dev.to/carminezacc/creating-a-kubernetes-cluster-with-fedora-coreos-36-j17][Kubernetes Cluster on Fedora CoreOS]]
+ [[https://www.matthiaspreu.com/posts/fedora-coreos-kubernetes-basic-setup/][Kubernetes/CoreOS basic setup]]

**** Guix
+ [[https://codeberg.org/allana/guix-system/commits/branch/main/allana/packages/kubernetes.scm][allana/guix-system]]

* [[https://k3s.io][K3S]]
Rancher installer script sets up SE Linux; installs via a custom RPM channel.

** Docs

** Resources
+ [[https://stevex0r.medium.com/setting-up-a-lightweight-kubernetes-cluster-with-k3s-and-fedora-coreos-12d504160366][Setting up a lightweight Kubernetes cluster with K3s and Fedora CoreOS]]
+ [[https://vmguru.com/2021/04/how-to-install-rancher-on-k3s/][Installing Rancher on HA K3s]]


** Issues
*** Installing On NixOS

Similar enough to Guix. Also nix/guix are usually concise documentation of
internals, service dependencies and build requirements. I guess packages usally
are ... except I actually find myself reading these. Arch/AUR are hard to clone.

+ [[https://nixos.wiki/wiki/K3s][nixos.wiki/wiki/K3s]]
  - [[https://github.com/TUM-DSE/doctor-cluster-config/tree/master/modules/k3s][TUM-DSE/doctor-cluster-config]]
+ [[https://nixos.wiki/wiki/kubernetes][nixos.wiki/wiki/kubernetes]]
  - [[https://github.com/cmollekopf/kube-nix][cmollekopf/kube-nix]]
  - [[https://github.com/saschagrunert/kubernix][saschagrunert/kubernix]]

**** NixOS references

+ [[https://r.ryantm.com/log/updatescript/k3s/][nix build logs for k3s]]
  - output for a NixOS build of k3s
+ [[https://github.com/NixOS/nixpkgs/issues/182085][issues/182085]] k3s: support HA cluster (pull/185231 [[https://github.com/NixOS/nixpkgs/pull/185231/commits/60e0d3d73670ef8ddca24aa546a40283e3838e69][commit]])
  - starts/completes a pullreq to modify k3s package & service initiation
+ [[https://github.com/NixOS/nixpkgs/pull/185231][pull/158089]]: k3s: v1.22.3+k3s1 -> 1.23.3+k3s1
  - upgrade k3s version, change build process, split into two derivations
+ [[https://github.com/NixOS/nixpkgs/pull/161906][pull/161906]] (#156615): k3s: update script is broken
  - one of the few things i didn't grok from the package source

+ NixOS/nixpkgs: all references are in this project
  - nixos/modules/module-list.nix mentions service in =./services/cluster/k3s/default.nix=
  - nixos/modules/services/cluster/k3s/default.nix defines the =k3s= service
  - pkgs/applications/networking/cluster/...
    - ./k3s/default.nix describes the build process in comments
    - ./k3s/update.sh
    - ./kube3d/default.nix
    - patch: ./k3s/patches/0001-scrips-download-strip-downloading-just-package-CRD.patch

***** Build

Second Phase

util-linux because [[https://github.com/kubernetes/kubernetes/issues/26093#issuecomment-705994388][kubelet wants 'nsenter' from util-linux]]

+ buildInputs: kmod, socat, iptables, iproute2, bridge-utils, ethtool, util-linux, conntrack-tools
+ nativeBuildInputs: makeWrapper, rsync, yq-go, zstd
+ propagatedBuildInputs k3sCNIPlugins, k3sContainerd, k3sServer, runc

**** NixOS Hashicorp

There are also nixos packages/services for:

+ consul/nomad
+ terraform/terraform-ls
+ hashi-ui

**** NixOS Vault

+ pkgs/tools/security/vault/...
  - default.nix
  - vault-bin.nix
  - update-bin.sh
+ nixos/modules/services/security/...
  - vault.nix
+ nixos/tests/...
  - vault.nix
  - vault-dev.nix
  - vault-postgresql.nix

***** Vault Tools
+ pkgs/applications/networking/cluster/hashi-up/default.nix: install
  consul/nomad/vault on remote linux hosts (no deps)
+ pkgs/tools/misc/vsh/default.nix: hashicorp vault interactive shell (no deps)
+ pkgs/tools/security/safe/default.nix: a CLI for Vault (no deps)
+ pkgs/tools/security/vault-medusa/default.nix: import/export vault secrets. no dependencies/patches?
+  but build deps and several haskell dependencies

**** NixOS Helm

Very few dependencies for the Helm =buildGoModule=

+ pkgs/applications/networking/cluster/helm/plugins/...
  - helm-secrets.nix: installs wrapper script for several tools (e.g. vault)

** Swarm

* Service Discovery
** Consul (hashicorp)

Requires configuration management.

*** Docs
+ [[https://www.consul.io/docs/intro][Getting Started]]

*** Resources


* Provisioning

* Virtualization Platforms
** vSphere

** Harvester

*** Docs
+ [[https://docs.harvesterhci.io/v1.0/reference/api/https://docs.harvesterhci.io/v1.0/reference/api/][API Docs]]: basically an API wrapper around Kubevirt

*** Resources

*** Issues

**** How to provision storage to Harvester Nodes/Guests?
+ usually requires Persistent Volume Claims (ala k8s)
+ see kubevirt resources

**** Can harvester support GPU passthrough?
+ The models listed in the [[https://docs.harvesterhci.io/v1.0/reference/api/][Harvester API Docs]] indicate so
  - however, these models also contain references to vGPU which is an nVidia feature.
  - Also, nVidia publishes a GPU Addon for "discovery" of nVidia GPU/vGPU's on
    the host. It's unclear whether the GPU model is synonymous with nVidia's
    plugin or simply confusing.
  - Regardless, neither the API model nor the Kubevirt addon should be required for GPU passthrough.
+ This [[https://kubevirt.io/user-guide/virtual_machines/host-devices/][should be possible]], if not through the Harvester interface then by:
  - adding device ID's to the =permittedHostDevices= in =KubeVirt CR=
  - then adding these devices to the KubeVirt VMI's

***** Possible Solution

Reference ArchWiki on [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF][OVMF passthrough]]  It may require:

+ a script writing to =/sys/.../driver_override=
+ bios config
+ amd_iommu=on & updated grub
+ updates to /etc/mkinitcpio.conf
+ blacklisting drivers (probably shouldn't be installed on harvester anyways) and setting device driver to =vfio_pci=
+ manually setting up OVMF within Harvester
+ For Guix:
  - install a VM Guest without graphics and with few disks
  - add non-guix channels & substitutes, update/reboot
  - clone the VM to backup
  - update the system image to include GPU drivers
  - poweroff, change the VM config & reboot
  - And (of course) ensure Harvester never starts with the wrong monitors plugged in, since Asus BIOS doesn't allow setting priority on GPU device with any stickiness

** Proxmox
*** Docs
*** Resources
+ [[https://forum.proxmox.com/][Forums]]

**** Networking
+ [[https://pve.proxmox.com/pve-docs/chapter-pvesdn.html][SDN docs]]
**** Storage
+ 2012 monograph on [[https://pthree.org/2012/12/05/zfs-administration-part-ii-raidz/][ZFS administration]]
*** Issues
**** PXE Boot
+ [[https://www.reddit.com/r/homelab/comments/st3bji/proxmox_zfs_pxe_booting_with_grub_for_bios_systems/][Proxmox + ZFS - PXE Booting with GRUB for BIOS systems]]
  - [[https://rpi4cluster.com/pxe/setup/][How to boot Windows and Linux using UEFI net boot and iPXE]]
    - generic, but describes a setup
+ [[https://www.apalrd.net/posts/2022/alpine_vdiclient/][Net Booting the Proxmox VDI Client (feat. Alpine Linux)]]
  - moreso PXE booting a VM image with a custom version of Proxmox's spice
+ [[https://github.com/morph027/pve-iso-2-pxe][morph027/pve-iso-2-pxe]]
**** Encrypting Proxmox
+ [[https://herold.space/proxmox-zfs-full-disk-encryption-with-ssh-remote-unlock/][Full Disk Encryption with SSH Remote Unlock]] (from [[https://forum.proxmox.com/threads/howto-wrapper-script-to-use-fedora-coreos-ignition-with-proxmox-cloud-init-system-for-docker-workloads.86494/][proxmox forum]])
+ [[https://wiki.geco-it.net/public:pve_fcos][Fedora CoreOS Ignition with Proxmox cloud-init]] (from [[https://forum.proxmox.com/threads/howto-wrapper-script-to-use-fedora-coreos-ignition-with-proxmox-cloud-init-system-for-docker-workloads.86494/][proxmox forum]])

** oVirt


* Virtualization
:PROPERTIES:
:ID:       cf2bd101-8e99-4a31-bbdc-a67949389b40
:END:

** Kubevirt

This is a VM provider using a k8s interface (i.e. you do VM things using the Kubernetes API)

*** Docs
+ [[https://kubevirt.io/user-guide/architecture/][Main]] (architecture)
+ [[https://kubevirt.io/api-reference/master/definitions.html][API Docs]]

*** Resources
+ [[https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/][Provisioning Storage]]
+ [[https://kubevirt.io/user-guide/virtual_machines/host-devices/][Host Prep for PCI Passthrough]]

** QEMU

+ [[https://wiki.archlinux.org/title/QEMU/Guest_graphics_acceleration][QEMU graphics accel]] (wiki)
+ [[https://alyssa.is/using-virtio-wl/][A technical overview of Virtio WL]] (qemu/libvirt)
+ [[https://developer.ibm.com/articles/l-virtio/][VirtIO an I/O virtualization framework]]

*** Docs

*** Resources

*** Issues
**** Getting vm's to share integrated graphics
+ see [[https://www.reddit.com/r/VFIO/comments/i9dbyp/this_is_how_i_managed_to_passthrough_my_igd/][this reddit post]]
+ details for [[https://www.reddit.com/r/VFIO/comments/s0rwxl/gpu_passthrough_on_lenovo_legion_5_amd_laptop_so/][passthrough on Legion 5 AMD laptop]]
  - may also require copying vBios and/or flashing firmware

** Libvirt


*** Tools
**** virt-manager

**** virsh

**** virt-install

**** cockpit-machine
+ Running [[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/configuring-virtual-machine-network-connections_configuring-and-managing-virtualization][Virt-Manager with Redhat/Centos]]

*** Issues

**** [[https://wiki.libvirt.org/page/TLSSetup][Setting up libvirt for TLS (Encryption & Authentication)]]
**** Adding an ISO after setup
  - virt-install :: [[https://serverfault.com/questions/833131/virt-install-cannot-use-iso-file-as-location][mount as iso]] and pass to =--location=
    - mount as a loopback device
      - =mount -t iso9660 -ro loop /dir/cdimage.iso /mnt/iso=
    - also pass loop device to guest
  - image in pool ::
**** Bridging a WIFI device
You can't, apparently. You can [[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-managing_guest_virtual_machines_with_virsh-attaching_and_updating_a_device_with_virsh][redirect a USB device]]
**** Redirecting a USB Device

+ Run =lsusb= to get the vendor/product ID
+ Create an =$xml= file defining it
  - bonus points for using =emmet-mode=
  - insert the vendor/product id like =0x1234= for hex
+ use =virsh list --all= to get the =$domain=
+ run =virsh attach-device $domain --file $xml --config= to attach
  -  use similar =detach-device= to remove it


** Admin Tools
+ dnsmasq :: dns
+ dhclient :: dhcp
+ dmidecode :: SMBIOS table, hardware compat/interoperability
+ ebtables :: NAT networking on the host
+ bridge-utils :: create virtual networking devices: TUN/TAP, bridge
