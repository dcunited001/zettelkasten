:PROPERTIES:
:ID:       ac2a1ae4-a695-4226-91f0-8386dc4d9b07
:END:
#+TITLE:     DevOps
#+AUTHOR:    David Conner
#+EMAIL:     noreply@te.xel.io
#+DESCRIPTION: notes

* Docs

* Resources
+ [[https://www.90daysofdevops.com/#][90 Days of DevOps]] ([[https://github.com/MichaelCade/90DaysOfDevOps/tree/216a4695ea7c553d272733713808db10f88513ca][repo]])
+ [[https://github.com/bregman-arie/devops-exercises][Devops Exercises]] a bit basic, but probably a great basic resource

** Articles

+ [[https://teamraft.com/2021/03/17/integrating-keycloak-and-opa-with-confluent.html][Integrating Keycload and OPA with Confluent]]

* Issues

* Networking
** eBPF
Intelligent packet filters for inter-container networking based on BPF (Berkeley
Packet Filter)

** Security
*** [[https://cilium.io/][Cilium]]
*** Litmus
+ Repo: [[https://github.com/litmuschaos/litmus][litmuschaos/litmus]]

An opinionated approach inspired by "chaos engineering". It uses fuzzing-like
approaches to testing container/cloud configs. Used for stress-testing or
constructing experiments to see what could go wrong.

Basically "the opposite of GitOps", in spirit.

* Orchestration
** Rancher
*** Docs
+ [[https://rancher.com/docs/rancher/v2.6/en/][Rancher]] 2.6
+ Rancher's own [[https://github.com/rancher/charts][rancher/charts]]

*** Resources
+ [[https://rancher.com/docs/rancher/v2.6/en/best-practices/rancher-server/rancher-in-vsphere/][Installing Rancher in a vSphere Environment]]
+ [[https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/][Cluster Provisioning with Rancher]]
+ [[https://rancher.com/docs/rancher/v2.6/en/helm-charts/][Helm Charts in Rancher]]
+ [[https://rancher.com/docs/rancher/v2.6/en/overview/architecture-recommendations/][Architecture Recommendations]]
+ [[https://rancher.com/docs/rancher/v2.6/en/k8s-in-rancher/][Links to Kubernetes Resources]]

** Swarm

* Service Discovery
** Consul (hashicorp)

Requires configuration management.

*** Docs
+ [[https://www.consul.io/docs/intro][Getting Started]]

*** Resources


* Provisioning
** Terraform
*** Docs

*** Resources
+ [[https://github.com/scraly/terraform-cheat-sheet][scraly/terraform-cheat-sheet]]
+ [[https://jayendrapatil.com/terraform-cheat-sheet/][terraform cheatsheet]] (web)
+ [[https://menendezjaume.com/post/gpg-encrypt-terraform-secrets/][GPG Encrypt Terraform Secrets]]

**** Tools

***** Docs

+ terraform-docs ::
+ terraform-plugin-docs :: generate/validate terraform plugin/provider docs

***** Inventory

+ terraform-inventory :: from terraform state $\rightarrow$ ansible dynamic
  inventory
+ terracognita :: read from existing cloud providers (rev. terraform) and
  generates your infrastructure as code on terraform config.

***** Misc

+ tfsec :: security scanner (static analysis)
+ tflint :: linter

**** Nix/Guix

Guix packages for Hashicorp products are a bit sparse. This guide details getting Terraform

*** Topics


**** Code & Docs Generation

***** Snippets

There's [[https://github.com/staticaland/terraform-generate-snippets][staticaland/terraform-generate-snippets]], which autogenerates snippets
from =terraform providers schema -json=

***** LSP

+ terraform-ls :: LSP server

***** Project

+ tfautomv :: generate terraform moved blocks automatically (for refactoring)
+ terraform-index :: print the AST of an HCL file

**** Network Configuration
***** ZeroTier with Terraform

**** Project Structure

[[https://www.youtube.com/watch?v=IDLGpkRmDXg][Structuring Repositories For Terraform Workspace]] A good intro to some of the
considerations for processes/structure in teams.

[[https://www.youtube.com/watch?v=Qg8VZsbaXxA][4 Considerations To Structure Terraform Code]] A deeper dive into issues for more
modular projects

+ consider rate of change of project/environment components when
  refactoring. components that would otherwise sprawl across multiple proj/env
  could be contained.
+ don't import external modules that lack semantic versioning/tagging...
+ when components could benefit from a separate release cycle, it begins to
  justify the cost/time and process overhead to do so.
+ use folder/module structure to constrain the set of resources affected by
  changes. this limits risk.
+ submodules can be bad for code reuse (they usually problematize your workflow,
  but are very useful when they're the right tool)

Be aware of where Terraform state is stored:

+ the "Terraform workspace" layout is easy to get started with, but the state for
  multiple environments by default shares the same backend!
  - i.e. your dev/staging/prod state may share the same backend: anyone with
    access to one may implicitly have access to others.
+ The "file-tree" workspace will require multiple =terraform apply= commands per
  environment

Within a specific context (like a project's environment), it's sometimes useful
to split out modules into sequenced subdirectories, like the following. The
indexes can be used to guarantee that state transformations occur in order.

+ 0_keeper :: resources that must exist in all following stages of a terraform
  deploy. this stage may not need to be re-run on every deploy.
+ 1_netsec :: security rules for the above TF resources which may change often
+ 2_expressroute :: apparently, the Express Routes are something in Azure that
  you do not want to change, since it's hard to predict when TF backend decides
  your resources should be torn-down and recreated. For Azure Express Routes, if
  they are fully dropped & recreated instead of mutated, it results in a "devops
  equivalent" of locking yourself out of your router/firewall.

  The idea behind the last item is to limit the blast radius (& risk) that
  changes may create. It's generally a good idea to be confident about this,
  without needing to inefficiently /look/ for dependencies and state validation.

***** Following the meaning of senses from Plato's Republic:

+ the visual sense is one where you must direct your visual focus (or attention)
  consciously. You can't see everything all at once. Thus, for tech, you want to
  simplify the structure or narrow the scope of things requiring your active
  attention.
+ whereas your auditory sense allows you to react to things you didn't need to
  be focused on. This follows the "push notification" or "event-driven" means of
  messaging [queues].
+ The correct approach to structuring these projects implicitly limits the sets
  of information or the set of event-streams (automailers/notifications) that
  people need to tune into. However, you can't just shuffle things around on a
  whim.

.... okay not exactly germaine to the topic.


**** Modules

 Some notes from [[https://www.youtube.com/watch?v=7xngnjfIlK4&t=7408s][Complete Terraform Course]]

***** Module Sources

Types of module sources

+ Root Module :: implicit from local dir
+ Child Module :: separate module from local file

Examples of module sources

+ Local paths
+ Github (Forges)
+ Terraform Registry. Over +3000 official modules served (and they're counting!)
  with keys/values to learn & appreciate
+ HTTPS URL's
+ S3/GCS Buckets

***** Module Design

****** Good Modules:

+ Raise abstraction level (from HCL base types or resource types)
+ Group resources logically (or promote such grouping)
+ Expose input vars to enable customization/composition (like a modular synth)
+ Provide useful defaults
+ Return Outputs to make further integrations possible

****** Code Rot

The first two are exceedingly obvious. The latter two, less so. The last one is
fairly specific to Terraform.

+ Unpinned versions
+ Deprecated deps
+ Out of band changes
+ Unapplied changes

****** Managing Secrets

Mark vars using the =sensitive= keyword

Pass with:
+ TF_ENV_NOT_REALLY :: No don't actually do this without some protection
  - the FBI hiding on your box scrolling through your =top= ...
+ -var :: use this tf cmdline option (secrets manager) ... okay maybe
+ external secret store :: congratulations you won a kubernetes!
  - or "simply" nomad + a love of dynamic network configurations.  the choice is
    yours.

****** Basic Types

Primative: string, number bool

Complex (types are optional)

+ list<type> :: lists
+ set<type> :: sets
+ tuple[<type>,...] :: tuples
+ object{attr=<type>} :: objects
+ map<type> :: maps

According to HCL docs, the following types behave identically in most situations:

+ Lists and Tuples
+ Maps and Objects

***** Design Patterns

Maybe just "design motifs" because they're pretty small in scope.

****** Reuse or Create

Use alternate ternary statements on variables/inputs to =DROP IF EXISTS; CREATE=

#+begin_src hcl
resource "aws_route53_zone" "primary" {
  count = var.create_dns_zone ? 1 : 0
  name = var.domain
}

data "aws_route53_zone" "primary" {
  count = var.create_dns_zone ? 0 : 1
  name = var.domain
}
#+end_src

****** Lifecycle

create_before_destroy

#+begin_src hcl
resource "azurerm_resource_group" "example" {
  # ...

  lifecycle {
    create_before_destroy = true
  }
}
#+end_src

+ ignore_changes
+ prevent_destroy
+ terraform_remote_state


****** Meta-args on Module Imports

Can use meta-args like =count, foreach, provides, depends_on= here

#+begin_src hcl
module "webapp" {
  source ...
  input_var = "..."
}
#+end_src

******* TODO examples of using metaargs there? not in notes


* Virtualization Platforms
** vSphere

** Harvester

*** Docs
+ [[https://docs.harvesterhci.io/v1.0/reference/api/https://docs.harvesterhci.io/v1.0/reference/api/][API Docs]]: basically an API wrapper around Kubevirt

*** Resources

*** Issues

**** How to provision storage to Harvester Nodes/Guests?
+ usually requires Persistent Volume Claims (ala k8s)
+ see kubevirt resources

**** Can harvester support GPU passthrough?
+ The models listed in the [[https://docs.harvesterhci.io/v1.0/reference/api/][Harvester API Docs]] indicate so
  - however, these models also contain references to vGPU which is an nVidia feature.
  - Also, nVidia publishes a GPU Addon for "discovery" of nVidia GPU/vGPU's on
    the host. It's unclear whether the GPU model is synonymous with nVidia's
    plugin or simply confusing.
  - Regardless, neither the API model nor the Kubevirt addon should be required for GPU passthrough.
+ This [[https://kubevirt.io/user-guide/virtual_machines/host-devices/][should be possible]], if not through the Harvester interface then by:
  - adding device ID's to the =permittedHostDevices= in =KubeVirt CR=
  - then adding these devices to the KubeVirt VMI's

***** Possible Solution

Reference ArchWiki on [[https://wiki.archlinux.org/title/PCI_passthrough_via_OVMF][OVMF passthrough]]  It may require:

+ a script writing to =/sys/.../driver_override=
+ bios config
+ amd_iommu=on & updated grub
+ updates to /etc/mkinitcpio.conf
+ blacklisting drivers (probably shouldn't be installed on harvester anyways) and setting device driver to =vfio_pci=
+ manually setting up OVMF within Harvester
+ For Guix:
  - install a VM Guest without graphics and with few disks
  - add non-guix channels & substitutes, update/reboot
  - clone the VM to backup
  - update the system image to include GPU drivers
  - poweroff, change the VM config & reboot
  - And (of course) ensure Harvester never starts with the wrong monitors plugged in, since Asus BIOS doesn't allow setting priority on GPU device with any stickiness

** Proxmox
*** Docs
*** Resources
+ [[https://forum.proxmox.com/][Forums]]

**** Networking
+ [[https://pve.proxmox.com/pve-docs/chapter-pvesdn.html][SDN docs]]
**** Storage
+ 2012 monograph on [[https://pthree.org/2012/12/05/zfs-administration-part-ii-raidz/][ZFS administration]]
*** Issues
**** PXE Boot
+ [[https://www.reddit.com/r/homelab/comments/st3bji/proxmox_zfs_pxe_booting_with_grub_for_bios_systems/][Proxmox + ZFS - PXE Booting with GRUB for BIOS systems]]
  - [[https://rpi4cluster.com/pxe/setup/][How to boot Windows and Linux using UEFI net boot and iPXE]]
    - generic, but describes a setup
+ [[https://www.apalrd.net/posts/2022/alpine_vdiclient/][Net Booting the Proxmox VDI Client (feat. Alpine Linux)]]
  - moreso PXE booting a VM image with a custom version of Proxmox's spice
+ [[https://github.com/morph027/pve-iso-2-pxe][morph027/pve-iso-2-pxe]]
**** Encrypting Proxmox
+ [[https://herold.space/proxmox-zfs-full-disk-encryption-with-ssh-remote-unlock/][Full Disk Encryption with SSH Remote Unlock]] (from [[https://forum.proxmox.com/threads/howto-wrapper-script-to-use-fedora-coreos-ignition-with-proxmox-cloud-init-system-for-docker-workloads.86494/][proxmox forum]])
+ [[https://wiki.geco-it.net/public:pve_fcos][Fedora CoreOS Ignition with Proxmox cloud-init]] (from [[https://forum.proxmox.com/threads/howto-wrapper-script-to-use-fedora-coreos-ignition-with-proxmox-cloud-init-system-for-docker-workloads.86494/][proxmox forum]])

** oVirt


* Virtualization
:PROPERTIES:
:ID:       cf2bd101-8e99-4a31-bbdc-a67949389b40
:END:

** Kubevirt

This is a VM provider using a k8s interface (i.e. you do VM things using the Kubernetes API)

*** Docs
+ [[https://kubevirt.io/user-guide/architecture/][Main]] (architecture)
+ [[https://kubevirt.io/api-reference/master/definitions.html][API Docs]]

*** Resources
+ [[https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/][Provisioning Storage]]
+ [[https://kubevirt.io/user-guide/virtual_machines/host-devices/][Host Prep for PCI Passthrough]]

** QEMU

+ [[https://wiki.archlinux.org/title/QEMU/Guest_graphics_acceleration][QEMU graphics accel]] (wiki)
+ [[https://alyssa.is/using-virtio-wl/][A technical overview of Virtio WL]] (qemu/libvirt)
+ [[https://developer.ibm.com/articles/l-virtio/][VirtIO an I/O virtualization framework]]

*** Docs

*** Resources

*** Issues
**** Getting vm's to share integrated graphics
+ see [[https://www.reddit.com/r/VFIO/comments/i9dbyp/this_is_how_i_managed_to_passthrough_my_igd/][this reddit post]]
+ details for [[https://www.reddit.com/r/VFIO/comments/s0rwxl/gpu_passthrough_on_lenovo_legion_5_amd_laptop_so/][passthrough on Legion 5 AMD laptop]]
  - may also require copying vBios and/or flashing firmware

** Libvirt
*** Resources

**** virtio

+ [[https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.pdf][Virtual I/O Device 1.2 Spec]]
+ [[https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/tex/][LaTeX source for the documentation]]

*** Tools
**** virt-manager

**** virsh

**** virt-install

**** cockpit-machine
+ Running [[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_virtualization/configuring-virtual-machine-network-connections_configuring-and-managing-virtualization][Virt-Manager with Redhat/Centos]]

*** Issues

**** [[https://wiki.libvirt.org/page/TLSSetup][Setting up libvirt for TLS (Encryption & Authentication)]]
**** Adding an ISO after setup
  - virt-install :: [[https://serverfault.com/questions/833131/virt-install-cannot-use-iso-file-as-location][mount as iso]] and pass to =--location=
    - mount as a loopback device
      - =mount -t iso9660 -ro loop /dir/cdimage.iso /mnt/iso=
    - also pass loop device to guest
  - image in pool ::
**** Bridging a WIFI device
You can't, apparently. You can [[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-managing_guest_virtual_machines_with_virsh-attaching_and_updating_a_device_with_virsh][redirect a USB device]]
**** Redirecting a USB Device

+ Run =lsusb= to get the vendor/product ID
+ Create an =$xml= file defining it
  - bonus points for using =emmet-mode=
  - insert the vendor/product id like =0x1234= for hex
+ use =virsh list --all= to get the =$domain=
+ run =virsh attach-device $domain --file $xml --config= to attach
  -  use similar =detach-device= to remove it


** Admin Tools
+ dnsmasq :: dns
+ dhclient :: dhcp
+ dmidecode :: SMBIOS table, hardware compat/interoperability
+ ebtables :: NAT networking on the host
+ bridge-utils :: create virtual networking devices: TUN/TAP, bridge
