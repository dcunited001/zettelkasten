:PROPERTIES:
:ID:       4ab045b9-ea4b-489d-b49e-8431b70dd0a5
:END:
#+TITLE: Data Science

* Resources
** Blogs
+ [[https://blog.christianperone.com/category/machine-learning/][Christian Perone (ML)]]

** Cheatsheets
+ [[https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5][Essential Cheatsheets for machine learning]]
  - also [[https://github.com/kailashahirwar/cheatsheets-ai][kailashahirwar/cheatsheets-ai]]
+ [[https://www.theinsaneapp.com/2020/12/machine-learning-and-data-science-cheat-sheets-pdf.html][Insane App: ML Cheatsheets]] (kinda crazy, but a lot of them)
+ [[github:mavam/stat-cookbook][mavam/stat-cookbook]]
+ [[https://github.com/rstudio/cheatsheets][rstudio/cheatsheets]]
+ [[https://github.com/aaronwangy/Data-Science-Cheatsheet][aaronwangy/Data-Science-Cheatsheet]]
  - inspired by [[https://github.com/ml874/Data-Science-Cheatsheet][ml874/Data-Science-Cheatsheet]]
  - inspired by [[github:wzchen/probability_cheatsheet][wzchen/probability_cheatsheet]]

* Languages

** R

** Matlab

** Julia

Works quite a lot better if you figure out how to forego python dependencies
(e.g. use Flux.jl instead of tensorflow/etc). It's amazing how few lines you can
implement a simple gradient descent problem in.

*** Flux.jl

+ Flux.jl example  [[id:3a7412c7-f75b-4772-85d9-015da383efbc][Attempt to solve a statics problem with Flux.jl]]


* Frameworks

** JAX

This is like Julia, but with python.

*** Docs
+ [[https://jax.readthedocs.io/en/latest/notebooks/quickstart.html][JAX quickstart]]

** Torch


*** Docs

*** Resources
+ [[https://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/][PyTorch Internal Architecture Tour]]

*** Topics

**** Triton

Basically the torch equivalent to TF XLA

+ [[http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf][Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations]]

* Multimedia

** Mediapipe

*** Docs
+ [[https://github.com/googlesamples/mediapipe][googlesamples/mediapipe]]
+ [[https://github.com/google/mediapipe#publications][publications]]
+ [[https://developers.google.com/mediapipe/solutions/guide.md][solutions guide]]
  - a new landmarks detection solution is coming soon
  - the old version is available [[https://github.com/google/mediapipe/blob/master/docs/solutions/holistic.md][here]]

*** Resources

+ [[https://google.github.io/mediapipe/solutions/pose.html][Mediapipe Pose]]

*** Topics

**** Human Pose Estimation

***** Landmarks

By type

#+begin_src python
import mediapipe as mp

lips = set()

for elem in mp.solutions.face_mesh_connections.FACEMESH_LIPS:
    lips.add(elem[0])
    lips.add(elem[1])
#+end_src

*** References

[[https://arxiv.org/pdf/1906.08172.pdf][MediaPipe: A Framework for Building Perception Pipelines]] a 2019 paper from
Google, [[https://scholar.google.com/scholar?cites=17822221160103463682&as_sdt=5,47&sciodt=0,47&hl=en][cited 300+ times]]:

+ [[https://doi.org/10.3390/app13042700][Human Pose Estimation Using MediaPipe Pose and Optimization Method Based on a
  Humanoid Model]]
+ [[https://arxiv.org/abs/2304.01555][Real-time Driver Monitoring Systems on Edge AI Device]]
+ [[Gesture Recognition of Sign Language Alphabet Using aMagnetic Positioning System][Gesture Recognition of Sign Language Alphabet Using a Magnetic Positioning System]]
+ [[https://doi.org/10.3390/s22072656][Context-Aware Automatic Sign Language Video Transcription in Psychiatric Interviews]]
+ [[https://doi.org/10.1145/3519391.3519396][Understanding Challenges and Opportunities of Technology-Supported Sign
  Language Learning]]


**** From [[https://doi.org/10.3390/electronics11193228][MediaPipe’s Landmarks with RNN for Dynamic Sign Language Recognition]]

***** Overview

+ Combines Mediapipe with RNN: GRU, LSTM and Bidirectional LSTM.
+ Creates DSL10-Dataset
+ Contrasts methods, including those for SSL (static sign language)

***** Notes on other research:

+ Recent approaches from [8-12] "included gyroscope, Kinect, accelerometer,
  electronic gloves, and depth camera" but were still restricted to a few simple
  gestures
+ A multi-sensor approach [14] has "user compfort drawbacks"
+ [18] used RCNN with video
+ [19] used a "selfie mode sign language video methodology" for Indial ASL
+ [20] used "fuzzy classification"

***** Static Sign Language

+ [27] used YCbCr color space to handle skin tones vs lighting conditions.
+ [28] achieved 92.9% on Arabic Sign Language using 1080 images.

Feature extraction techniques from [26] on static photos

+ HOG :: Histogram of oriented gradients
+ ZIM :: Zernike Invariant Movements

***** Methodology


***** Dataset

Related code on github: [[https://github.com/gerges-hanna/Sign-Language-Recognition][gerges-hanna/Sign-Language-Recognition]]. This includes an
extended DSL46-Data dataset (in [[doi:10.3390/s23010002 ][10.3390/s23010002]])

+ DSL10: license unclear (it is stated that people can use it, the paper is
  under CC-BY 4.0, but data is distributed separately from article)
+ [[https://osf.io/t92sd/][DSL46]]: licensed for educational purposes (written, but not declared as
  license)

DSL10-data produced with the following rules:

#+begin_quote
1 Signer body: the full signer’s body must appear in all the frames of the video as shown in Figure 8A.

2 Signer movement: the whole movement details must be clear and bounded between the camera frame as shown in Figure 8B.

3 Background: it is better to record the dataset in a stable background that does not contain any other hands or faces except those of the signer.

4 Lighting: it is preferred to record in good lighting conditions to make sure all the keypoints will be clear as shown in Figure 8C.

5 Camera: set up your camera on a fixed stand to ensure that the videos are as unshakable and focused as possible as shown in Figure 8D.

6 Video duration and frame count: the clip duration and number of frames should be determined before the recording process.

7 Quality: any camera with a 640 × 480 resolution sensor can be used for the recording process since the most common sensors on the market are available in this size or higher.
#+end_quote


* Docker Images

** AMD [[https://hub.docker.com/u/rocm][ROCm]]

I think I need /both/ pytorch /and/ tensorflow on a Jupyter image. Otherwise,
I'mma stop and start that shit every five seconds. It is what it is.

*** [[https://hub.docker.com/r/rocm/pytorch][rocm/pytorch]]

The Dockerfile for rocm/pytorch:latest should set the =PYTORCH_ROCM_ARCH=
variable. There are no code references for this variable in the
[[github:ROCmSoftwarePlatform/pytorch][ROCmSoftwarePlatform/pytorch]] fork, but it does show up in the [[https://github.com/ROCmSoftwarePlatform/pytorch/wiki/Building-PyTorch-for-ROCm][Building PyTorch
for ROCm]] wiki page:

See README.md notes on [[https://github.com/ROCmSoftwarePlatform/pytorch#docker-image][Docker Image]] ... nevermind, this dockerfile receives
upstream updates, but still builds for nvidia.

*** [[https://hub.docker.com/r/rocm/tensorflow][rocm/tensorflow]]

The [[https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/tree/develop-upstream/tensorflow/tools/ci_build][tensorflow dockerfiles]] are found in the =tools/ci_build= directory of the
[[github.com:ROCmSoftwarePlatform/tensorflow-upstream][ROCmSoftwarePlatform/tensorflow-upstream]] fork.

+ Look for recently updated dockerfiles.
+ It would seem that the main rocm/tensorflow:latest is in [[https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/blob/develop-upstream/tensorflow/tools/ci_build/Dockerfile.rocm][Dockerfile.rocm]]

* Data Sets

** ASL

*** [[https://huggingface.co/datasets/ncslgr][ncslgr]]

TF: [[https://www.tensorflow.org/datasets/community_catalog/huggingface/ncslgr][huggingface:ncslgr/entire_dataset]]



** Language

*** [[https://universaldependencies.org][Universal Dependencies]]

TF: [[https://www.tensorflow.org/datasets/catalog/universal_dependencies][tfds.datasets.universal_dependencies]]. Load with Builder:
=tfds.datasets.universal_dependencies.Builder=


* In-Memory Reps
** Pandas
v2.0.0 includes datatypes backed by Apache Arrow.

*** Topics
**** Using pyarrow
Using a pyarrow backend enables integer representations alongside nullable
data. Before v2.0.0, these would automatically convert the column to a float,
requiring more memory or more intensive CPU ops.

** Apache Arrow

** Polars

* Workflows

** Common Workflow Language

* Tools

** Google Colab

Cloud notebooks

** Data Visualization

*** Meshplot

Visualize 3D data from Jupyter with pythreejs (repo: [[https://skoch9.github.io/meshplot/tutorial/][scotch9/meshplot]])



* Optimizers

** Resources
+ [[https://www.ruder.io/optimizing-gradient-descent][ruder.io/optimizing-gradient-descent]]
