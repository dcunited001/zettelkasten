:PROPERTIES:
:ID:       4ab045b9-ea4b-489d-b49e-8431b70dd0a5
:END:
#+TITLE: Data Science

* Resources

** Cheatsheets
+ [[https://startupsventurecapital.com/essential-cheat-sheets-for-machine-learning-and-deep-learning-researchers-efb6a8ebd2e5][Essential Cheatsheets for machine learning]]
  - also [[https://github.com/kailashahirwar/cheatsheets-ai][kailashahirwar/cheatsheets-ai]]
  -
+ [[https://www.theinsaneapp.com/2020/12/machine-learning-and-data-science-cheat-sheets-pdf.html][Insane App: ML Cheatsheets]] (kinda crazy, but a lot of them)
+ [[github:mavam/stat-cookbook][mavam/stat-cookbook]]
+ [[https://github.com/rstudio/cheatsheets][rstudio/cheatsheets]]
+ [[https://github.com/aaronwangy/Data-Science-Cheatsheet][aaronwangy/Data-Science-Cheatsheet]]
  - inspired by [[https://github.com/ml874/Data-Science-Cheatsheet][ml874/Data-Science-Cheatsheet]]
  - inspired by [[github:wzchen/probability_cheatsheet][wzchen/probability_cheatsheet]]


* Languages

** R

** Matlab

** Julia

Works quite a lot better if you figure out how to forego python dependencies
(e.g. use Flux.jl instead of tensorflow/etc). It's amazing how few lines you can
implement a simple gradient descent problem in.

*** Flux.jl Example

The struggle in statics was real, as was [[id:a226f047-8a95-42e4-8c55-7c055a1d5fc2][configuring a Julia environment that
behaved reasonably in both Arch and on Guix System.]]

+ Usages of =@bind= are artifacts from running in a Pluto notebook.
+ The code likely contains syntax/runtime errors, as I was shuffling code/vars
  around, attempting to solve an improperly(?) derived equation.
+ Given appropriate constraints via a solvable equation, it was converging
  quickly.
+ I used SymPy to validate my assumptions

#+begin_src julia :eval no
using Markdown
using InteractiveUtils

using Plots, Pluto, PlutoUI, HypertextLiteral
using LinearAlgebra, Statistics
using SymPy, Flux
using AbstractPlutoDingetjes.Bonds, AbstractPlutoDingetjes

# From Integration: `12000(2*14)/35 = x(w2(x-1) + w1)`, where x = 35

#### Problem Setup

# pounds/in
w0 = 12000.0
l0 = 14.0
l12 = 35.0

# kips/ft
w0 = (w0/1000.0)/12.0
l0 = l0/12.0
l12 = l12/12.0

R = [1.0 34.0]
totalWeight = [(w0*2*l12)/l12]
# totalWeight = [(12000*28.0)/35.0]

#### Test Solving with SymPy

# @syms w1,w2
# load1 = (l12*w2)
# load2 = (l12*(w1-w2))/2
# loadTotal = load1 + load2
# sumFy = loadTotal - 12000

# #w1sol = solve(sumFy, w1, w2)[1]
# #solve(sumFy,w1=>w1sol[1],w2)
# p12 = ((l12*[0.5 2/3.0]*[load1;load2])/loadTotal)
# solve(p12,w1)

#### Setup loss/prediction functions

function predict(x)
	R*x
end

function loss(b)
	ŵ = R*θ
	# sum((w - ŵ).^2)
	sum((totalWeight - ŵ).^2)
end

# from sympy
# x=l12: x*(w1/6.0 + w2/6.0)/(0.25*w1 + 0.25*w2)
r1 = [1/6.0 1/6.0]
r2 = [1/4.0 1/4.0]
function loss2(b)
	ŵ = (r1*θ)[1]/(r2*θ)[1]
	(totalWeight[1] - ŵ)^2
end

#### Model Hyperparameters

α = 0.0000001
# global θ = [40.0;500.0] # pounds
global θ = [1;1.0] # kips
# predict(start)
# loss(start,totalWeight)

#### Run Gradient Descent

global v = θ
global gg = []
for i in (1:100000)
	# gs = gradient(() -> loss(totalWeight), Flux.params(v,totalWeight))
	gs = gradient(() -> loss2(totalWeight), Flux.params(v,totalWeight))
	v′ = gs[θ]
	v .-= α .* v′
end

[gg,loss2(v), v, r1, r2]

# [R, v, R*v]
# [loss(v), v, v′]

#+end_src

Also, social isolation really sucks. Overall going back to school has helped
with this a ton.

#+begin_quote
I basically failed the class because I wasn't working in the Fablab and I was
never on campus that summer. I had assumed that the student tutoring service
there was closed as well ... I was so hyperfocused on trying to study (knowing
zero people in the same class) that I never really pressed the issue or went to
the school to figure it out. I really drop the ball sometimes ... but I know
like ZERO people in my life. It's gotten much better at school since then and
it's because it's an online class & I didn't ask for help ... but one simple
thing would have sidestepped most problems and routed my energy towards being
effective: social connection.

In high school, I was the kind of student that always needed to borrow a pencil
or ask about homework at the last minute, yet I was never challenged with the
course material. Whether in IB Biology or Calculus BC which mostly got through
Taylor Series, I mostly slept through everything. I was not culturally prepared
for college and no one around me knew. My course attendance rates at Virginia
Tech from 2004-2006 were like 20-50% depending on the week and semester. I spent
the entire time travelling to skate.
#+end_quote

It's just incredibly difficult to repair someone's social health when their
social life has bottomed out (there's no bottom it's like an abyss). Not enough
time had really passed since I started back at school for connections to form
and I have attachment disorder (and etc) issues that make it incredibly
difficult to form new meaningful relationships.

Not everything that can be broken can be fixed.

* Frameworks

** Torch

** Tensorflow
*** Topics
**** Keras
The high-level layers API, more or less.

* In-Memory Reps
** Pandas
v2.0.0 includes datatypes backed by Apache Arrow.

*** Topics
**** Using pyarrow
Using a pyarrow backend enables integer representations alongside nullable
data. Before v2.0.0, these would automatically convert the column to a float,
requiring more memory or more intensive CPU ops.

** Apache Arrow

** Polars

* Workflows

** Common Workflow Language

* Tools

** Google Colab

Cloud notebooks
