:PROPERTIES:
:ID:       e93eebbe-d702-46e6-864e-114fc4e61fc5
:END:
#+TITLE: Video
#+CATEGORY: topics
#+TAGS:

* Docs

* Resources

* FFMPEG

** Issues

*** Convert Video To GIF

[[https://superuser.com/questions/1049606/reduce-generated-gif-size-using-ffmpeg][Create PNG Palette, Then Convert]]

#+begin_src shell
ffmpeg -y -i file.mp4 -vf palettegen palette.png
ffmpeg -y -i file.mp4 -i palette.png -filter_complex paletteuse -r 10 -s 480x320 file.gif
#+end_src

* OBS Studio

** Docs

** Resources

** Topics

*** MQTT

MQTT may be appropriate in lieu of 0MQ.

**** obs-mqtt-status

There is a [[https://github.com/glikely/obs-mqtt-status][glikely/obs-mqtt-status]] project, which receives updates from OBS,
forwards them to MQTT intermediary. When listening devices connected to the MQTT
broker are forwarded messages, it changes the color of a display to indicate
recording state. (the code to perform the latter isn't in the project, per se)

*** 0MQ


**** Remote Control

For remote control of files rendered by OBS. This prevents a safe-ish boundary
between applications that manage renderable state.

***** Example: Update on screen text

Render on screen text, which e.h. may contain _most recent emacs command_ and _most
recent file_. Emacs has 0MQ bindings, used with jupyter.el.

+ 0MQ messages from Emacs received by 0MQ server listening to socket. Or 0MQ
  messages may contain current state of network devices, rendered on screen as
  things come up & go down.
+ 0MQ server runs in either OBS application extension or there is a file-based
  boundary between 0MQ server and OBS -- i.e. OBS reads file, does _not_ manage
  state of a 0MQ plugin, etc. This avoids a dump-truck full of problems (it also
  allows processes running elsewhere in the network to update the rendered file)
+ Messages contain short strings or data structures with guarantees on
  compactness.

**** Stability Guarantees

Assuming:

+ The 0MQ server type-checks received messagse
+ That specific components in the OBS cannot crash or crash "safely"
+ That OBS handles crashes by replacing receiving renderables with safe data
  structures

Then it's sufficiently stable -- this may need some work. If stability is an
issue, then OBS can render from a file and (I believe) updates on-screen
rendered text if file changes.

*** OCR

**** [[https://github.com/dram55/MarioMaker2OCR][Mario Maker 2 OCR]]

+ Extract text with OCR and insert into Video Stream Overlay
+ Written in C# and uses OpenCV/Tesseract (libs wrapped by [[http://www.emgu.com/wiki/index.php/Main_Page][EmguCV]] for .NET)
+ [[https://github.com/unosquare/embedio][EmbedIO]] provides a .NET-based Websocket/HTTP server interface
+ Example video: [[https://www.youtube.com/watch?v=myG9h01B4Bs&t=445s][Karibukai Play]]

**** Crowdsourced OCR Setup

also, a smarter way to solve this would be to crowdsource the translations. set
up a chat command that forwards suggested translations to another chat
channel/room. then someone running your stream in the backend gets a list of
suggested translations along with:

- an "up/down" function to promote/demote the user making suggestions
- a function to pin suggested translation on-screen.

this way, you're only pinning on-screen what is relevant to the audience while
you're encouraging audience participation. it's also much more resilient:

- it doesn't require multiple OBS/OCR/etc configs per game
- it requires a web service and most everything else can be set up by someone
  who can write/customize a discord chat bot

**** Hypothetical OCR Setup

#+begin_quote
Responding to a reddit post on OBS/OCR with translation
#+end_quote

Translating the text is going to require a few moving parts.

I don't have much familiarity with the OBS or OCR ecosystems, but this
[[https://github.com/dram55/MarioMaker2OCR][MarioMaker2OCR]] project solves some of the problems.

Having a PCIe capture card would distribute the load across multiple computers,
which itself would require some networking/automation. Without that or multiple
GPU's, it's likely that the load from the Game, OCR and additional transcoding
would be intermittent.

This would require several services:

- OBS
- OCR-service
- Translation-service

You'd need to transcode several times and decide what kind of workload you want
the OCR to have and how its output is encoded back onto the output video stream:

- receive streaming video source into OBS
- either duplex that stream (fast, but potentially brittle req. networking) or
  have OBS transcode a video output to make available to OCR (slow, but easy;
  also brittle if OBS crashes/lags)
- at this point, sending a trigger to OCR would be useful. this could be done
  with an FFMPEG script that takes a screenshot to forward to the OCR, so OCR
  isn't actively processing it's input stream the entire time.
- layer on OCR output into video -- again, creating an image-based processing
  flow here or using an capture card would lighten the load.
- have the OCR service send it's output to a translation web service, given some
  conditions. it receives it's HTTPS translation response and then another
  service makes a file available to OBS as a composited image
- have OBS combine the final video streams as output from OBS to streaming
  server

This is what makes sense to me as a Linux guy. Depending on how OBS responds to
file updates between Windows/Linux implementations, it may be able to detect
changes in a file. In this case, you can just create a new image file with
ImageMagick and update a symlink (not in Windows)

Ultimately, there are a few UI/UX problems to be solved:

- is there background content you don't want to translate (background images
  with Kanji)
- do you always want to translate text boxes?
- where do you place the translated output on stream?

It's easier to solve these problems if you have an app running on a streaming
station with the capture card that displays the video on an overlay and
translates content given a mouse click. However, it's a bit tough to get any of
this to work, since it would require game-specific settings. Unless you're using
Ansible/Git to automate a VM that gets a capture card on passthrough, whatever
you set up will eventually become buggy and cease to work. (Windows is smoll
brain)

So then with multiple computers, there are other problems to be solved:

- automation/network
- testing and backing up the OBS & OCR service configs
- handling service failure

Getting something like this to work on a single workstation would require less
networking/automation, but still it's likely that the configs/environment would
be brittle.
*** Ansible
