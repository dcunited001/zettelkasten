:PROPERTIES:
:ID:       994cd777-d937-4c01-8387-74588b71b349
:END:
#+TITLE: AI: Enforcing policy on model output depends on XAI
#+CATEGORY: slips
#+TAGS:

* Roam
+ [[id:cea7d11c-8357-4e4f-90b3-fa8210eff796][AI]]

* Notes

Most ML models that need policy-enforcement tacked on need it layered on top of
an index built from Explainable AI ... or should be delivered as separate
products.

Let me write that one more time, for the liberals in the back:

#+begin_quote
_if you want to censor/restrict the usage or output of an ML model (or just
enforce policy) this necessarily requires the product/model to be more
complicated._

You can usually strip out the policy layers from within ... and if not then you
can jailbreak it ...
#+end_quote

It requires more training, more compute, more storage, more water to cool, more
carbon to offset, more energy to drive price distortions ... oh and it's likely
to be more dysfunctional.

And the increase to compute is not small ... just generally $O(.)^{2}$ is a good
rule. I mean do people even know why they do what they do? No. Do you want to
rationalize every thought or action to others to be critiqued? Why not? It's
complicated and requires time/energy..

* Potential options

I've known this for a long time. It's about as paradoxical as Beijing's brief
flirt with blockchain LMAO. There are ways to address the need to enforce policy
on XAI, but yeh. Good luck with that. Objective reality is just simpler and
objective knowledge is simple to manage.

Here's a few ways to deal with the XAI problem

+ Changing the model outputs or interface.
+ Getting several different analyses different clusters in the model or arranged
  along the axes of various parameters.
+ Focusing on counterfactual analysis without too much causal statistics
  (develop future possibilities instead of relying on past data to determine it)
+ Emphasizing qualitative factors or narratives/outcomes/habits. Computers suck
  at existential things (angst is literally a state where you can't compute fast
  enough)

* Shapley Values

+ The [[https://christophm.github.io/interpretable-ml-book/evaluation-of-interpretability.html][Interpretable ML book]] covers the subject. I haven't read it, but the
  section on [[https://christophm.github.io/interpretable-ml-book/shapley.html#advantages-16][Shapley values]] conveys the complexity there.

[[https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap][Kernel]] SHAP: $O(T*L*2^M)$

[[https://christophm.github.io/interpretable-ml-book/shap.html#treeshap][Tree]] SHAP: $O(T*L*D^2)$ works for Decision Tree-based models

And you need access to the data, so it don't work too great for the end user --
but hey! that's an upsell Big AI goes up quarter of a point. Because "privacy is
sacred to the individual" or some bs.
