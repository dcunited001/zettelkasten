:PROPERTIES:
:ID:       e8893914-20e8-469f-9736-f3b51000352b
:END:
#+TITLE: Data Science: Why Not ZMQ
#+CATEGORY: slips
#+TAGS:  
* Roam
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
  
* Notes

And i like Nix/Nixos. i really do... but i don't love it. Guix made the
conscientious choice to keep more eggs in less baskets. I don't think they like
criticism though. idk. that's more of a comparison, but I got the impression a
few years ago that there's a "thing" they have with Guix.

again, =¯\_(ツ)_/¯= and python on Guix /is/ "missing" a lot of packages. but do you
realllly want them?

I'd rather have a zmq backend that knows how to read jobs and maybe has shared
memory. I mean jupyter _exactly_ that (at least the first part) for
ipython-compatible kernels. NVMe disk is close enough to RAM speeds and it's not
like the calling application needs to keep tabs on intermediate states -- okay
maybe it does, sometimes. but, does it really? the complexity in "interface
impedance mismatch" has to go somewhere. do you /really/ want that somewhere to be
boilerplate?

The ZMQ thing would basically define a jobs interface and both sides would need
to be somewhat aware of job dependencies. This is basically what a lot of the
large-scale ML workload orchestration software stuff is doing.

+ You'd need to define some standard for the interface and a use a
  contract-driven development (CDD) paradigm for defining what was supposed to
  happen when the job completed.
+ The CDD piece would need to be generated from the maths. So, application needs
  $XYZ_ijklmn$ and $ABC_{iknpq}$ tensors to contract along =ikn= indices. It can
  define what the result looks like and how its been transformed/pooled/etc
+ The ZMQ orchestrator could either be statically/dynamically compiled and
  should also integrate other programs/modules... somehow. Build once, deploy
  many times. No high-level runtime necessary
+ You'd like the orchestration piece to be as static as possible and the
  memory-sharing to permit usage of NVMe to pass data over a fence. So idk what
  design is best there

The front-end should really be fully decoupled from the data processing. No more
typescript/javascript in my python projects, jesus. The best parts about this:

+ applications are more modular. frontend does frontend stuff.
+ it should, in theory, decouple to permit light network-based jobs
+ flexibility in the backend that can instantiate datastructures from large
  blocks of storage, manage parallel copys of them and GC what's not needed
+ no more "ATLAS/LAPACK" for my GPU/TPU/etc

But that's all good, in theory and on a cocktail napkin. In practice everyone
has all this garbage slop code and the best way to fix it? [[https://www.youtube.com/watch?v=QzBxzvC6nV0][Burn that =!@%^$&!@$=
down]]. Why should people prob up a garbage paradigm? Obviously, using computer
algebra as a basis to define tensor operations for CDD is like cake right?

I'm pretty sure what I'm describing is just Apache Spark. Turns out those OASIS
folks already did everything the ITU couldn't do with C structs.
