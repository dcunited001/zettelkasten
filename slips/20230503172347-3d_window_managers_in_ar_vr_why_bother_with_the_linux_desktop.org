:PROPERTIES:
:ID:       4c5a5c7c-a47d-4b17-b43e-825485c00d98
:END:
#+TITLE: 3D Window Managers in AR/VR: Why Bother With The Linux Desktop
#+CATEGORY: slips
#+TAGS:
* Roam
+ [[id:bdae77b1-d9f0-4d3a-a2fb-2ecdab5fd531][Linux]]
+ [[id:39e14ffe-75c9-49e6-b852-6c492c4ee3e0][KDE]]

* Automating Interactive Brokers (IB)

This is an example of a well-designed application that can be well-integrated
into a Linux Window Manager. Offhand I don't know how the application
specifically structures its window classes or titles. However, with a bit of
work, it would integrate /very well/ into an XMonad or I3-based tiling window
manager.

The "Stock Trader's Desktop" is relatable example for a use case that needs to
be highly customized is a Window Manager running a stock trader’s terminal. You
need lots of dynamic visibility. You want highly configurable tiling window
arrangements — it could benefit quite a bit from XMonad’s approach at
configuring/launching window arrangements.

* Your Computer Shouldn't Make You Stupid Or Dependent

Your computer should be a tool. The tool should be something you use, not
something that uses you.

Most use cases are hard for the average person to relate to since the only
people who use linux are developers/systems/DevOps people.

And to be clear: I'm not saying 100% of people need to be capable of
understanding all this. Instead I'm saying that society needs about 5% of people
to comprehend these things.

+ To hire a plumber, you need to recognize there's a problem.
+ To remodel your kitchen, you need to understand how you want to use it, to
  imagine the possibilities were it to change and to imagine how you would like
  to use it.

Instead, our population has between 0.1% and 1% who presently have the ability
to reason about UI/UX and automation in the way that the should. I'm being
generous here. Most people don't care.

And technology should be accessible. To employ the "automobile" example that
compares implementation to interface: we shouldn't limit the set of people who
drive cares to the set of people who could design/repair one ... but we do need
more people to be capable of that.

** AR/VR are doomed

There are classes of problems that plague AR/VR adoption:

+ Business Problems
+ Branding Problems (after poor customer reactions)
+ Economic Problems
+ Software/Hardware Platform Problems:
  - Computing Topology problems (you need wires. you want wires)
  - Sensor Integration problems (these should maybe be wireless)
+ Other platform problems (oh you have an Xbox? well shit, I guess we're never
  playing street fighter then, damn)

But the biggest one: cultural problems. Our population just isn't capable of
configuring these devices for anything other than video games.

Oh and VR/AR will be an abysmal failure as long as there aren’t more than 1% of
people who can configure Linux WM automations/workflows. The only solution for
reducing the complexity of configuring VR/AR application integrations is AI, but
AI can’t replace your imagination: you still have to indicate to the AI what you
want the computer to do.

As long as configuring SMTP with SSL is too complicated for the average person,
then any VR/AR-based desktop for multitasking will fail abysmally. It will be
more of a nuisance to use, since without configuring integrations (thing i3 in
3D or 3D HTML/CSS) you can’t really get the display to render more than one
VR/AR application simultaneously without overstimulating the users.

IoT has similar issues.

** How about a 3D HTML/CSS Or A 3D Tiling Window Manager?

Will society end up with a system for AR/VR like what is described here?

[[https://github.com/dcunited001/dcunited001.github.io/blob/sources/_drafts/virtually-rifted-yet-prodigiously-gifted.md][virtually-rifted-yet-prodigiously-gifted.md]] (2015-2017)

No, probably not.

Note particularly the sections on using category theory for [[https://github.com/dcunited001/dcunited001.github.io/blob/sources/_drafts/virtually-rifted-yet-prodigiously-gifted.md#mapping-identical-spaces][mapping/transforming
spaces]]. So you sit down at some "desk-shaped" surface and your 3D XMonad
workspaces arrange themselves according to various rules. When you want your
clock widget on the desk, it's there and the Window Manager knows how to fit
things onto a general surface according to rules established by some 3D CSS or
something.

This enables "AR popup games," initially for locally networked users, then
eventually for globally-networked users.

** It's the Third Dimension That Complicates Things

Any time you add dimensions to designs or problems, it undermines many
assumptions you didn't realize you were making and it forces you to
generalize. In doing so, you lose a lot of the simplicity.

So when configuring 2D desktops on 2D displays, there are limits to the
arrangements of things. You can neatly divide 2D Window Managers into "Floating"
or "Tiling" or "Hybrid."

*** For example: 2D taskbars:

+ your WM or compisitor can make assumptions about where the taskbar is and how
  many pixels it occupies
+ that taskbars can have two side-to-side margins and its interior elements can
  have four margins, four paddings and simple width/height.
+ effects like transparency or highlights/shadows are simple for compositors to
  optimize.
+ there's no "screen-door" effect that requires something like Tensorflow to
  properly handle font smoothing

*** And 3D taskbars:

+ You need some clever means of deciding where the XDG app widgets go. In 2D,
  these can just go to the right/left or somewhere between two taskbar elements.
+ You may use cubes to determine margins/paddings and associated elements, but
  what do you name them?
+ Is your coordinate system left-handed or right-handed? Do your users know what
  those are?
+ Can the HTML approach to DOM trees work for 3D? This works for 2D and CSS tree
  transformations on DOM because many assumptions can be made. How much more
  complicated are those assumptions, given the extra dimension?

*** And that is just one little thing

Maybe this is why the architecture for Wayland compositing is ... a little
different. It would make sense if applications were capable of handing their
presentation or rendering to multiple display presentations (2D or 3D).

That you would even have a taskbar is perhaps an assumption or bias that only
exists because that displays are 2D is itself an assumption -- and one that
almost every extant application/software makes, by the way. So how do you handle
everything that already exists?

And does the need for backwards compatibility fit neatly into your creative
solution that defies existing conceptions of human-computer interaction? Too
bad.

There's no doubt that academics, large corporations and government contractors
have thought long and hard about these issues.

** Step 1: Gamestop Gamma Squeeze, Step 2: ???

I actually had a vision for how to use Gamestop's massive stock bubble to
transform their existing retail space into a business whose model would be to
prototype space. My plan included the PR necessary to sell shares at the
inflated price while retaining the sky-high stock price, so I was taking it into
account.

If some other small business were to do the same thing. So for an AR/VR center
offering telepresence and dynamic escape room experiences, where the proper
systems/sensors are all configured maintained, the I think I calculated that
your smaller business would need:

+ $250,000 upfront, $1,000,000 additional to compensate for 3 years operating
  expenses. The business would need at least a dozen employees,
+ It would require additional revenue streams (like a capucino machine), making
  the operations dicey at best.
+ and if it didn't make $300,000 within a year and $1,500,000 within three
  years, you fail.
+ about fifty different risk factors are in the red zone.
+ oh and you need things like devops contracting and some very generous software
  developers who are willing to work for bragging rights.

So, basically, for any small business upstart to do this, you'd basically be
lighting cash on fire. I can't remember how I calculated costs/prices, but they
were insane (like $50-250 per person, per hour) or something ... Yeh, but maybe
if you robbed a train or robbed some shady hedge fund, you'd have what you need.

I didn't think about this very long or take it seriously mind you -- but perhaps
it would have been smart for society to take me seriously once in a while
because whatever it is, I'm probably better at thinking about it then you
are. Maybe I can't "execute" or whatever ... but didn't Steve Jobs just make
that somebody else's problem?

Did anyone else actually believe the Gamestop thing was anything other than a
hazardous bubble? Did anyone actually have a plan or a vision?

I DID. It was smart, but maybe wouldn't work out. It certainly wasn't realistic
to expect myself to gain the contacts/trust required to get control needed to
act as interim CEO (or ... whatever).

* Wherefore Art Thou, KDE Activities?

In late 2019 or early 2020 (and in earlier Linux installations), I remember
spending quite a bit of time working to set up KDE Activities ... just to
discover that the feature had been implemented in 2011 and almost
abandoned. Since then, it's always been prominently featured in the UI. I
couldn't figure out how to use it. Really, there are no more

Furthermore, the KDE configuration setup is at odds with it's features for
customization. The most depressing part of this: fixing this requires
comprehensive changes to QT. I submitted [[https://bugs.kde.org/show_bug.cgi?id=468997][a feature request on KDE's bugtracker]]
describing what I'm looking for. It looks like it would take a while, since the
changes would touch almost every project on [[https://invent.kde.org][invent.kde.org]]

Quite a few KDE features seem to be hidden. I'm sure they're accessible to a KDE
developer, but the docs don't seem to prominently feature them -- or perhaps I
haven't checked recently enough.

Tiling is just a visible example of a feature set that’s hard to get elsewhere.
The biggest differences in Linux-style workflows:

+ Configuration of integration between arbitrary desktop applications.
+ IPC or service oriented automations that makes lifting to networked services transparent.

* The Browser's Ubiquity is a Problem

The browser is the biggest impediment to highly personalized window manager
configurations, where window tiling & launching makes displaying sets of
information that is task related.

Perhaps WASM-based apps will help, if they can have separate cookie stores and
configurable WM integration like "window class/title". We could have had similar
features in "Site Specific Browsers" but separate cookie stores kinda kills one
of Google's revenue streams (and the streams of many other tech corps)

A good example to focus on here is KDE’s right click context menus — the WM has
to extract context from the application or the application (a webapp) has to
pass this information to the action in the right click menu. However, the
browser can’t have access to this. As long as most apps are webapps, it’s
exceedingly difficult/cumbersome for users to launch applications (WM can’t
extract window class info from browser tabs). In other words, as long as the
browser and JavaScript rule the world, the average person will never see someone
use Linux-style WM workflows.

The more that Microsoft/Apple can just snipe features -- 10 years after they’re
relevant like window tiling -- the less apparent the distinguishing features &
workflows will seem to the average person. This is bullshit. This is the epitome
of the Apple 1984 Superbowl commercial.

* What is truly novel UI/UX?

It is very difficult to imagine what you've never experienced. And no, I'm not
talking about "riced" desktops on r/unixporn -- those are mostly a waste of
time. All of the distinguishing features there are superficial.

If there isn't a time-dimension in the media that demonstrates the
configuration, then that's not what I'm talking about. For example, a picture
cannot show you a clever automation/workflow -- these require not just change on
a desktop/application, but demonstrating how UI/UX can differ actually requires
comparing one workflow against another. You cannot do this with a picture.

To further complicate this, you cannot simply compare two workflows. For some
objective, there are many combinations of features or applications that could be
considered. You can't consider all of them: it's too complicated. You need to
simplify, simplify, simplify until you understand the most basic components of
technology UI/UX.

** Generalized concepts for simplifying UI/UX ideation:

These are difficult to see today from the midst of the singularity maelstrom
were in. These concepts were much simpler to understand decades ago when tech
implementations were pure.

[[file:img/mit-borg.jpg]]

This list is not exhaustive. To learn the story behind the above picture, read
[[https://www.bostonglobe.com/business/2012/07/14/former-mit-borgs-still-back-wearable-technology/2EL5NgdbQ5VzjoBUGFZk4I/story.html][Former MIT ‘borgs’ still back wearable technology]]

*** The Cycle of Thin/Thick Clients

How the ebb/flow of thin client to thick client cycles in software
usage/configuration change how end user specifies or experiences UI/UX

*** Networks Convolute in the Thin/Thick Client Cycle

How network access, PC Bus bandwidth, storage change, IPC, etc, convolute the
thin/thick client cycle. It plays out again and again. Human minds are actually
no different than computers: we all develop software in our minds and to
expedite this process, the Enlightenment eventually delivered us the education
system. In a network, a thick client has more value because it is responsible
for much of the heavy lifting.

*** Processes, Messages, Services, Servers as Objects

How a process is almost an irreducible element of computing and how to treat
concepts like processes, messages, servers, etc as objects (in the
object-oriented sense).

*** The Conditions of Possibility for UI/UX

How human languages and programming languages are both stuck with the "naming"
problem and an "addressing/routing" problem.

+ TCP/IP and DNS create possibilities for reliably networked computers.
+ The headers in the HTTP protocol created the possibility for session tracking
  and eventually for Google's Doubleclick purchase.
+ Google Fiber was an experiment to help understand how increased bandwidth
  creates possibilities.
+ Similarly, 100 Gb/s networking technology (and early VR/AR bus topologies) may
  create a need for microkernel-based or "multikernel-based" OS architectures.

These all expect norms agreed upon names for exchanging, computing and storing
information.

*** Consensus in Consensus Seeking Processes

For any nodes in networks (including IRL human social networks) to network,
there has to be some level of consensus or 1-consensus -- that is, consensus in
how to find consensus.

** The Critical Takeaway Here

It's too complicated to reason about this stuff, so compressing the combinations
of possibilities is critical -- similar to how the AI that solved Go compressed
the complexity of the space.

You do this by creating sets of fundamental principles that generally explain
everything. These usually need to be applied monadically to be useful.

e.g. a set of functors that lifts the concepts of "thin/thick clients" and
"networks" to a category that describes how the former cycle plays out given
that:

+ many systems connect to many other systems
+ no systems can contain even the data that describes the total structure of the
  network.

So-called ontological thinking is also useful here: create a preferably
partitioned set of categories/concepts within where concepts fit neatly into one
category. You don't simply need one ontology or system of categories.
