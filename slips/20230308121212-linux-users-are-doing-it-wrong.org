:PROPERTIES:
:ID:       f541e274-0691-472d-8e93-62599b544321
:END:
#+TITLE: Linux Users Are Doing It Wrong
#+CATEGORY: slips
#+TAGS: philosophy

This has already been said, but I think it's more useful to acknowledge this
than it is to have much of an opinion on SystemD.

* What The Users Demand

By looking at what features/software are available/supported, you can tell that
this is what the people demand. It is what it is, but most people imitate
patterns they see. There are a few creatives or obstinate opinionated people who
like to explore something different. I'm not an expert here, there aren't enough
people who really understand "the Unix way" and thus there aren't very many
examples for people to imitate.

...

The reason these problems get worse is because Linux enthusiasts are pushing for
the "Year of the Linux Desktop". Designers have too much control over what UI/UX
feels like and are you going to say no to volunteers? Further, this is what the
users themselves demand and I'm sure the telemetry data supports that. But
there's skew in that data because people cannot imitate what doesn't already
exists /and/ if it only exists with limited adoption, then learning/imitating
that can only grow at that rate.

...

* How Was I Doing It Wrong?


Though this will only show my Jungian shadow, I can give a lot of examples from
personal experience:

+ I didn't know to systematically RTFM, generally before using the project/lib.
+ I didn't know /to/ write packages or /how to/ write packages. At various
  points, I cared a lot about repeatable builds with custom build options --
  building high-level OpenCV libraries that reference dependencies like FFMPeg
  built with specific options.
+ I didn't quite understand how to make XDG directories portable.
+ I knew a =~/.bin= folder was helpful, but I didn't understand how important.
+ I didn't know to approach things from VM first or how to script VM creation
  (until using Vagrant)


**** TODO finish enumerating examples

* What's My Experience With Trying to Use Linux?

Basically, being curious and spending a lot of time trying to use it while
suffering from Dunning Krueger in a society full of people who told me I was
smart even though I wasn't. Then, running into artificial limitations imposed by
hardware/software vendors who didn't want to have to compete with free -- so
they systematically RUINED anyone's experience using Linux by /not doing
anything to support it/. Fuck you Broadcom, nVidia, Microsoft.

I tried to adopt Linux at various points, always at a transition point or
inflection point:

+ In 2004/2005 when it was hard: I couldn't deal with drivers.
+ In 2008: windows just seemed simpler for my work and storage/image management
  made VM's a PITA. Issues reading/writing NTFS meant it was a PITA. I had
  already decided that I wouldn't use NFS/Samba since I didn't like the idea
  that the gubment could read/write to my disks/files, potentially planting data
  -- paranoid, but I knew I didn't know enough about setting up security to be
  sure.
+ In 2009: tried to install Linux on a new ASUS laptop. If I had been
  successful, I would've gotten back into Ruby/Rails development ... but I ran
  into GPU problems since Microsoft and their partners decided to fuck Linux one
  more time by making it a giant PITA for anyone to dare try the free shit
  without suffering. This is the "bumblebee" drama that made Linus Torvald's
  middle finger so famous.
+In 2011: here, I installed Arch, but package managers still needed to handle
  init scripts. I didn't know enough about what I didn't know about to work
  through that or it wasn't very fresh in my mind. At this point, AFAIK, SystemD
  was about to take over, which is basically the end of "traditional Unix"
  Linux. There were a lot of pros/cons in standardizing Linux init with SystemD,
  but I basically didnt' know what it was.
+ In 2013: still not learning much. Oh and proprietary disk formats without
  vendor support fucked it up again. Can't read/write to HFS. And did you read
  the note on Samba/NFS above? Do you think I trusted Dropbox? No, never used
  it.
+ In 2015: needed to run cool machine-learning shit in Docker for competitions
  in Toptal/Kaggle. But OSX/MacOS at that point had a HyperKit that wouldn't let
  you do that -- no machine learning for you, unless you want to open a Jupyter
  Notebook and push buttons like the 3 year old that thinks they're playing an
  arcade game. So I needed Linux ... but I had no idea what SystemD was. I
  didn't know to write packages. Everything was fine until I upgraded and
  something more than a hosed Grub happened to my install.
+ In 2019: needed to use Linux for POS Macbook Pro, but went with ArchLinux and
  my hardware model was specifically not getting nVidia updates, so trying to
  figure that out was where all my time when. Fuck you nVidia.
+ In 2020 (macbook pro 2011 hard disk and RAM issues)
+ In 2021 (sway issues)

* Misc

** From a random reddit post

this is more of a response than you asked for probably, but oh well lol.

yeh, IMO nowhere but servers should be pulling their emacs packages from
Deb/RPM. I find Emacs in console infinitely userful, but usually unnecessary
when you can use Tramp. On servers, Vanilla Emacs should basically be sufficient
and if you don't understand enough about Emacs to add config to it with almost
zero reliance on packages, then in most cases you shouldn't be using it on a
server! instead, emacs dependencies should be pulled by straight.el or provided
by Guix. I don't have much of an opinion on M/ELPA though. The model provided by
straight.el allows users to iterate very quickly on emacs packages:

- you can check out branches/tags
- you can fork and with forge, you can submit issues/pullreqs
- it builds your packages for you

The complete workflow from start to end is available to any packages handled by
straight. Guix should be used for emacs packages where stability is needed. The
stats for downloads on those Deb's should be taken into account, but IMO I would
phase that out if possible.

Guix can also produce RPM packages, which comes with some [important
caveats](https://guix.gnu.org/manual/devel/en/guix.html#index-RPM_002c-build-an-RPM-archive-with-guix-pack)
when Guix is installed on that system.

#+begin_example shell
guix pack -f rpm -R -C xz -S /usr/bin/hello=bin/hello hello
sudo rpm --install --prefix=/opt /gnu/store/...-hello.rpm
#+end_example

The benefit to this is that you get the stability of emacs packages built by
guix, but you don't need it installed on the system. with relocatable guix
packages, then your users can simply decompress that archive and have those
tools in there local `~/.emacs.d`

The more time goes on, the greater the volume of open source software. If you're
a software developer, using newer versions of things is so critical and not
having people who can testpilot new versions of software really holds the whole
community back. for automated system configuration, there are tools like ansible
(and again, nix/guix). however, this tooling just expands the technological
sprawl of tools/names/repos that you need to learn. when the rate of expansion
increases:

- the more lag there is between packages available in Ubuntu `n.04` and `n.10`
- the more technical debt there is to play catchup on
- the more that the content provided by blogs & guides age
- the more bloat there is in resources for getting help.

when the function/design of this tooling is significant, this is a net gain --
but when the design is arbitrary and thus an unnecessary abstraction, it's
definitely a net loss that diffuses the collective productivity of
packagers/maintainers/developers. all too often, this tooling overlaps almost
entirely with something that already exists that most users seem to have
forgotten about. this creates a lot of "churn" in the tooling that desktop users
gravitate towards, which

Canonical does value stability. i've been an on/off Archlinux user for ~10 years
only recently after having a ton of problems with AMD ROCm builds and driver
unavailability do I really now understand what this means.

I wish the differences between workflow when working with packages were made
clearer between Deb/RPM/Arch and now Nix/Guix. Things like:

- finding packages
- tooling for graphing dependencies
- finding repositories (and finding keys)
- making packages (esp. tools like mock)
- hosting a network local repository or personal repository

I do think that Debian may have a better model for integrating packages into
your package manager, but since it's more decentralized, this encourages

Some things change this I guess, like containers, flatpaks & snaps, so perhaps
it's not all that relevant anymore. Oh and now there's nix/guix too. And for
languages there's usual a *env like pyenv/rbenv/nvm.

But this is all quite a bit to configure on a new system, those configurations
are prone to breaking and they aren't as portable as they should be.

Having to maintain packages in RPM and Debian is just slow. Having to choose
between the first-party support for packages produced for OpenCL/GPU drivers on
Deb/RPM and the occasionally sparse support for such in Arch is very problematic

I think everything would be better if most software was written in lower-level
languages like C/C++, Rust or Zig, since the presence/config of `*.so` files,
fontconfig/etc and maybe service dependencies are some of the few requirements
for a compiled program to run. If everyone stayed at this lower level, you just
wouldn't need the volume of packages that are required for "higher-level"
scripting langauges.

But this requires "compiling software" oh noes! the web-developers who were
pushing all these scripting languages are now exhalting the benefits of strongly
typed languages. You definitely want the options for both, but IMO, you want
fast stuff with reasonable compile/runtime error messages to be the first
option, then the scripting languages glue everything together ... while the
scripting language layer remains flat.

such an approach requires a set of users comfortable with:

- bash
- autotools/make
- ldd
- package tooling
- c/c++ and etc

instead of:

- nodejs
- python/pypi/etc

and if the dependencies for their scripting language of choice is also to remain
fairly thin/flat, then they also need to be good at that. For example, you
basically should not need many dependencies as a _new python user_ because if
you read the Standard Lib documentation, it does almost everything you could
need for you. only after mastering those tools, should you wheel us all on to
dependency hell.

instead there's a combinatorial mountain of packages/tooling to maintain which
ages fast (but not fast enough) and ultimately serves as confusing crutches for
people. this drains the productivity of more experienced users and just makes
things too easy for new users who never really learn what's what...

* Roam

+ [[id:b82627bf-a0de-45c5-8ff4-229936549942][Guix]]
+ [[id:bdae77b1-d9f0-4d3a-a2fb-2ecdab5fd531][Linux]]
+ [[id:ca4acf9b-775b-4957-b19a-0988b7f429c5][RPM]]
