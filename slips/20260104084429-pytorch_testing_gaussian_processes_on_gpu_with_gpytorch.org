:PROPERTIES:
:ID:       78dce676-1ff4-482c-94b3-92102aa6c6da
:END:
#+TITLE: Pytorch: Testing gaussian processes on gpu with gpytorch
#+CATEGORY: slips
#+TAGS:

* Roam
+ [[id:fbf026c8-6c89-4ad3-a72e-2d693371c76a][Machine Learning]]
+ [[id:b4c096ee-6e40-4f34-85a1-7fc901e819f5][Python]]
+ [[id:79d41758-7ad5-426a-9964-d3e4f5685e7e][Compute]]

* Resources
Working from [[https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_on_Classification_Labels.html][this example]] (i'd rather be using tensorflow...)

* Notes

+ Python and torch are [[https://github.com/dcunited001/ellipsis/blob/98d5c0ab7d07ca99d9f61e56b53ef0e6349ec9a2/nixos/hosts/kratos/python.nix][installed on NixOS as such]]
+ I was running in a scratch buffer using =M-x +python/open-ipython-repl=
  - =C-c C-r= to run region, =C-c C-e= to run 
  - functionality like eldoc works fine (LSP IS NOT EVEN ON)

#+begin_src python
import math
import torch
import numpy as np
import gpytorch
from matplotlib import pyplot as plt

# %matplotlib tk # when in M-x +python/open-ipython-repl
#+end_src

Need to set the device

#+begin_src python
if torch.accelerator.is_available():
    device = torch.accelerator.current_accelerator()
else:
    device = torch.device("cpu")
#+end_src

TODO: umm trace through tensor references and call =.to(device)=

(i changed like 3 lines here. out of time)

#+begin_src python
def gen_data(num_data, seed = 2019):
    torch.random.manual_seed(seed)

    x = torch.randn(num_data,1)
    y = torch.randn(num_data,1)

    u = torch.rand(1)
    data_fn = lambda x, y: 1 * torch.sin(0.15 * u * 3.1415 * (x + y)) + 1
    latent_fn = data_fn(x, y)
    z = torch.round(latent_fn).long().squeeze()
    return torch.cat((x,y),dim=1), z, data_fn

# Tensor, Tensor, function
train_x, train_y, genfn = gen_data(500)

plt.scatter(train_x[:,0].numpy(), train_x[:,1].numpy(), c = train_y)
#+end_src

This shows some random data that has some correlation

#+begin_src python
test_d1 = np.linspace(-3, 3, 20)
test_d2 = np.linspace(-3, 3, 20)

test_x_mat, test_y_mat = np.meshgrid(test_d1, test_d2)
test_x_mat, test_y_mat = torch.Tensor(test_x_mat), torch.Tensor(test_y_mat)

test_x = torch.cat((test_x_mat.view(-1,1), test_y_mat.view(-1,1)),dim=1)
test_labels = torch.round(genfn(test_x_mat, test_y_mat))
test_y = test_labels.view(-1)

plt.contourf(test_x_mat.numpy(), test_y_mat.numpy(), test_labels.numpy())
#+end_src

Convert to tensors

#+begin_src python
from gpytorch.models import ExactGP
from gpytorch.likelihoods import DirichletClassificationLikelihood
from gpytorch.means import ConstantMean
from gpytorch.kernels import ScaleKernel, RBFKernel

class DirichletGPModel(ExactGP):
    def __init__(self, train_x, train_y, likelihood, num_classes):
        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))
        self.covar_module = ScaleKernel(
            RBFKernel(batch_shape=torch.Size((num_classes,))),
            batch_shape=torch.Size((num_classes,)),
        )

    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

# initialize likelihood and model
# we let the DirichletClassificationLikelihood compute the targets for us
likelihood = DirichletClassificationLikelihood(train_y, learn_additional_noise=True)
model = DirichletGPModel(
    train_x,
    likelihood.transformed_targets,
    likelihood,
    num_classes=likelihood.num_classes).to(device)
#+end_src



#+begin_src python
import os
smoke_test = ('CI' in os.environ)
training_iter = 2 if smoke_test else 50

# Find optimal model hyperparameters
model.train()
likelihood.train()

# Use the adam optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters

# "Loss" for GPs - the marginal log likelihood
mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

for i in range(training_iter):
    # Zero gradients from previous iteration
    optimizer.zero_grad()
    # Output from model
    output = model(train_x)
    # Calc loss and backprop gradients
    loss = -mll(output, likelihood.transformed_targets).sum()
    loss.backward()
    if i % 5 == 0:
        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (
            i + 1, training_iter, loss.item(),
            model.covar_module.base_kernel.lengthscale.mean().item(),
            model.likelihood.second_noise_covar.noise.mean().item()
        ))
        optimizer.step()
#+end_src

fdsa

#+begin_src python
model.eval()
likelihood.eval()

with gpytorch.settings.fast_pred_var(), torch.no_grad():
    test_dist = model(test_x)
    pred_means = test_dist.loc

fig, ax = plt.subplots(1, 3, figsize = (15, 5))

for i in range(3):
    im = ax[i].contourf(
        test_x_mat.numpy(), test_y_mat.numpy(), pred_means[i].numpy().reshape((20,20))
    )
    fig.colorbar(im, ax=ax[i])
    ax[i].set_title("Logits: Class " + str(i), fontsize = 20)
#+end_src

fdsa

#+begin_src python
pred_samples = test_dist.sample(torch.Size((256,))).exp()
probabilities = (pred_samples / pred_samples.sum(-2, keepdim=True)).mean(0)

fig, ax = plt.subplots(1, 3, figsize = (15, 5))

levels = np.linspace(0, 1.05, 20)
for i in range(3):
    im = ax[i].contourf(
        test_x_mat.numpy(), test_y_mat.numpy(), probabilities[i].numpy().reshape((20,20)), levels=levels
    )
    fig.colorbar(im, ax=ax[i])
    ax[i].set_title("Probabilities: Class " + str(i), fontsize = 20)
#+end_src

fdsa

#+begin_src python
fig, ax = plt.subplots(1,2, figsize=(10, 5))

ax[0].contourf(test_x_mat.numpy(), test_y_mat.numpy(), test_labels.numpy())
ax[0].set_title('True Response', fontsize=20)

ax[1].contourf(test_x_mat.numpy(), test_y_mat.numpy(), pred_means.max(0)[1].reshape((20,20)))
ax[1].set_title('Estimated Response', fontsize=20)
#+end_src


* Jupyter...

=ob-jupyter= is a bit much to pick up at first, but it's not that difficult.

+ Kernel management & reproducible environments are much more work
+ the main problem with jupyter is that can't easily transition into code
  that's testable or exportable as a library (which is a pita anyways)
+ when you do, it breaks all your notebooks, if you _created_ something new...
  in which case it probably wasn't clearly-communicated code anyways. and from
  that point, it won't even resemble the final implementation.
+ but basically, you can't really _create_ anything except for colorful
  communication that strongly depends on libraries already having been written
  (which is where complexity lies)
+ still, being able to communicate is powerful
