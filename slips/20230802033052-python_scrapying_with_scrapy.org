:PROPERTIES:
:ID:       0e016582-6e91-47e8-b75f-8857f7af26e5
:END:
#+TITLE: Python: Scrapying with Scrapy
#+CATEGORY: slips
#+TAGS:

#+begin_quote
Keep in mind I don't exactly know what I'm doing here.
#+end_quote

Various things can dramatically affect how you approach this.

+ Particularly, whether you're spiders begin on a Domain and are encouraged to
  explore links apropos to that domain (from pages on Domain A to pages on
  Domain B).
+ Whether you're permitted to crawl the data source.
+ Whether you distribute your processes and why.
+ Whether you're using something like Squid.
+ How much javascript affects the content of the code.
+ How clean the datasource is and what your own downstream processes look like.

You should almost always use an API anyways! An API is intended /to produce/
data /suitable to consumers/.

* Overview

Everything is written on top of the [[https://twistedmatrix.com/trac/][Twisted]] framework, so many/most calls are
run asynchronously, but probably transparently to your code (for small tasks).

For small tasks, esp. involving homogenous data input/output, all your code
could go in a single file, but if so it will need to construct a runtime context
for Twisted.

** [[https://docs.scrapy.org/en/latest/topics/architecture.html][Scrappy Architecture]]

Just refer to this image (source at docs) to get a feel for the components

[[file:img/scrapy-architecture.png]]

** Classes/Concepts

+ Spiders :: these will parse responses for a category of URL. you will likely
  want to bind them to a single domain. They are not intended to correspond to a
  single webpage's format. The [[https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.Spider.parse][parse(res)]] function must return =Request=, =Item=, =[<Request | Item>]= or =None=
+ Items :: these can be as simple as native python lists/dicts or you can
  construct classes. the =@dataclass= can help facilitate the getters/setters on
  these.

#+begin_quote
Items provide the /container/ of scraped data, while [[https://docs.scrapy.org/en/latest/topics/loaders.html][Item Loaders]] provide the
mechanism for populating that container.
#+end_quote

* Topics

** Structuring Data

Ideally, something or a spider can easily tell you how much has changed and
whether you need to scrape everything. You should think of Scrapy as something
of a query on an expensive database. The items should represent rows in a
database (potentially denormalized). But the items a spider returns may not
correspond to the items you want to store as the end result of your Scrapy
Pipeline or further down other pipelines (outside of Scrapy).

You should enumerate parameters/types that, if this were a database, you might
query on to completely select all the results across separate queries. e.g. if
some table type has 4 categories, then in SQL, you might include =where
tbl.the_type = 1= and so forth to complete cover the space. These loosely would
correspond to the URL query parameters, but data on the web is a little less
structured than that.

** Detecting Updates

+ RSS or other streams.
+ For an item, you should identify a sets of fields used as input to a hashing
  function.  This hash can act as a shortcut to the items identity or in other
  cases can imply whether there are downstream updates.

* Issues

** Pagination

Probably the first thing that's going to come up. This would require identifying
a subset of URL's to pass as requests to a single object.

+ [[https://scrapeops.io/python-scrapy-playbook/scrapy-pagination-guide/][Six most popular pagination methods]]

** Getting blocked

You should probably be straightforward with your =UserAgent= and traffic if you
can and eliminate dirty code that strains someone's web service. Most tasks
should be event-driven

Most tasks should identify key data points that either indicates a need to run
or what pages/params to run on or, in worse cases, what to crawl. However,
crawling is going to be expensive /for you/ and for others.

** Limiting Requests

This was an initial issue I ran into. However, AFAIK if =your_spider.parse(res)=
doesn't yield any requests, it's not going to call any spiders.

** Project Structure

I generated my project awhile ago and wanted to flatten the autogenerated
structure so =scrapy.cfg= occupied the root alongside =poetry.toml= and
=.git=. Lacking real-world pypy experience, this wasn't exactly clear from the
"learn scrapy in 21 days or less" books that I read.

If the =scrapy.cfg= file is moved, this may require changing it's module name
references. Other class names mentioned below in "module names" may need to be
changed.

*** Poetry

The =.venv/bin/scrapy= executable needs to have its hashbang changed -- run
=poetry install= again and force the deps to be reinstalled with =rm -rf .venv=

The =pyproject.toml= must also specify a valid [[https://docs.python.org/3/tutorial/modules.html#the-module-search-path][project module root]]. Change
=tool.poetry.packages= to =packages = [{include = "$moduleroot"}]=

*** Module names

#+begin_quote
For metaprogramming, Ruby > Python ... although /tbf/ Scrapy can't really
benefit from many of the new language features in recent versions of Python3.
#+end_quote

The =scrapy startproject $projectname= command adds =$projectname= to some of
the generated modules/classes. 

#+begin_src sh :dir (identity myprojroot) :results code output :wrap example diff
diff scraping yparcs2
#+end_src

#+RESULTS:
#+begin_example diff
diff '--color=auto' yparcs2/yparcs2/items.py ../scraping/scraping/items.py
9c9
< class Yparcs2Item(scrapy.Item):
---
> class ScrapingItem(scrapy.Item):
diff '--color=auto' yparcs2/yparcs2/middlewares.py ../scraping/scraping/middlewares.py
12c12
< class Yparcs2SpiderMiddleware:
---
> class ScrapingSpiderMiddleware:
59c59
< class Yparcs2DownloaderMiddleware:
---
> class ScrapingDownloaderMiddleware:
diff '--color=auto' yparcs2/yparcs2/pipelines.py ../scraping/scraping/pipelines.py
11c11
< class Yparcs2Pipeline:
---
> class ScrapingPipeline:
diff '--color=auto' yparcs2/yparcs2/settings.py ../scraping/scraping/settings.py
1c1
< # Scrapy settings for yparcs2 project
---
> # Scrapy settings for scraping project
10c10
< BOT_NAME = "yparcs2"
---
> BOT_NAME = "scraping"
12,13c12,13
< SPIDER_MODULES = ["yparcs2.spiders"]
< NEWSPIDER_MODULE = "yparcs2.spiders"
---
> SPIDER_MODULES = ["scraping.spiders"]
> NEWSPIDER_MODULE = "scraping.spiders"
17c17
< #USER_AGENT = "yparcs2 (+http://www.yourdomain.com)"
---
> #USER_AGENT = "scraping (+http://www.yourdomain.com)"
48c48
< #    "yparcs2.middlewares.Yparcs2SpiderMiddleware": 543,
---
> #    "scraping.middlewares.ScrapingSpiderMiddleware": 543,
54c54
< #    "yparcs2.middlewares.Yparcs2DownloaderMiddleware": 543,
---
> #    "scraping.middlewares.ScrapingDownloaderMiddleware": 543,
66c66
< #    "yparcs2.pipelines.Yparcs2Pipeline": 300,
---
> #    "scraping.pipelines.ScrapingPipeline": 300,
Common subdirectories: yparcs2/yparcs2/spiders and ../scraping/scraping/spiders
#+end_example

* Roam
 + [[id:b4c096ee-6e40-4f34-85a1-7fc901e819f5][Python]]
