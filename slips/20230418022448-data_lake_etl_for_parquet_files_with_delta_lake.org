:PROPERTIES:
:ID:       e2880db1-13fa-4bfe-9d66-8d8070d61cce
:END:
#+title: Data Lake: ETL for Parquet Files with Delta Lake

* Roam
+ [[id:0b80782f-92a8-4b48-958c-a41e7ff8713e][Data Lake]]
+ [[id:4ab045b9-ea4b-489d-b49e-8431b70dd0a5][Data Science]]
+ [[id:4c531cd8-3f06-47fb-857a-e70603891ed8][Hadoop]]

* Resources

+ Official Delta Lake tutorial on Zeppelin
+ [[https://towardsdatascience.com/deep-dive-into-delta-lake-via-apache-zeppelin-d59db1673584][Deep Dive into Delta Lake via Apache Zeppelin]]
  - [[github:zjffdu/zeppelin-notebook][zjffdu/zeppelin-notebook]]
  - follow instructions in [[https://zjffdu.medium.com/learn-spark-on-zeppelin-in-docker-container-9f3f7b2db230][Learn Spark on Zeppelin in Docker Container]]

** Compatibility
*** Spark
=Zeppelin 10.1= is only compatible up to =Spark 3.2.x=, given the following
error message:

#+begin_quote
You can set zeppelin.spark.enableSupportedVersionCheck to false if you really want to try this version of spark.
#+end_quote

Although you can add =zeppelin.spark.enableSupportedVersionCheck false= to the
=%spark.conf=. Changing this requires restarting the interpreter (kernel). And
this apparently works ... for now.

*** Delta Lake

Delta Lake =2.1.x= through =2.3.x= are only compatible with =Spark 3.3.x=

... so need to run Delta Lake =2.0.2=

* Overview

It's a lot of extra work & boilerplate to work with the data this way. However,
there's a few advantages:

+ More control over OOM errors and disk space issues when reading/transforming
  the parquet files. Tools like =dsq= for example will ingest the data into
  =sqlite= and that lacks the necessary memory.
+ Zeppelin is a pretty useful tool: basically as complicated as Jupyter (to
  which I'm new) and I get to write SQL queries
+ Eventually, if I do transform the parquet database, then I can set
  checkpoints. However, it's so large that I'm not sure this is realistic.

* Prepare dependencies

+ Download Spark 3.2.0
+ Pull the =apache/zeppelin:0.10.1= image
+ Get the dataset downloaded


* Create Workspace and Volumes

Three volumes are needed, which must be separate trees:

+ Delta databases
+ Zeppelin notebook workspace
+ Spark

The directory structure should look something like this

#+begin_quote
.
├── asl
│   ├── asl-signs.zip
│   ├── delta
│   ├── dsl10-data
│   ├── dsl46-data
│   └── sign2pred.json
├── spark
│   ├── spark-3.2.4-bin-hadoop3.2
│   └── spark-3.2.4-bin-hadoop3.2.tgz
└── zeppelin-notebook
    ├── Flink
    ├── Python
    ├── R
    ├── README.md
    └── Spark
#+end_quote

The goal is to create a delta table in =./asl/delta/mp= and do some ETL
transformation to trip some columns and push them to a new delta table in
=./asl/delta/lake=. The directory structure for =delta= should look like this
before creating the tables.

#+begin_quote
.
├── lake
└── mp
    ├── sign_to_prediction_index_map.json
    ├── train.csv
    └── train_landmark_files
        ├── 16069
        ├── 18796
        ├──  ...
        ├── 61333
        └── 62590
#+end_quote

There may be some problems encountered in the partitioning formt should include
the partition field names in their directory structure. The directories
contained in =train_landmark_files=

* Start the Zeppelin Container

#+begin_src emacs-lisp
(setq bigdata-home (getenv "PWD"))
(setq bigdata-home "/data/vm/bigdata")
#+end_src

#+begin_src shell :tangle (expand-file-name "drun.sh" bigdata-home) :shebang #!/bin/sh  :tangle-mode (identity #o744) :mkdirp yes

docker run -u $(id -u) -p 8080:8080 -p 4040:4040 --rm \
       -v ${spark_location}:/opt/spark \
       -v ${zeppelin_notebook}:/opt/notebook \
       -e ZEPPELIN_NOTEBOOK_DIR=/opt/notebook \
       -e SPARK_HOME=/opt/spark \
       -e ZEPPELIN_LOCAL_IP=0.0.0.0 \
       --name zeppelin apache/zeppelin:0.10.1
#+end_src


* Attempt to run the basic notebook

Change =delta-core_2.12:1.0.0= to =delta-core_2.12:2.3.0= or the java gods get
angry.

#+begin_src shell
%spark.conf

spark.jars.packages io.delta:delta-core_2.12:2.3.0
spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog
spark.sql.warehouse.dir /tmp/warehouse
#+end_src

Crash the box. Java gods not satisfied with your sacrifice.

#+begin_src sql
%spark.sql

CREATE TABLE IF NOT EXISTS events (
  id INT,
  data STRING)
USING DELTA
#+end_src

Check versions and rerun. It works!

* Notes
