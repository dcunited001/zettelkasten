:PROPERTIES:
:ID:       a267b09f-4321-4bec-8eb1-c7c5069346be
:END:
#+title: From The Latex Barcode Docs To Multiagent Information Systems And Charles Peirce

* Roam
+ [[id:45b0ba21-fb20-44dc-9ee9-c4fed32acbde][On Fallibalism, Peirce and Combinators]]
+ [[id:cea7d11c-8357-4e4f-90b3-fa8210eff796][AI]]  
+ [[id:0bef6f3e-3007-4685-8679-e5edbcbb082c][LaTeX]]

* Outline
** Intro
*** Appetite For Complexity
Presented with the CTAN barcode library for LaTeX, where does it extrapolate?
What could possibly be the value of learning this information

*** But why would you learn that?

+ it's essential for integrating data sources for distributed systems
+ anticipating policy or decisions in complex economic, corporate, social and/or
  governmental systems
+ =identity= is necessary for robust statistics that governs research and policy
  outcomes. it kinda prefigures the shapes of decisions and information.
+ anticipating changes in assumptions about what's normal or normative

**** An entry point to complex systems

How can i find the shortest path through complex systems?

**** Examples

+ where will blockchain really provide leverage/value in non-speculative
  applications that outweights its energy costs & data privacy concerns?
+ how can human-produced content be validated without an identifier? 
+ how can I meaningfully join datasets from =data.gov= if they, their research
  category, their financing grants or their associated research publications
  can't be identified
+ anonymized medical research data & the benefits/problems thereof. can we
  really generatively produce data to train AI applications for medical
  purposes? where? why? this depends on:
  - the specifics of how data-sharing is enabled
  - how the specific research application could be affected by "leaking"
    variables in AI applications
  - whether finance for research is motivated by a need to cover up bad chemical
    medicine with rigged data (not sure they can)

*** Information Theory

**** An agent's capacity to extract valuable information from compressed information
**** A piece of informations capacity to decompress into extrapolated information

**** The complexity of information in agents that extends to valuable extrapolation

This is the critical piece. It applies to artificial agents and organic agents.

+ Information is intrinsic to things. Information has measure(s) of complexity
+ Agents are things and are thus defined by information.
+ Information systems can be described by volumes in high-dimensional space

**** Extrapolation has associated energy, time and resource utilization costs

Agents that can unpack information and extrapolate it are more valuable, but
it's more expensive

**** The paths through such a high-dimensional space and have various meaningful interpretations

This depends on definitions of "things"

+ your prompt is a path through a vector space composed of sequences of tokens
+ the agent's response is a path through vector space
+ an agent's actions are a path through an action-space (however it is
  represented, it is useful)
+ attention states are a path through intermediate attention-head space. this
  is probably complicated & difficult to capture, but perhaps useful.
  - it's essentially metacognition if you want your agents to "JIT" their
    responses
  - besides dimensionality, performance & retention concerns, the main
    problem: it's not necessarily possible to unify these paths into a single
    space where you (or an algorithm) can meaningfully compare them

**** There is a volume bounding the space of extents which agents extrapolate into

This also depends on definitions

A bigger volume here is not always better ... because it implies vastly
increased costs.

**** The definition intelligence is according to it's measure

There are many measures of intelligence

Given examples of information which are measurably compressed, i.e.

+ you have some reasonable measure that fairly retains its estimation of
  compression across a reasonable domain of examples of compressed information.
+ the nature of information examples doesn't somehow bias the measure of agent
  intelligence (which needs to be a separate measure)

Then you can establish separate measures of information extrapolation, including

+ measures of consistency, normalcy (of response or reasoning), specificity
+ the volume(s) of space over which agents can extrapolate to: bigger is better,
  but $\frac{V,E}$ where =V= is volume and =E= is energy is best
  - e.g. "you are an expert Snookie player" as a prompt alongside an information
    example probably isn't going to win an AI Nobel Prize.
+ the novelty/originality of paths are perhaps the most important measures
  because (according to this definition of intelligence) these paths move into
  separate volumes of the space. If novel/original, these should be unusual.
  - If also efficient, then this implies the agent/LLM can parameterize logical
    arguments with unique while highly correspondant/determinitive parameters,
    which other agents choose to ignore.

#+begin_quote
There are an endless number of problems problem with quantitatively "measuring"
this.
#+end_quote

**** This volume of space representing the range responses needs to be validatable

This implies that the test observers need to be some level of the following
qualities:

+ more knowledgable than tested agents
+ more intelligent than tested agents
+ more comprehensive/dynamic with their validation of logic

And they need to be somewhat capable of recognizing peculiar features of
information systems which are _valid likelihood estimators_, despite being
seemingly unrelated or tangential features that parameterize some argument.

I cannot understate how critical it is for intelligence to correspond in part to
the ability to ideate many paths which:

+ *are highly _original_, yet efficiently validatable.*
+ *sufficiently cover a volume of space*
+ *retain the ability to tone down tangential explanations* & reasoning strategies
  which as reasoning parameters do not efficiently reduce the "possibility
  space"
  - e.g. when presented with a fuzzy-valued modal logic system, which variables
    (or even constant values) have the greatest effect on the outputs of the
    system.
  - if the modal logic system is permitted to expand/contract the "cognitive
    closure", adding input and contextual parameters as priors whereby the fuzzy
    values, then different kinds of additional contextual paramters will have
    varying degrees of effect on the logic system's values
  - if the modal logic system is permitted to use outputs of other "logic
    statements" in the "alphabet/language" of the logic system, then if it can
    connect the posterior of those statements into the original logical system
    ... then it done real good. (probably not computable)
  - see [[Logic Tensor Networks][Logic Tensor Networks]]. This approach to manipulation of fuzzy-logic
    systems is similar to Charles Peirce's abductive logic.
+ *include components/parameters in the "cognitive closure"...* which are not
  immediately perceived as germaine, but which contribute value to "likelihood
  estimators". once validated and shown to "lift" to a category of applicable
  reasoning strategies, these parameters can provide "shortcut paths" or at
  least additional methods/heuristics for partial validation of other
  information systems (responses, arguments, etc).

***** Example of novelty

Below is an example of a "novel/original" reasoning that challenges the notion
that "tobacco causes cancer" came as a surprise to people in the 1960s. This is
a "path" through a "volume" where most LLM (depending on training, data,
prompting, etc), may not otherwise identify this path unless specifically nudged
in thisdirection)

#+begin_quote
The 1940's medical establishment, despite being super knowledgable &
authoritative, just would not admit the possibility that smoking tobacco may
cause lung cancer. in society, people may have also assumed the same and science
may have provided "objective" counterexamples.

But, clearly, this insight, despite authorities' rejection, is an extrapolation
of well-intentioned logic that considers many aspects of smoking -- many
disparate subsets of "features" of common experience could combine to construct
this insight. e.g.

+ less rationally-minded people with less access to objective & validated
  information select more experiential paramters for reasoning (and they often
  assume heuristical approaches to regulating their lives)
  - they would certainly recognize that variation in tobacco supply & smoking
    would be coupled to subjective aspects of their experience (e.g. buildup of
    phlegm, mucous; changes in response to tobacco when sick; susceptibility to
    communicable disease)
  - many people were too poor to smoke. often supply was not regular -> hence
    sufficient variability in experience to provide some insights
  - some combination of subsets of their subjective insights on smoking would
    lead some to believe that it is possible to smoke too much (and unhealthy to
    do so).
  - *(novelty)*: some nations completely outlawed tobacco import (e.g. Russia in the
    late 17th century, esp. around their spat with Britain). thus there was
    sufficient variation in tobacco availability & usage habits
+ chewing tobacco, esp with habitual selection of parameters for usage -- always
  in the same side of the mouth -- creates some pretty clear outcomes that make
  its carcinogenic nature undeniable. I mean even the people in the middle ages
  were not stupid.
+ autopsies are kinda irrefutable. parameter extraction here could easily
  correlate the appearance of a highly anomolous smoker's lung to that of the
  tobacco naive (or for younger)
  - the appearance of smoker's lung in autopsies is irrefutably correlated to
    age (assuming commoditization of tobacco with consistent supply), even if
    smoking status/habits are not known.
  - validating this, given the physical _medical sign_ -- a literal neon signpost
    of "hey these lungs look diseased as hell"
  - many advancements in late 19th century & early 20th century medicine would
    never have happened if they could not recognize very simple _signs_ and
    _symptoms_
  - operating on a restricted set of methodologies and quantifiable tests thus
    similarly limited doctors to examining subjectively observed signs/symptoms
    over quantifiable data points.
#+end_quote

LLM doesn't actually reason about subsets of knowledge/information, nor does it
place artifial constraints on its knowledge system while following multiple
"subtrees of thought" (the closest to this is the "system of experts")

*** Charles Peirce

**** Deductive Logic and Inductive Logic -> Abductive Logic

Sounds creepy when considered literally, doesn't it? ... ugh
**** 

** Dimensional Analysis of Integrated Systems
+ Norms and information compression
+ Complexity of working with multiple git forges
+ Master vs. Main 

** LaTeX Bar Codes

*** An Entry Point To Complex Systems

The =identity= function

+ from a programmer's perspective
  - can distinguish values or references
+ from an archvist's perspective
  - distinguishing recognized publications in the Library of Congress
  - tracking citations of a publication with a DOI, making it simple to
    share those publications by URL or attributing an individual publication to
    a journal or ontological category
+ from the purchaser or retailer's perspective
  - cell-phone IMEI-to-phone-number activation
+ from a marketing perspective
  - tracking session IDs across visits to a website
+ from a logicistics perspective
  - can track supplies, parts, failures RMA, productivity, processes, results
    of processes, etc.
  - e.g. you can track parts by source to correlate them to production failures
    or product RMA failures. this helps provide a layer of validation to
    attribution of cause/effect.
+ from the MBA's perspective
  - can save a lot of money. it also provides a layer of validation in claimed
    results (it enables tracking data to produce statistics)
+ from the perspective of arbitrary governing bodies or institutions

*** The =identity= function

If two things are "different" according to some definition of uniqueness...
then, given two values that are not the same, =(eq (identity a) (identity b))= is
false. Otherwise, it's true**

#+begin_quote
 ** some rules and regulations apply. check your local state distributor for
    more information on null and void values.
#+end_quote

**** Types of identifiers

Each type of barcode corresponds to an identifier. Types of identifiers include:

+ UUID or GUID: these are identifiers whose values are not trusted, per-se, but
  are unique enough. you can mix in information to derive UUID's or GUID's from
  parameters, whether you need to validate they were derived or not
+ Database =id= column
+ Barcode: a product identifier. this is useful for:
  - integrating information across disparate databases
  - implementation of regulatory policy and the compliance therof
+ QR Code: the integer value corresponds to the encoded string.
  - the integer value must be derived from the qrcode spec for validating the
    bits in the QR Code.
  - The integer can also correspond to a string. =_Every_ string _is_ an integer!=
  
**** From a mathematician's perspective

+ a means of distinguishing values. this can be used to examine sets that
  contain subsets with "metadata" where the primarily value in each subset may
  not be identical depending on the "metadata"
+ to actually implement this, you need to layer in a "UUID" or a global value
  drawn from a set of identifiers $I$ whose size $|I|$ is greater than the
  number of values that need to be represented
+ this is usually glossed over as infinity is assumed to be large enough, but
  infinity does not exist in the real world (not in any conventional sense...
  perhaps the infinitessimal exists, but does it?)

**** Other identifiers
+ integers: integers are distinguishable, but similar
+ pointers: programmers who need to distinguish between references & values
+ prime numbers: form a set of numbers whose influence on the product of other
  numbers is only discernable in terms of the prime numbers's value.
  - e.g. the product of the first 10 primes is not divisible by any product of
    coprimes (composed of the )
+ Hash: an identifier whose value is assumed to be unique based on trust in the
  hashing algorithm.
  - each input to the hashing algorithm is assumed to result in distinct outputs
  - the hamming distance of two nearby inputs should not predict the hamming
    distance of two outputs
  - you cannot change the input to an ideal hashing function and predict the
    influence on the output. if you can produce a desired output hash by
    changing the input, it is not a secure hashing function. even if you can
    only do this with less than $\frac{1,2^24}$) probability

* Lost in Details

*** Provably "shorter" paths through information systems with identifiers

These are extremely high-dimensional spaces, but fortunately they join along
/relatively/ small dimensions with very finite discrete values: the taxonomic &
typological systems of identification, categorization and identification of
categorization

So there are provably "shorter" paths through information systems. the discrete
dimension provided by an =identity= function mapping usually has arbitrary values
+ values not intended to be compared for size; e.g. integers have magnitude
  whereas a database id does, but isn't intended to be a =group= with addition/etc
  
So, even though there can be relations between categories and subcategories,
their identifiers are topologically each separate paths through the
=n-1=-dimensional space of the remaining data dimensions.

+ i.e. for a dimension where customers rank their experience from =1= to =10=,
  you can say the numbers =1= and =2= are topologically "next to" each other and
  you can have =metric='s that determine distance between =5= and =9=. this is
  more useful when the metric also includes other dimensions.
+ but the values of an =identifier= are usually discrete _and_ arbitrary, so they
  are each their own topological bridges through their dimensions of some
  space. they still provide paths between other dimensions, but along singular
  values.

Therefore, when you join disparate systems along such identifiers, they maintain
aspects of these topological implications. The local topology in the regions of
high-dimensional space around such identifiers must also be constrained.

Imagine joining $A = R^4 \otimes Z^3 \otimes I^4$ to some other =$B = I^3 \otimes Z^3
\otimes R^4= space along the discrete & finite identifier in =I^4=.

+ Assume three $I^3$ components represent joinable identifiers for =product=,
  =supplier= and =customer=. Assume the last $I^1$ component is a truly unique GUID.
+ Assume the $Z^3$ dimensions are values bounded from =1= to =10= for 3 types of
  ratings in customer surveys and that one component of $R^4$ is time in UTC.
+ Assume the remaining $R^3$ are =x=, =y=, =z= in a supplier's warehouse. The records
  represent vectors that share a common origin.
  - They could be affine or fixed ... but affine bc the origin in vector spaces
    is usually arbitrary (you don't understand the "vector space" in "vector"
    "space" if you don't understand why)

#+begin_quote
Time in UTC is a global data point and a singular unifying dimension for OLAP
cubes. it's exceedingly useful because it can be chunked into a set of
hierarchical identifiers (hours, days, months years). Space can similarly be
chunked.
#+end_quote

Anyways

+ Shortest paths in =A= or =B= must use euclidean distance in the $R^4$ dimensions
+ Shortest paths in =A= or =B= must use manhattan distance in the $Z^3$ dimensions.
  - this effectively increases length of valid paths through =A= and =B= dimensions.
+ Shortest paths in =A= or =B= can _not_ use manhattan distance. This affects the
  "volumes" of space over which valid paths may be considered.

Joining =A= with =B= (and only retaining the GUID and the joined $I^3$ columns)
doesn't significantly affect the volume that bounds A and B. To "fairly" compare
the volumes, consider the convex hull of =A= with that of =B=: basically the total
space over which objects are stored in the warehouse.

Compare this with the $C=outerjoin\left(A,B,I_{i,j,..}\right)$ space -- which
would give you e.g. the locations of an instance of an item type, if you retain
the value for $R^3$ which is sourced from either =A= or =B=, depending on where that
item was sourced)... retaining granularity here is probably more frustrating for
me than you.

Anyways, this should result in an $C = R^4 \otimes Z^3 \otimes I^2$ space, if GUID and the
=item.type= are retained. The set of paths through this space needs to fit within
a convex hull for the combined spaces that is approximately the shape of
$hull\left(A\right) \cup hull\left(B\right)$.

But!

+ For each value in =item.type= $I_i$, there are distinct convex hulls, the
  dimensionality of which is $R^3$ and the volume of which is provably less than
  the product of =count(item.type)= and $volume\left(hull\left(C\right)\right)$.
+ And because each distinct volumes is /probably/ _far less_ than than one copy of
  volume $\left(hull\left(C\right)\right)$, this means total length of all paths
  through each =item.type='s slice of $R^3$ is far less -- tell me it's infinite
  and i'll tell you to go "foliate" yourself with a bound on =max\left(K\right)=
  curvature. (that's not a foliation by the way)

In other words, these spaces are not contiguous at all. Unless an =item.subtype=
relation exists and is relevant, no =item.type= shares a manifold surface with
another item type. They can be treated as continguous by evaluating the
=outerjoin= first and forgetting about item type.

The important takeaway here: the impact that identifiers have on the volume and
pathlength in these spaces makes them easier to "navigate" and extract
information ... _especially_ when you join & integrate them!

* Misc
* Meta
** Chapters
** Resources
+ CTAN barcode library for LaTeX
  
*** Prereqs
+ Information Theory
+ Ontology and Representations
+ Entity and Agency (artificial and organic)
+ Logistics and Operations
+ Identity Function
  
** LaTeX
*** Printing Press
